[
  {
    "path": "posts/2022-11-16-class-9-heatmap/",
    "title": "Class 9: Introduction to heatmaps",
    "description": {},
    "author": [
      {
        "name": "Kristen Wells",
        "url": "https://github.com/kwells4"
      }
    ],
    "date": "2022-12-09",
    "categories": [],
    "contents": "\n\nContents\nGoals for this class\nLoad packages\nDownload files\nMaking a heatmap\nCleaning up by normalizing values\nCleaning up by scaling values\nChanging the color palette\nAdding annotations\nClustering\nRemove column and row names\nOther aesthetics\n\nAcknowldgements and additional references\nI’m a biologist, why should I care?\n\nThe Rmarkdown for this class is on github\nGoals for this class\nLearn how to make a heat map using pheatmap\nUnderstand how data is generally processed before making a heat map, understand what interpretations can be made given the processing.\nLearn how to change the aesthetics of a heat map\nLearn how to visualize and access clustering information from the heat map\nLoad packages\n\n\nlibrary(tidyverse)\nlibrary(pheatmap) # install.packages(\"pheatmap\")\nlibrary(viridis) # install.packages(\"viridis\")\nlibrary(MetBrewer) # install.packages(\"MetBrewer\")\nlibrary(here)\n\n\nDownload files\nBefore we get started, let’s download all of the files you will need for the next three classes.\n\n\n# conditionally download all of the files used in rmarkdown from github \nsource(\"https://raw.githubusercontent.com/rnabioco/bmsc-7810-pbda/main/_posts/2022-11-10-class-7-matricies/download_files.R\")\n\n\nMaking a heatmap\nToday we are going to continue working with the same data we used to talk about matrices and clustering, the top 100 boy and girl names by state for 2020.\n\n\nmale_names_mat <- read.csv(here(\"class_7-9_data\", \"boy_name_counts.csv\"),\n                           row.names = 1) %>%\n  as.matrix()\n\nfemale_names_mat <- read.csv(here(\"class_7-9_data\", \"girl_name_counts.csv\"),\n                             row.names = 1) %>%\n  as.matrix()\n\n\nWe’ve loaded in both the boy and the girl names as separate matrices, we can combine these using rbind to bind the rows\n\n\nnames_mat <- rbind(male_names_mat, female_names_mat)\n\n\nTo make a heatmap, we will use the pheatmap package\n\n\npheatmap(names_mat)\n\n\n\nNotice here we can only see the names for California\nCleaning up by normalizing values\nIt’s pretty hard to see much strucutre in the data just using the raw values. Let’s try with our normalized values\n\n\nnormalized_mat <- t(t(names_mat) / colSums(names_mat))\npheatmap(normalized_mat)\n\n\n\nYou can see that when we try to normalize the values by dividing each number by the total number for the state, the trends in the data are much more clear and we don’t only see the data for California.\nCleaning up by scaling values\nAnother popular way to normalize the data for a heatmap is called a z-score. This is also know a mean-centered scaled data. The z-score is calculated as follows\n\n\\[\nzscore = \\frac{x - \\mu}{\\sigma}\n\\]\n\n\nWhere x is each value \\(\\mu\\) is the mean for the values and \\(\\sigma\\) is the standard deviation for the values. Here, \\(x - \\mu\\) is called “centering” and dividing by \\(\\sigma\\) is called “scaling”\nIn R, we can scale the data using the scale function. Scaling is done on the columns, so here we can use the original matrix without any transformations\nscale takes 3 arguments\nx - a numeric matrix(like object).\n\ncenter - either a logical value or numeric-alike vector of length equal to the\nnumber of columns of x, where ‘numeric-alike’ means that as.numeric(.) will be\napplied successfully if is.numeric(.) is not true.\n\nscale   - either a logical value or a numeric-alike vector of length equal to the\nnumber of columns of x.\nThe center refers to \\(x - \\mu\\) from the equation above and scale refers to dividing by \\(\\sigma\\). We will set both to be true to be a z-score.\n\n\nscaled_mat <- scale(names_mat, scale = TRUE, center = TRUE)\n\n\n\n\npheatmap(scaled_mat)\n\n\n\nNotice how we again can see more structure in the data.\nThe scale shows both negative and positive values. Negative values had counts less than the mean, positive values had counts above the mean, and 0 means the value was equal to the mean. An important note here, values that are negative do not mean they were zero. For example, the scaled value for “Grayson” in California is negative:\n\n\nscaled_mat[\"Grayson\", \"California\"]\n\n[1] -1.043828\n\nbut there are still several hundred individuals named “Grayson” in California.\n\n\nnames_mat[\"Grayson\", \"California\"]\n\n[1] 453\n\nNote if you want a z-score of gene expression data, you will want to center and scale by the genes not the samples. Most gene expression data is stored as gene x sample so you will likely need to transform the matrix before scaling:\nscaled_mat <- t(scale(t(unscaled_mat), scale = TRUE, center = TRUE))\nExercise\nLooking at the three heatmaps, what is the most popular name? What heatmap is easiest to interpret?\nChanging the color palette\nThe default color palette for pheatmap isn’t always the color palette you want. One color palette I like is the magma color from the viridis package. To use this package you can just call any of the color functions. and provide the number of colors you want in the palette. We will use the magma function below.\n\n\npheatmap(scaled_mat, color = magma(100))\n\n\n\nYou can also add custom color palettes using colorRampPalette. This creates a function to generate a color palette with your colors for any number of values:\n\n\ncolor_function <- colorRampPalette(c(\"navy\", \"white\", \"red\"))\ncolor_function\n\nfunction (n) \n{\n    x <- ramp(seq.int(0, 1, length.out = n))\n    if (ncol(x) == 4L) \n        rgb(x[, 1L], x[, 2L], x[, 3L], x[, 4L], maxColorValue = 255)\n    else rgb(x[, 1L], x[, 2L], x[, 3L], maxColorValue = 255)\n}\n<bytecode: 0x7fa771faaaa0>\n<environment: 0x7fa71300c4a8>\n\n\n\ncolor_function(10)\n\n [1] \"#000080\" \"#38389C\" \"#7171B8\" \"#AAAAD4\" \"#E2E2F0\" \"#FFE2E2\"\n [7] \"#FFAAAA\" \"#FF7171\" \"#FF3838\" \"#FF0000\"\n\n\n\npheatmap(scaled_mat, color = color_function(100))\n\n\n\nYou can also run this in one line of code and not save the function\n\n\npheatmap(scaled_mat, color = colorRampPalette(c(\"navy\", \"white\", \"red\"))(100))\n\n\n\nYou can even use custom color palettes with any HEX color codes you want. A personal favorite of mine is a blue/yellow palette from the ArchR package.\n\n\nblueYellow <- c(\"#352A86\", \"#343DAE\", \"#0262E0\", \"#1389D2\", \"#2DB7A3\",\n                  \"#A5BE6A\", \"#F8BA43\", \"#F6DA23\", \"#F8FA0D\")\n\npheatmap(scaled_mat, color = blueYellow)\n\n\n\nExercise\nMake a heatmap with your own color palette\n\n\n# TODO make the heatmap with your unique color palette\n\n\nAdding annotations\nWe can help interpretation by adding row and column annotations to the plot. Let’s first add some row annotations showing if the name is a male or a female name. We start by making a data frame of the names and indicating if the name was from the male or female database.\n\n\nnames_annotation <- data.frame(\"sex\" = c(rep(\"female\", nrow(female_names_mat)),\n                                         rep(\"male\", nrow(male_names_mat))),\n                               row.names = c(rownames(female_names_mat),\n                                             rownames(male_names_mat)))\n\nhead(names_annotation)\n\n             sex\nAva       female\nOlivia    female\nEmma      female\nCharlotte female\nHarper    female\nAmelia    female\n\nNote here the row names are the same as the row names of our matrix and we have one column named “sex”\nWe can now add this to the plot using the annotation_row. The key here is that the row names of your matrix must be the same as the row names of your data frame.\n\n\nblueYellow <- c(\"#352A86\", \"#343DAE\", \"#0262E0\", \"#1389D2\", \"#2DB7A3\",\n                  \"#A5BE6A\", \"#F8BA43\", \"#F6DA23\", \"#F8FA0D\")\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_row = names_annotation)\n\n\n\nNow you can see very clearly what names were in the female database and what names were in the male database. Again, we can change the default colors by using a list object. Here the names of the list must match the column names in your data frame and the names of the colors in the list must match the levels of that column.\nWe can either use the name of colors:\n\n\nall_colors <- list(\"sex\" = c(\"male\" = \"red\", \"female\" = \"yellow\"))\n\nall_colors\n\n$sex\n    male   female \n   \"red\" \"yellow\" \n\n\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_row = names_annotation,\n         annotation_colors = all_colors)\n\n\n\nOr a HEX code:\n\n\nall_colors <- list(\"sex\" = c(\"male\" = \"#B067A3\", \"female\" = \"#9C954D\"))\n\nall_colors\n\n$sex\n     male    female \n\"#B067A3\" \"#9C954D\" \n\n\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_row = names_annotation,\n         annotation_colors = all_colors)\n\n\n\nExercise\nAdd your own colors to the row annotations\n\n\n# TODO Add your own row annotaiton colors\n\n\nWe can also add annotations to the columns using the same approach. Let’s load in some of the data I’ve compiled for the states\n\n\nstate_info <- read.csv(here(\"class_7-9_data\", \"state_info.csv\"))\n\nhead(state_info)\n\n       State Location   Size Density\n1    Alabama    South Middle       4\n2     Alaska     West  Small       6\n3    Arizona     West Middle       4\n4   Arkansas    South Middle       5\n5 California     West  Large       2\n6   Colorado     West Middle       5\n\nThis table has information about the location, the size (based on population as small middle or large) and the density (on a scale of 1-6).\nWe first will need to reformat so that the row names are the sates. Now the row names must be the same as the column names in the matrix.\n\n\nstate_info <- tibble::column_to_rownames(state_info, \"State\")\n\n\nhead(state_info)\n\n           Location   Size Density\nAlabama       South Middle       4\nAlaska         West  Small       6\nArizona        West Middle       4\nArkansas      South Middle       5\nCalifornia     West  Large       2\nColorado       West Middle       5\n\nWe can now pass this data frame to annotation_col in pheatmap\n\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_col = state_info)\n\n\n\nYou can see that we now have three levels of annotation on our heatmap. One thing to notice is that our population density is given as a scale of colors because it is seen as a numeric score. If we instead use this as a factor, this scale will go away:\n\n\nstate_info$Density <- factor(state_info$Density)\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_col = state_info)\n\n\n\nAs before, we can add in our own color palettes by using a named list where the list names correspond to the column names of our data frame. Here, I’m going to use the package MetBrewer to generate palettes for me. You can see all possible palette options here\n\n\nlocation_colors <- met.brewer(\"Pissaro\",\n                              n = length(unique(state_info$Location))) %>%\n  as.character()\n\nnames(location_colors) <- unique(state_info$Location)\n\nlocation_colors\n\n      South        West New_England     Midwest \n  \"#4c825d\"   \"#8cae9e\"   \"#8dc7dc\"   \"#0e2a4d\" \n\nsize_colors <- met.brewer(\"Navajo\",\n                          n = length(unique(state_info$Size))) %>%\n  as.character()\n\nnames(size_colors) <- unique(state_info$Size)\n\nsize_colors\n\n   Middle     Small     Large \n\"#660d20\" \"#e59a52\" \"#edce79\" \n\ndensity_colors <- met.brewer(\"Juarez\",\n                             n = length(levels(state_info$Density))) %>%\n  as.character()\n\nnames(density_colors) <- levels(state_info$Density)\n\ndensity_colors\n\n        1         2         3         4         5         6 \n\"#a82203\" \"#208cc0\" \"#f1af3a\" \"#cf5e4e\" \"#637b31\" \"#003967\" \n\nall_colors <- list(\"Location\" = location_colors,\n                   \"Size\" = size_colors,\n                   \"Density\" = density_colors)\n\nall_colors\n\n$Location\n      South        West New_England     Midwest \n  \"#4c825d\"   \"#8cae9e\"   \"#8dc7dc\"   \"#0e2a4d\" \n\n$Size\n   Middle     Small     Large \n\"#660d20\" \"#e59a52\" \"#edce79\" \n\n$Density\n        1         2         3         4         5         6 \n\"#a82203\" \"#208cc0\" \"#f1af3a\" \"#cf5e4e\" \"#637b31\" \"#003967\" \n\n\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_col = state_info,\n         annotation_colors = all_colors)\n\n\n\nFinally, we can add in the annotation from the rows as well. First we can add in the colors we had before\n\n\nall_colors <- c(all_colors, \n                list(\"sex\" = c(\"male\" = \"#B067A3\", \"female\" = \"#9C954D\")))\n\nall_colors\n\n$Location\n      South        West New_England     Midwest \n  \"#4c825d\"   \"#8cae9e\"   \"#8dc7dc\"   \"#0e2a4d\" \n\n$Size\n   Middle     Small     Large \n\"#660d20\" \"#e59a52\" \"#edce79\" \n\n$Density\n        1         2         3         4         5         6 \n\"#a82203\" \"#208cc0\" \"#f1af3a\" \"#cf5e4e\" \"#637b31\" \"#003967\" \n\n$sex\n     male    female \n\"#B067A3\" \"#9C954D\" \n\nAnd then add the names_annotation back into the argument for annotation_row\n\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_col = state_info,\n         annotation_row = names_annotation,\n         annotation_colors = all_colors)\n\n\n\nExercise\nWhat happens if all_colors only has some of the colors in the annotation data frames?\n\n\n# TODO try making the heatmap with a list that only contains color arguments for\n# some of the columns in the annotation data frames\n\n\nNotice how the annotation_colors argument accepts the color arguments for both the rows and the columns. The important things about this argument to color the annotation is\nIt must be a list\nThe names of the list must be in the columns for the annotation_row or annotation_col data frame\nThe names of the colors must match the values in the matching column\nYou do not need to have all columns present in your color list - any missing columns will be given the default colors\nThere are also a few important aspects of the annotation data frames\n1. The rownames of annotation_row must match the rownames of the matrix (although the order does not need to be the same)\n2. The rownames of annotation_col must match the column names of the matrix (although the order does not need to be the same)\n3. You can add as many annotations to the rows and columns that you want. You just need to include these as columns in either the annotation_row or annotation_col data frame.\nClustering\nOne aspect of the plots that you’ve probably noticed is dendrograms for both the rows and the columns. The clustering is done using hclustlike we did in the clustering lecture.\nJust like with our clustering methods we can cut the dendrogram based on an expected number of clusters to group either the states or the names. pheatmap has a function that uses cutree to identify clusters and physically separates these clusters with a white space using cutree_rows and cutree_cols. First, we can visualize three clusters of the names.\n\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_col = state_info,\n         annotation_row = names_annotation,\n         annotation_colors = all_colors,\n         cutree_rows = 3)\n\n\n\nOr we can visualize three clusters of the states\n\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_col = state_info,\n         annotation_row = names_annotation,\n         annotation_colors = all_colors,\n         cutree_cols = 3)\n\n\n\nWe could even visuzalize both at once\n\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_col = state_info,\n         annotation_row = names_annotation,\n         annotation_colors = all_colors,\n         cutree_rows = 3,\n         cutree_cols = 3)\n\n\n\nExercise\nTry making the heatmap with different numbers of clusters. What number of clusters makes the most sense?\n\n\n# TODO Make the heatmap cutting to make different numbers of clusters\n\n\nAs with using cutree ourselves, you can pick any number of clusters to visualize in this way.\nIn addition to visualizing these clusters, we can also pull the clustering information out of the heatmap object. First we can save the plot to a variable. Right now we are setting silent to TRUE so the heatmap isn’t drawn\n\n\nheatmap <- pheatmap(scaled_mat,\n                    color = blueYellow,\n                    annotation_col = state_info,\n                    annotation_row = names_annotation,\n                    annotation_colors = all_colors,\n                    cutree_rows = 3,\n                    cutree_cols = 3,\n                    silent = TRUE)\n\nnames(heatmap)\n\n[1] \"tree_row\" \"tree_col\" \"kmeans\"   \"gtable\"  \n\nThe hclust object for the row are in tree_row and the hclust object for the column are in tree_col\nWe can plot just the dendrogram as we did previously\n\n\nplot(heatmap$tree_col)\n\n\n\n\n\nplot(heatmap$tree_row)\n\n\n\nWe can also now use cutree on these hclust objects in the exact same way we did in the clustering lecture to pull out clusters\nColumns:\n\n\ncutree(tree = heatmap$tree_col, k = 3)\n\n    Alabama      Alaska     Arizona    Arkansas  California \n          1           2           3           1           3 \n   Colorado Connecticut    Delaware     Florida     Georgia \n          2           3           3           3           1 \n     Hawaii       Idaho    Illinois     Indiana        Iowa \n          3           2           3           2           2 \n     Kansas    Kentucky   Louisiana       Maine    Maryland \n          2           1           1           2           3 \n\nRows:\n\n\ncutree(tree = heatmap$tree_row, k = 3)\n\n  William     James      John    Elijah      Noah      Liam     Mason \n        1         1         2         1         1         1         2 \n   Oliver     Henry   Jackson    Samuel     Jaxon     Asher   Grayson \n        1         2         2         3         3         2         2 \n     Levi   Michael    Carter  Benjamin   Charles     Wyatt    Thomas \n        2         2         2         2         3         2         3 \n    Aiden      Luke     David      Owen    Daniel     Logan    Joseph \n        3         3         3         2         2         2         3 \n    Lucas    Joshua      Jack Alexander  Maverick   Gabriel     Ethan \n        2         3         2         2         3         3         2 \n      Eli     Isaac    Hunter      Ezra  Theodore       Ava    Olivia \n        3         3         3         3         2         1         1 \n     Emma Charlotte    Harper    Amelia Elizabeth    Evelyn  Isabella \n        1         1         2         1         3         2         2 \n     Ella     Avery   Abigail    Sophia     Layla       Mia   Madison \n        3         3         3         2         3         2         3 \n     Lily     Ellie      Nova   Eleanor      Zoey  Brooklyn     Riley \n        3         3         3         3         3         3         3 \n     Nora      Aria      Mila    Stella   Natalie      Luna  Penelope \n        3         3         3         3         3         3         3 \n   Aurora    Claire \n        3         3 \n\nExercise\nWhat names are the most similar? What states are the most similar?\nAnother option is to remove the clustering all together using cluster_rows and cluster_cols\n\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_col = state_info,\n         annotation_row = names_annotation,\n         annotation_colors = all_colors,\n         cluster_rows = FALSE,\n         cluster_cols = FALSE)\n\n\n\nIf we don’t cluster, the order of the rows and columns is taken directly from the order of the matrix. We can change to alphabetical order as follows\n\n\nscaled_mat_ord <- scaled_mat[order(rownames(scaled_mat)), ]\n\npheatmap(scaled_mat_ord,\n         color = blueYellow,\n         annotation_col = state_info,\n         annotation_row = names_annotation,\n         annotation_colors = all_colors,\n         cluster_rows = FALSE,\n         cluster_cols = FALSE)\n\n\n\nWe could even order the states by the density\n\n\nstate_info <- state_info %>%\n  dplyr::arrange(Density)\n\nscaled_mat_ord <- scaled_mat[ , order(match(colnames(scaled_mat),\n                                            rownames(state_info)))]\n\npheatmap(scaled_mat_ord,\n         color = blueYellow,\n         annotation_col = state_info,\n         annotation_row = names_annotation,\n         annotation_colors = all_colors,\n         cluster_rows = FALSE,\n         cluster_cols = FALSE)\n\n\n\nExercise\nOrder the heatmap by density, but cluster the names\n\n\n# TODO order the heatmap by density but cluster the names\n\n\nExercise\nOrder the heatmap by location, but cluster the names\n\n\n# TODO order the heatmap by location and cluster the names\n\n\nRemove column and row names\nOne other helpful way to adjust your heatmap is to remove the row and column names. You can control if the row and column names are seen using show_rownames and show_colnames\n\n\npheatmap(scaled_mat,\n         color = blueYellow,\n         annotation_col = state_info,\n         annotation_row = names_annotation,\n         annotation_colors = all_colors,\n         show_rownames = FALSE,\n         show_colnames = FALSE)\n\n\n\nOther aesthetics\nThere are many possible adjustments you can make, we’ve just gone through the adjustments I use the most often. To see all possible arguments see ?pheatmap\nAcknowldgements and additional references\nThe content of this class borrows heavily from previous tutorials:\nmaking heatmaps\nI’m a biologist, why should I care?\nHeatmaps are common tools to visualize data\nThey are frequently used for sequencing data - RNA-seq, scRNA-seq, ATAC-seq\nThey are a good way of showing lots of data\nBecause of the scaling, they can be misleading so it’s good to know how to interpret them\n\nYou will likely encounter heatmaps in papers that you read. My goal is to improve your understanding of the techniques so you can better interpret them on your own\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-11-16-class-9-heatmap/class-9-heatmap_files/figure-html5/unnamed-chunk-31-1.png",
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-15-class-8-clustering/",
    "title": "Class 8: Introduction to clustering and PCA",
    "description": {},
    "author": [
      {
        "name": "Kristen Wells",
        "url": "https://github.com/kwells4"
      }
    ],
    "date": "2022-12-08",
    "categories": [],
    "contents": "\n\nContents\nGoals for this class\nLoad packages\nDownload files\nWhat is clustering\nK - means clustering\nHierarchical clustering\n\nUsing dimensionality reduction to visualize data\nVisualizing the data with PCA\n\nImplementing clustering on our dataset\nK means clustering of names\nHierarchical clustering of names\nRepeating by clustering states\nHierarchical clustering of states\n\nI’m a biologist, why should I care?\nAcknowldgements and additional references\n\nThe Rmarkdown for this class is on github\nGoals for this class\nLearn about different types of clustering\nCluster data using these different approaches\nLearn how to use dimensionality reduction to visualize complex data\nLoad packages\n\n\nlibrary(tidyverse)\n# install factoextra if needed\n# install.packages(\"factoextra\")\nlibrary(factoextra)\nlibrary(here)\n\n\nDownload files\nBefore we get started, let’s download all of the files you will need for the next three classes.\n\n\n# conditionally download all of the files used in rmarkdown from github \nsource(\"https://raw.githubusercontent.com/rnabioco/bmsc-7810-pbda/main/_posts/2022-11-10-class-7-matricies/download_files.R\")\n\n\nWhat is clustering\nClustering is grouping together similar objects or elements. Clustering is a very important part of any type of data analysis.\nClustering is important for grouping important data togeher - it is what powers those “for you” posts you get when clicking on news articles\nCluster is also important for biology, it helps us group samples together in RNA-seq or in any experiment where you can get many measurements for samples, cells together in single cell methods\nIt can also be used to group together states or districts that are likely to vote in similar ways\nClustering is an unsupervised task - this means that it tries to group together items with no previous knowledge\nK - means clustering\nI’m loosely following the tutorial here through this section. Click on the above link for a more in-depth discussion.\nk-means clustering uses the distance between points as a measure of similarity. The basic idea is that we are adding k points to the data, called centroids that will try to center themselves in the middle of k clusters. This means that the value of k - set by the user, is a very important value to set.\nConsider the dataset below\n\n\n\nIn the figure above you would probably say that there are 3 clear clusters. Let’s start by giving a k of 3.\nFirst the centroids are placed randomly and the distance from each point to each centroid is calculated. Points are then assigned to the cluster that corresponds to the nearest centroid: if a point is closest to centroid 1, it is placed into cluster 1.\n\n\n\nNext, the mean distance from all points belonging to a cluster to that cluser’s centroid is calculated.\nThen, the “centroids” are moved to that mean value that was calculated and the process of assigning points and finding the mean value is repeated.\nThis is done until the distance between the centroid and the nearest points doesn’t change. All points that are closest to the centroid are placed in that cluster\n\n\n\nLet’s try clustering on our own example:\n\n\ntheme_set(theme_classic(base_size = 10))\n\ntest_data <- data.frame(\"x_val\" = c(1, 3, 10, 12, 11,\n                                    13, 11, 3, 2, 15,\n                                    9, 2, 13, 12, 11),\n                        \"y_val\" = c(10, 12, 3, 2, 10,\n                                    3, 1, 10, 13, 2,\n                                    1, 13, 12, 13, 11))\n\nrownames(test_data) <- letters[1:nrow(test_data)]\n\n\nLet’s quickly plot it to get a feeling of what it looks like\n\n\nggplot(test_data, aes(x = x_val, y = y_val)) +\n  ggplot2::geom_point(size = 2)\n\n\n\nHere it looks like we have three clusters. Let’s try running the k means clustering algorithm. We will be using kmeans. The centers corresponds to the number of clusters we expect in the data. The nstart corresponds to the number of random starting points to try (like the randomly placed centroids above).\n\n\nset.seed(123)\nkm.res <- kmeans(test_data, centers = 3, nstart = 25)\n\n\nThe return of this function is a list. To figure out the components returned we can use names()\n\n\nnames(km.res)\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"    \n[5] \"tot.withinss\" \"betweenss\"    \"size\"         \"iter\"        \n[9] \"ifault\"      \n\nThe actually cluster information is in km.res$cluster, the position of the centroids is in km.res$centers\n\n\nkm.res$cluster\n\na b c d e f g h i j k l m n o \n1 1 3 3 2 3 3 1 1 3 3 1 2 2 2 \n\nkm.res$centers\n\n     x_val y_val\n1  2.20000  11.6\n2 11.75000  11.5\n3 11.66667   2.0\n\nLet’s add our clustering information to our data frame\n\n\ncluster_k3 <- cbind(test_data, \"cluster\" = km.res$cluster)\n\n\nAnd replot with the new clusters\n\n\ncluster_k3$cluster <- factor(cluster_k3$cluster)\nggplot(cluster_k3, aes(x = x_val, y = y_val, color = cluster)) +\n  ggplot2::geom_point(size = 2)\n\n\n\nThis looks like it did a pretty good job! What if we repeat with different values for k?\nK = 2\n\n\nset.seed(123)\nkm.res2 <- kmeans(test_data, centers = 2, nstart = 25)\n\ncluster_k2 <- cbind(test_data, \"cluster\" = km.res2$cluster)\n\ncluster_k2$cluster <- factor(cluster_k2$cluster)\nggplot(cluster_k2, aes(x = x_val, y = y_val, color = cluster)) +\n  ggplot2::geom_point(size = 2)\n\n\n\nNow two populations that appear visually different have been clumped together because we allowed for only 2 centroids.\nExercise\nRepeat the process above, but compute for 4 clusters\n\n\n# TODO find k = 4 clusters\n\n\nNote, the nstart value and the set seed is very important. If we don’t set a seed and only do one start, we end up with very different results when running several times:\n\n\nkm.res <- kmeans(test_data, centers = 3, nstart = 1)\ncluster_k3 <- cbind(test_data, \"cluster\" = km.res$cluster)\ncluster_k3$cluster <- factor(cluster_k3$cluster)\nggplot(cluster_k3, aes(x = x_val, y = y_val, color = cluster)) +\n  ggplot2::geom_point(size = 2)\n\n\n\n\n\nkm.res <- kmeans(test_data, centers = 3, nstart = 1)\ncluster_k3 <- cbind(test_data, \"cluster\" = km.res$cluster)\ncluster_k3$cluster <- factor(cluster_k3$cluster)\nggplot(cluster_k3, aes(x = x_val, y = y_val, color = cluster)) +\n  ggplot2::geom_point(size = 2)\n\n\n\n\n\nkm.res <- kmeans(test_data, centers = 3, nstart = 1)\ncluster_k3 <- cbind(test_data, \"cluster\" = km.res$cluster)\ncluster_k3$cluster <- factor(cluster_k3$cluster)\nggplot(cluster_k3, aes(x = x_val, y = y_val, color = cluster)) +\n  ggplot2::geom_point(size = 2)\n\n\n\n\n\nkm.res <- kmeans(test_data, centers = 3, nstart = 1)\ncluster_k3 <- cbind(test_data, \"cluster\" = km.res$cluster)\ncluster_k3$cluster <- factor(cluster_k3$cluster)\nggplot(cluster_k3, aes(x = x_val, y = y_val, color = cluster)) +\n  ggplot2::geom_point(size = 2)\n\n\n\nHierarchical clustering\nAnother option for clustering is using hierarchical clustering. This is another popular method and results in a dendrogram which you’ve likely seen before.\nI’m loosely following the tutorial here through this section. Click on the above link for a more in-depth discussion.\nBasically, hierarchical clustering starts by treating each sample as a “cluster”. It then does the following steps\nIdentify the two clusters that are closest together\nMerge these two clusters to form a new cluster\nSteps 1 and two are repeated until only one large cluster remains\n\n\n\nEach merging step is used to build a dendrogram\n\n\n\nTo get an idea of how this is working, we can use the same toy data we generated above\n\n\ntest_data <- data.frame(\"x_val\" = c(1, 3, 10, 12, 11,\n                                    13, 11, 3, 2, 15,\n                                    9, 2, 13, 12, 11),\n                        \"y_val\" = c(10, 12, 3, 2, 10,\n                                    3, 1, 10, 13, 2,\n                                    1, 13, 12, 13, 11))\n\nrownames(test_data) <- letters[1:nrow(test_data)]\n\ntest_data %>%\n  dplyr::mutate(sample = rownames(.)) %>%\n  ggplot(aes(x = x_val, y = y_val, label = sample)) +\n    geom_point(size = 2) +\n    geom_text(hjust=2, vjust=0)\n\n\n\nFirst, we should find the distances between the points. Below is the eucledian distances. Notice that the distance is computed for the rows. If we wanted to compute distances for the columns we would need to transform the matrix using t()\n\n\ndist(test_data[1:5,])\n\n          a         b         c         d\nb  2.828427                              \nc 11.401754 11.401754                    \nd 13.601471 13.453624  2.236068          \ne 10.000000  8.246211  7.071068  8.062258\n\nWe can now run hclust which will perform hierarchical clustering using the output of dist\n\n\nhc <- hclust(dist(test_data))\n\nplot(hc)\n\n\n\nBased on the dendrogram above, we can see what points are the most similar (ie were clustered first). We can also see that it makes the most sense to break the data into 3 clusters. We can pull out these clusters using cutree\n\n\nhc_clusters3 <- cutree(tree = hc, k =3)\n\nhc_clusters3\n\na b c d e f g h i j k l m n o \n1 1 2 2 3 2 2 1 1 2 2 1 3 3 3 \n\nhc_clusters3 <- cbind(test_data, \"cluster\" = hc_clusters3)\n\nhc_clusters3$cluster <- factor(hc_clusters3$cluster)\n\nhc_clusters3 %>%\n  dplyr::mutate(sample = rownames(.)) %>%\n  ggplot(aes(x = x_val, y = y_val, label = sample, color = cluster)) +\n    geom_point(size = 2) +\n    geom_text(hjust=2, vjust=0)\n\n\n\nAnd now you can see that the clusters agree exactly with the k - means clusters\nExercise\nRepeat finding clusters using different points on the tree. For example, what if you make 2 clusters? What if you make 4 or 5?\n\n\n# TODO repeat finding clusters with different numbers of clusters\n\n\nQuestion\nAre the clusters always the same as the k-means clusters? Do they ever differ?\nUsing dimensionality reduction to visualize data\nIn the example above, we only had two dimensions and could easily visualize them. What about our example dataset with 50 names and 20 states? How do we visualize that type of data? Even more daunting,what if we have 20,000 genes and 18 samples as part of a RNA-seq experiment, or even worse, 20,000 genes and 30,000 cells as part of a single cell RNA-seq experiment? To visualize this data, we have to use something called dimensionality reduction. PCA is a common dimensionality reduction to use and will work well for lots of data type (like bulk RNA-seq). Other dimensionality reduction tools will need to be used (like UMAP) to visualize single cell RNA-seq and CYTOF datasets (and other single cell approaches).\nVisualizing the data with PCA\nThere is an amazing tutorial on how PCA works here. We will talk a little today about how PCA works, but I highly recommend looking at this book for PCA and to learn many of the important statistical techniques used regularly in computational biology. One thing I like about the book is it includes a lot of R code that you can run yourself so you can really understand how these techniques work.\nThe overall goal of PCA is to find linear models that best fit the data.\nLet’s walk through PCA (following the tutorial in Modern Statistics for Modern Biology) with only Alabama and Colorado to see how it works.\n\n\nnames_mat <- read.csv(here(\"class_7-9_data\", \"boy_name_counts.csv\"),\n                      row.names = 1) %>%\n  as.matrix()\n\n\nnormalized_mat <- t(t(names_mat) / colSums(names_mat))\n\n# Selecting just Alabama and Colorado, making a data frame for plotting\nnormalized_dat <- normalized_mat %>%\n  data.frame %>%\n  dplyr::select(Alabama, Colorado)\n\n\nThe first way we can think about going from 2 dimensions to 1 dimension is to simply just use the values from one of the dimensions. For example, if we have Alabama and Colorado, we can simply describe the data using only the values for Alabama. This idea is shown as the red points below\n\n\ndim_plot <- ggplot(normalized_dat, aes(x = Alabama, y = Colorado)) +\n  geom_point(size = 2)\ndim_plot + geom_point(aes(y = 0), colour = \"red\") +\n  geom_segment(aes(xend = Alabama, yend = 0), linetype = \"dashed\")\n\n\n\nUnfortunately, this loses all of the information from Colorado. Instead, we can also try to find a line of best fit using linear regression. Linear regression is done using a model with response and explanatory variables. A line is fit to the data that attempts to minimize the distance from the response variables to the line. Because there is always a response and explanatory variable, we can perform linear regression to minimize the distance for Colorado or Alabama.\nLet’s first minimize the distance for Colorado. To perform linear regression, we will use the lm function. If we want to minimize the distance to Colorado, Colorado will be the response variable and our model will be Colorado ~ Alabama\n\n\nreg1 <- lm(Colorado ~ Alabama, data = normalized_dat)\nnames(reg1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nAfter running lm the return value is a list. The coefficients will give us the slope and the intercept (think \\(y = ax + b\\) ). We can also use the fitted.values which are the predicted values for Colorado based on the model.\n\n\na1 <- reg1$coefficients[1] # intercept\nb1 <- reg1$coefficients[2] # slope\npline1 <- dim_plot + geom_abline(intercept = a1, slope = b1,\n                                col = \"blue\")\npline1 + geom_segment(aes(xend = Alabama, yend = reg1$fitted.values),\n                      colour = \"red\", arrow = arrow(length = unit(0.15, \"cm\")))\n\n\n\nWe can find the variance of the points on the blue line by using Pythagoras’ theorem (because the values have x and y coordinates).\n\n\nvar(normalized_dat$Alabama) + var(reg1$fitted.values)\n\n[1] 0.0001614616\n\nWe can now repeat linear regression and now minimize the distance for Alabama to the regression line (now Alabama is the response variable).\n\n\nreg2 <- lm(Alabama ~ Colorado, data = normalized_dat)\na2 <- reg2$coefficients[1] # intercept\nb2 <- reg2$coefficients[2] # slope\npline2 <- dim_plot + geom_abline(intercept = -a2/b2, slope = 1/b2,\n                              col = \"darkgreen\")\npline2 + geom_segment(aes(xend=reg2$fitted.values, yend=Colorado),\n                      colour = \"orange\", arrow = arrow(length = unit(0.15, \"cm\")))\n\n\n\nWe can also find the variance of the points that are fit to this regression line\n\n\nvar(normalized_dat$Colorado) + var(reg2$fitted.values)\n\n[1] 0.000115524\n\nSo far we have attempted to minimize either the distance of Alabama or Colorado values from the regression line. Instead of minimizing just one distance, PCA tries to minimize the total distance from the line (in both the X and Y directions).\nLet’s run PCA and use matrix multiplication to visualize the first PC in our x-y space\n\n\nsvda <- svd(as.matrix(normalized_dat))\npc <- as.matrix(normalized_dat) %*% svda$v[, 1] %*% t(svda$v[, 1]) # Matrix multiplication\nbp <- svda$v[2, 1] / svda$v[1, 1]\nap <- mean(pc[, 2]) - bp * mean(pc[, 1])\ndim_plot + geom_segment(xend = pc[, 1], yend = pc[, 2]) +\n  geom_abline(intercept = ap, slope = bp, col = \"purple\", lwd = 1.5)\n\n\n\nWe can now find the variacne of the points in the PC space\n\n\napply(pc, 2, var)\n\n[1] 8.633852e-05 7.907309e-05\n\nsum(apply(pc, 2, var))\n\n[1] 0.0001654116\n\nNotice that this variance is the largest of the three lines we have fit.\nFrom Modern Stats for Modern Biology\n\nIf we are minimizing in both horizontal and vertical directions we are in fact minimizing the orthogonal projections onto the line from each point.\nThe total variability of the points is measured by the sum of squares ofthe projection of the points onto the center of gravity, which is the origin (0,0) if the data are centered. This is called the total variance or the inertia of the point cloud. This inertia can be decomposed into the sum of the squares of the projections onto the line plus the variances along that line. For a fixed variance, minimizing the projection distances also maximizes the variance along that line. Often we define the first principal component as the line with maximum variance.\n\nAlthough it’s good to know the inner workings of PCA, we can simply run this PCA analysis using prcomp.\nNow that we have learned about some of the clustering techniques, we are going to continue working with the same data we used to talk about matrices, the top 100 boy and girl names by state for 2020. We will want to use the normalized data for these examples.\n\n\nnames_mat <- read.csv(here(\"class_7-9_data\", \"boy_name_counts.csv\"),\n                      row.names = 1) %>%\n  as.matrix()\n\n\nnormalized_mat <- t(t(names_mat) / colSums(names_mat))\n\n\n\n\nnames.pca <- prcomp(normalized_mat)\n\n\nThe values for the dimensionality reduction are found in the x slot.\n\n\npca_vals <- names.pca$x %>%\n  data.frame()\n\npca_vals[1:4, 1:4]\n\n                 PC1           PC2         PC3         PC4\nWilliam -0.047369020  0.0205532201 -0.03357595 -0.01476594\nJames   -0.039425249  0.0051117061 -0.02086795 -0.01309033\nJohn     0.008912962 -0.0008331344 -0.03046000 -0.01522468\nElijah  -0.052513118 -0.0058489913 -0.02466740  0.01209649\n\nWe can also find the percent of variance explained by each component using the sdev slot\n\n\nnames.pca$sdev[1:5]\n\n[1] 0.036137672 0.016696954 0.013635319 0.008543108 0.005674905\n\npercentVar <- names.pca$sdev^2 / sum( names.pca$sdev^2 )\n\n\nNow we can combine the two to make a PCA plot\n\n\nggplot(pca_vals, aes(x = PC1, y = PC2)) +\n  geom_point(size = 2) +\n  xlab(paste0(\"PC1: \",round(percentVar[1] * 100),\"% variance\")) +\n  ylab(paste0(\"PC2: \",round(percentVar[2] * 100),\"% variance\"))\n\n\n\nImplementing clustering on our dataset\nNow that we’ve learned how to visualize our data, we will be able to more easily see the clustering results. Let’s put together everything we’ve learned so far and try out the different clustering methods on our names data.\nK means clustering of names\nThe first thing we need to do is decide if we want to cluster the states or the names. Our first question is what names have the most similar usage? To answer this question, we will need to cluster the names.\nFrom the PCA plot above, it doesn’t look like there are strong clusters. Let’s start with 2 clusters.\n\n\nset.seed(123)\nkm.res2 <- kmeans(normalized_mat, centers = 2, nstart = 25)\nkm.res2$cluster\n\n  William     James      John    Elijah      Noah      Liam     Mason \n        1         1         2         1         1         1         2 \n   Oliver     Henry   Jackson    Samuel     Jaxon     Asher   Grayson \n        1         1         2         2         2         2         2 \n     Levi   Michael    Carter  Benjamin   Charles     Wyatt    Thomas \n        2         2         2         1         2         2         2 \n    Aiden      Luke     David      Owen    Daniel     Logan    Joseph \n        2         2         2         2         2         2         2 \n    Lucas    Joshua      Jack Alexander  Maverick   Gabriel     Ethan \n        2         2         2         2         2         2         2 \n      Eli     Isaac    Hunter      Ezra  Theodore \n        2         2         2         2         2 \n\nWe can again add this information to the plot\n\n\npca_cluster <- cbind(pca_vals, \"cluster\" = km.res2$cluster)\n\npca_cluster$cluster <- factor(pca_cluster$cluster)\nggplot(pca_cluster, aes(x = PC1, y = PC2, color = cluster)) +\n  geom_point(size = 2) +\n  xlab(paste0(\"PC1: \",round(percentVar[1] * 100),\"% variance\")) +\n  ylab(paste0(\"PC2: \",round(percentVar[2] * 100),\"% variance\"))\n\n\n\nLet’s try again with 3 clusters\n\n\nset.seed(123)\nkm.res3 <- kmeans(normalized_mat, centers = 3, nstart = 25)\n\npca_cluster <- cbind(pca_vals, \"cluster\" = km.res3$cluster)\n\npca_cluster$cluster <- factor(pca_cluster$cluster)\nggplot(pca_cluster, aes(x = PC1, y = PC2, color = cluster)) +\n  geom_point(size = 2) +\n  xlab(paste0(\"PC1: \",round(percentVar[1] * 100),\"% variance\")) +\n  ylab(paste0(\"PC2: \",round(percentVar[2] * 100),\"% variance\"))\n\n\n\nExercise\nTry with 4 clusters, what names cluster together?\n\n\n# TODO Repeat with 4 clusters and repeat what names cluster together\n\n\nHierarchical clustering of names\nNow let’s repeat the clustering of names using hierarchical clustering\n\n\nhc <- hclust(dist(normalized_mat))\n\nplot(hc)\n\n\n\nBased on the above plot, it seems like 3 clusters might be a good starting point\n\n\nhc_clusters3 <- cutree(tree = hc, k =3)\n\n\nhc_clusters3 <- cbind(pca_vals, \"cluster\" = hc_clusters3)\n\nhc_clusters3$cluster <- factor(hc_clusters3$cluster)\n\nggplot(hc_clusters3, aes(x = PC1, y = PC2, color = cluster)) +\n  geom_point(size = 2) +\n  xlab(paste0(\"PC1: \",round(percentVar[1] * 100),\"% variance\")) +\n  ylab(paste0(\"PC2: \",round(percentVar[2] * 100),\"% variance\"))\n\n\n\nExercise\nWhat two values are their own cluster? Hint either remake the plot with the points labeled or look at the dendrogram above.\nRepeating by clustering states\nWhat if our question is what states have the most similar name usage? We will answer this by instead clustering on the states.\nAgain we will start by making a PCA plot. This time we will need to transform the matrix first before running PCA to run it on the sates instead\n\n\nstates_pca <- prcomp(t(normalized_mat))\n\n\nWe can again pull out the values for the PCA from the x slot and the percent of the variance explained by each PC by using the sdev slot.\n\n\npca_vals <- states_pca$x %>%\n  data.frame()\n\npercentVar <- states_pca$sdev^2 / sum( states_pca$sdev^2 )\n\n\nNow we can combine the two to make a PCA plot\n\n\nggplot(pca_vals, aes(x = PC1, y = PC2)) +\n  geom_point(size = 2) +\n  xlab(paste0(\"PC1: \",round(percentVar[1] * 100),\"% variance\")) +\n  ylab(paste0(\"PC2: \",round(percentVar[2] * 100),\"% variance\"))\n\n\n\nWe can now start by looking at 3 clusters using the kmeans clustering algorithm. Again, we will need to first transform the data to cluster the states rather than the names\n\n\nset.seed(123)\nkm.res3 <- kmeans(t(normalized_mat), centers = 3, nstart = 25)\nkm.res3$cluster\n\n    Alabama      Alaska     Arizona    Arkansas  California \n          1           2           3           1           3 \n   Colorado Connecticut    Delaware     Florida     Georgia \n          2           3           3           3           1 \n     Hawaii       Idaho    Illinois     Indiana        Iowa \n          3           2           3           2           2 \n     Kansas    Kentucky   Louisiana       Maine    Maryland \n          2           1           1           2           3 \n\nNotice now we are clustering by the state instead of the names.\n\n\npca_cluster <- cbind(pca_vals, \"cluster\" = km.res3$cluster)\n\npca_cluster$cluster <- factor(pca_cluster$cluster)\nggplot(pca_cluster, aes(x = PC1, y = PC2, color = cluster)) +\n  geom_point(size = 2) +\n  xlab(paste0(\"PC1: \",round(percentVar[1] * 100),\"% variance\")) +\n  ylab(paste0(\"PC2: \",round(percentVar[2] * 100),\"% variance\"))\n\n\n\nThis seems to pretty nicely segregate our data.\nHierarchical clustering of states\nNow let’s repeat the clustering of names using hierarchical clustering, again we will need to transform the data first.\n\n\nhc <- hclust(dist(t(normalized_mat)))\n\nplot(hc)\n\n\n\nIn the plot above we see 3 clear clusters\n\n\nhc_clusters3 <- cutree(tree = hc, k =3)\n\n\nhc_clusters3 <- cbind(pca_vals, \"cluster\" = hc_clusters3)\n\nhc_clusters3$cluster <- factor(hc_clusters3$cluster)\n\nggplot(hc_clusters3, aes(x = PC1, y = PC2, color = cluster)) +\n  geom_point(size = 2) +\n  xlab(paste0(\"PC1: \",round(percentVar[1] * 100),\"% variance\")) +\n  ylab(paste0(\"PC2: \",round(percentVar[2] * 100),\"% variance\"))\n\n\n\nExercise\nTry out several different cluster numbers for both the kmeans clustering and the hierarchical clustering. Do you ever find cases where they differ? What if you try different numbers of starts for kmeans?\n\n\n# TODO test out several hierarchical and k means clustering cluster numbers\n\n\nI’m a biologist, why should I care?\nClustering and visualizations are important for any question you are answering\nClustering and dimensionality reduction are key aspects of single cell RNA-seq analysis\nCYTOF also relies on clustering and dimensionality reduction techniques\nEven a project where you’ve taken several measurements (that you expect to be related) can be used to cluster your samples\nPhylogenetic trees are just a form of hierarchical clustering\n\nYou will likely encounter clustering in papers that you read. My goal is to improve your understanding of the techniques so you can better interpret them on your own\nAcknowldgements and additional references\nThe content of this class borrows heavily from previous tutorials:\nK- means clustering\nBackground\nVideo\nR example\n\nHierarchical clustering\nBackground\nR example\n\nPCAs\nTutorial\n\n",
    "preview": "posts/2022-11-15-class-8-clustering/class-8-clustering_files/figure-html5/unnamed-chunk-44-1.png",
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-10-class-7-matricies/",
    "title": "Class 7: Introduction to matricies",
    "description": {},
    "author": [
      {
        "name": "Kristen Wells",
        "url": "https://github.com/kwells4"
      }
    ],
    "date": "2022-12-07",
    "categories": [],
    "contents": "\n\nContents\nGoals for this class\nLoad packages\nDownload files\nWhat is a matrix?\nMatrix opearions\nBasic operations\nMatrix multiplication\n\nPerforming functions on a matrix\nBasic functions\nSummary functions\nTransposition\nStatistical tests\n\nUsing dataframes and matricies\nI’m a biologist, why should I care?\nAcknowldgements and additional references\n\nThe Rmarkdown for this class is on github\nGoals for this class\nLearn what is a matrix\nDescribe difference between matrix and data frame\nPerform mathematical functions\nConvert between matrix and data frames\nLoad packages\n\n\nlibrary(tidyverse)\nlibrary(here)\n\n\nDownload files\nBefore we get started, let’s download all of the files you will need for the next three classes.\n\n\n# conditionally download all of the files used in rmarkdown from github \nsource(\"https://raw.githubusercontent.com/rnabioco/bmsc-7810-pbda/main/_posts/2022-11-10-class-7-matricies/download_files.R\")\n\n\nWhat is a matrix?\nA Matrix is an 2 dimensional object in R. We create a matrix using the matrix function\n\n\nM <- matrix(c(10:21), nrow = 4, byrow = TRUE)\nM\n\n     [,1] [,2] [,3]\n[1,]   10   11   12\n[2,]   13   14   15\n[3,]   16   17   18\n[4,]   19   20   21\n\nWe can also use as.matrix on an existing dataframe\n\n\ndf <- data.frame(\"A\" = c(10:13), \"B\" = c(14:17), \"C\" = (18:21))\ndf\n\n   A  B  C\n1 10 14 18\n2 11 15 19\n3 12 16 20\n4 13 17 21\n\n\n\nnew_mat <- as.matrix(df)\nnew_mat\n\n      A  B  C\n[1,] 10 14 18\n[2,] 11 15 19\n[3,] 12 16 20\n[4,] 13 17 21\n\nJust like data frames, we can name the rows and columns of the Matrix\n\n\nrownames(new_mat) <- c(\"first\", \"second\", \"third\", \"forth\")\ncolnames(new_mat) <- c(\"D\", \"E\", \"F\")\n\nnew_mat\n\n        D  E  F\nfirst  10 14 18\nsecond 11 15 19\nthird  12 16 20\nforth  13 17 21\n\nWe can look at the structure of the matrix using str\n\n\nstr(new_mat)\n\n int [1:4, 1:3] 10 11 12 13 14 15 16 17 18 19 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"first\" \"second\" \"third\" \"forth\"\n  ..$ : chr [1:3] \"D\" \"E\" \"F\"\n\nHere you can see that the type of this structure is int because it is a matrix consisting of integers. We can also see the row names and column names.\nAs with data frames, we can check the size of the matrix using nrow, ncol and dim\n\n\nnrow(new_mat)\n\n[1] 4\n\nncol(new_mat)\n\n[1] 3\n\ndim(new_mat)\n\n[1] 4 3\n\nWe can also access data using brackets[\nSelecting a single value:\n\n\nnew_mat[1,2]\n\n[1] 14\n\nSelecting a section of the matrix:\n\n\nnew_mat[1:3,2]\n\n first second  third \n    14     15     16 \n\nIf we don’t provide an index for the row, R will return all rows:\n\n\nnew_mat[, 3]\n\n first second  third  forth \n    18     19     20     21 \n\nThe same is true for the columns\n\n\nnew_mat[3,]\n\n D  E  F \n12 16 20 \n\nBecause this matrix has row and column names, we can also pull out data based on those\n\n\nnew_mat[\"second\", \"D\"]\n\n[1] 11\n\nExercise\nWhat value is in row 2 and column 3 of new_mat?\n\n\n# TODO find the value in the matrix at row 2 and column 3\n\n\nIf we can make a matrix from a data frame, what’s the difference?\nMatrices can only have values of one type –> integer, boolean, character, while a dataframe can be a mix of types:\n\n\ndf <- data.frame(\"A\" = c(10:12),\n                 \"B\" = c(\"cat\", \"dog\", \"fish\"),\n                 \"C\" = c(TRUE, TRUE, FALSE))\n\ndf\n\n   A    B     C\n1 10  cat  TRUE\n2 11  dog  TRUE\n3 12 fish FALSE\n\n\n\nM <- as.matrix(df)\n\nM\n\n     A    B      C      \n[1,] \"10\" \"cat\"  \"TRUE\" \n[2,] \"11\" \"dog\"  \"TRUE\" \n[3,] \"12\" \"fish\" \"FALSE\"\n\n\n\ntypeof(df[,1])\n\n[1] \"integer\"\n\ntypeof(M[,1])\n\n[1] \"character\"\n\nBut Matrices can take any type of input\n\n\nM <- matrix(rep(c(TRUE, FALSE), 4), nrow = 4, byrow = TRUE)\nM\n\n     [,1]  [,2]\n[1,] TRUE FALSE\n[2,] TRUE FALSE\n[3,] TRUE FALSE\n[4,] TRUE FALSE\n\n\n\ntypeof(M[,1])\n\n[1] \"logical\"\n\nMatrix opearions\nIf you’ve taken linear algebra, you’ve probably worked with matrices before. These same matrix operations can be done in R\nBasic operations\nWe can do any of the mathematical operations for a matrix and one value. For example, we can add 5 to all values in a matrix, or subtract 2, or divide by 10\n\n\nM <- matrix(c(10:21), nrow = 4, byrow = TRUE)\nM\n\n     [,1] [,2] [,3]\n[1,]   10   11   12\n[2,]   13   14   15\n[3,]   16   17   18\n[4,]   19   20   21\n\nM + 1\n\n     [,1] [,2] [,3]\n[1,]   11   12   13\n[2,]   14   15   16\n[3,]   17   18   19\n[4,]   20   21   22\n\nM + 2\n\n     [,1] [,2] [,3]\n[1,]   12   13   14\n[2,]   15   16   17\n[3,]   18   19   20\n[4,]   21   22   23\n\nM - 5\n\n     [,1] [,2] [,3]\n[1,]    5    6    7\n[2,]    8    9   10\n[3,]   11   12   13\n[4,]   14   15   16\n\nM / 3\n\n         [,1]     [,2] [,3]\n[1,] 3.333333 3.666667    4\n[2,] 4.333333 4.666667    5\n[3,] 5.333333 5.666667    6\n[4,] 6.333333 6.666667    7\n\nM * 10\n\n     [,1] [,2] [,3]\n[1,]  100  110  120\n[2,]  130  140  150\n[3,]  160  170  180\n[4,]  190  200  210\n\nWe can also provide a vector or another matrix to perform element-wise functions with.\n\n\nvector <- c(2, 3, 4, 5)\n\nM + vector\n\n     [,1] [,2] [,3]\n[1,]   12   13   14\n[2,]   16   17   18\n[3,]   20   21   22\n[4,]   24   25   26\n\nHere you can see that each element of the vector is added to a row ie element 1 is added to row 1, element 2 is added to row 2, etc.\nThe same is true for subtraction\n\n\nM - vector\n\n     [,1] [,2] [,3]\n[1,]    8    9   10\n[2,]   10   11   12\n[3,]   12   13   14\n[4,]   14   15   16\n\nAnd multiplication and division\n\n\nM / vector\n\n         [,1]     [,2] [,3]\n[1,] 5.000000 5.500000  6.0\n[2,] 4.333333 4.666667  5.0\n[3,] 4.000000 4.250000  4.5\n[4,] 3.800000 4.000000  4.2\n\nM * vector\n\n     [,1] [,2] [,3]\n[1,]   20   22   24\n[2,]   39   42   45\n[3,]   64   68   72\n[4,]   95  100  105\n\nWhat happens if there are a different number of rows as elements in the vector?\n\n\nvector <- c(2, 3, 4)\n\nM\n\n     [,1] [,2] [,3]\n[1,]   10   11   12\n[2,]   13   14   15\n[3,]   16   17   18\n[4,]   19   20   21\n\nM + vector\n\n     [,1] [,2] [,3]\n[1,]   12   14   16\n[2,]   16   18   17\n[3,]   20   19   21\n[4,]   21   23   25\n\nNote how the vector just gets reused, no error is thrown.\nWe can also perform these operations on two matrices\n\n\nM1 <- matrix(c(10:21), nrow = 4, byrow = TRUE)\nM2 <- matrix(c(110:121), nrow =4, byrow = TRUE)\n\nM1\n\n     [,1] [,2] [,3]\n[1,]   10   11   12\n[2,]   13   14   15\n[3,]   16   17   18\n[4,]   19   20   21\n\nM2\n\n     [,1] [,2] [,3]\n[1,]  110  111  112\n[2,]  113  114  115\n[3,]  116  117  118\n[4,]  119  120  121\n\nM1 + M2\n\n     [,1] [,2] [,3]\n[1,]  120  122  124\n[2,]  126  128  130\n[3,]  132  134  136\n[4,]  138  140  142\n\nNote how elements in the same position of each matrix are added together\nNote this also is true of vectors\n\n\nv1 <- c(1,2,3)\nv2 <- c(4)\n\nv1 + v2\n\n[1] 5 6 7\n\n\n\nv3 <- c(5, 6)\nv1 + v3\n\n[1] 6 8 8\n\n\n\nv4 <- c(10, 11, 12, 13, 14, 15)\n\nv1 + v4\n\n[1] 11 13 15 14 16 18\n\nExercise\nMultiply, subtract, and divide the two matrices M1 and M2\n\n\n# TODO multiply, subtract and divide M1 and M2\n\n\nMatrix multiplication\nWe will only briefly touch on matrix multiplication, but one reason matrices are very important in R is that you can perform multiplication with them. Exactly how this is done is explained nicely in a math is fun tutorial.\nLet’s try to multiply two matrices together. Remember our first matrix has 4 rows and 3 columns:\n\n\ndim(M)\n\n[1] 4 3\n\nSo our new matrix must have 3 rows\n\n\nM2 <- matrix(c(5:19), nrow = 3, byrow = TRUE)\nM2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    5    6    7    8    9\n[2,]   10   11   12   13   14\n[3,]   15   16   17   18   19\n\nLet’s perform matrix multiplication with these\n\n\nM %*% M2\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  340  373  406  439  472\n[2,]  430  472  514  556  598\n[3,]  520  571  622  673  724\n[4,]  610  670  730  790  850\n\nPerforming functions on a matrix\nSo far, a matrix has looked a lot like a dataframe with some limitations. One of the places where matrices become the most useful is performing statistical functions because all items in a matrix are of the same type.\nFor this next section, let’s use some data that I downloaded from the social security office. This has the top 100 boy and girl names by state for 2020.\nWe can now read in the data and convert it to a matrix\n\n\nnames_mat <- read.csv(here(\"class_7-9_data\", \"boy_name_counts.csv\"),\n                           row.names = 1) %>%\n  as.matrix()\n\nnames_mat[1:5, 1:5]\n\n        Alabama Alaska Arizona Arkansas California\nWilliam     366     36     174      152       1021\nJames       304     34     215      105       1148\nJohn        267     20      97       94        623\nElijah      254     42     284      143       1586\nNoah        243     35     397      138       2625\n\nAbove you can see that we have the number of males with each name in each state. Looking at the structure, we can see that it is an integer matrix.\n\n\nstr(names_mat)\n\n int [1:40, 1:20] 366 304 267 254 243 207 187 183 173 166 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:40] \"William\" \"James\" \"John\" \"Elijah\" ...\n  ..$ : chr [1:20] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n\nWe can now explore this data set using many of the functions you have already learned such as rowSums and colSums\nBasic functions\nFirst, lets find the sum for all of the rows - how many total babies were named each name?\n\n\nrowSums(names_mat)\n\n  William     James      John    Elijah      Noah      Liam     Mason \n     5067      5005      3295      5921      8118      8512      4040 \n   Oliver     Henry   Jackson    Samuel     Jaxon     Asher   Grayson \n     5961      4121      3599      3549      2571      3212      2938 \n     Levi   Michael    Carter  Benjamin   Charles     Wyatt    Thomas \n     3717      4061      3017      5265      2427      3255      2475 \n    Aiden      Luke     David      Owen    Daniel     Logan    Joseph \n     3872      3230      3554      3369      4223      3933      3356 \n    Lucas    Joshua      Jack Alexander  Maverick   Gabriel     Ethan \n     4794      2645      3399      4530      2588      3091      4322 \n      Eli     Isaac    Hunter      Ezra  Theodore \n     2202      3029      1940      2999      3409 \n\nAnd then the columns - how many babies were included from each state?\n\n\ncolSums(names_mat)\n\n    Alabama      Alaska     Arizona    Arkansas  California \n       5687         986        7371        3428       41256 \n   Colorado Connecticut    Delaware     Florida     Georgia \n       6396        4211        1117       21661       11791 \n     Hawaii       Idaho    Illinois     Indiana        Iowa \n       1111        2306       13407        8283        3508 \n     Kansas    Kentucky   Louisiana       Maine    Maryland \n       3630        5518        4955        1372        6617 \n\nWhat if we want to find the percent of children with a given name across all states (divide the value by the row sum * 100) - what percent of total babies for each name came from each state:\n\n\npercent_mat <- names_mat / rowSums(names_mat) * 100\npercent_mat[1:5, 1:5]\n\n         Alabama    Alaska  Arizona Arkansas California\nWilliam 7.223209 0.7104796 3.433985 2.999803   20.14999\nJames   6.073926 0.6793207 4.295704 2.097902   22.93706\nJohn    8.103187 0.6069803 2.943854 2.852807   18.90744\nElijah  4.289816 0.7093396 4.796487 2.415133   26.78602\nNoah    2.993348 0.4311407 4.890367 1.699926   32.33555\n\nRemember from above that division using a vector will divide every element of a row by one value, so we can only do this using rowSums. In a few minutes we will discuss how do do this on the columns.\nSummary functions\nWe can also find the minimum, maximum, mean, and median values of the whole matrix and any column. First, lets get summary data for the whole matrix using summary\n\n\nsummary(names_mat)[ , 1:3]\n\n    Alabama           Alaska         Arizona     \n Min.   : 61.00   Min.   :14.00   Min.   : 84.0  \n 1st Qu.: 94.75   1st Qu.:18.00   1st Qu.:143.8  \n Median :125.00   Median :22.50   Median :174.5  \n Mean   :142.18   Mean   :24.65   Mean   :184.3  \n 3rd Qu.:163.00   3rd Qu.:28.00   3rd Qu.:191.2  \n Max.   :366.00   Max.   :44.00   Max.   :451.0  \n\nYou can see that this calculates the min, max, mean, median, and quartiles for the columns.\nWhat if we just want the minimum value for the “Alabama” names? We can run min while subsetting to just the column of interest\n\n\nmin(names_mat[, \"Alabama\"])\n\n[1] 61\n\nWe can do the same for the rows Lets try this for “William”\n\n\nmin(names_mat[\"William\",])\n\n[1] 29\n\nWhat if we wanted to find the smallest value in the whole matrix?\n\n\nmin(names_mat)\n\n[1] 13\n\nmax works the same as min\nExercise\nFind the maximum value in the for “Noah”\n\n\n# TODO Find the maximum value in the second row and the whole matrix\n\n\nWe can also find the mean, median, and standard deviation of any part of the matrix\nBy row:\n\n\nmean(names_mat[\"William\", ])\n\n[1] 253.35\n\n\n\nmedian(names_mat[\"William\", ])\n\n[1] 178\n\n\n\nsd(names_mat[\"William\", ])\n\n[1] 239.8189\n\nBy column:\n\n\nmean(names_mat[ , \"Alabama\"])\n\n[1] 142.175\n\n\n\nmedian(names_mat[ , \"Alabama\"])\n\n[1] 125\n\n\n\nsd(names_mat[ , \"Alabama\"])\n\n[1] 67.66012\n\nTransposition\nOne important quality of a matrix is being able to transpose it to interchange the rows and columns - here the rows become columns and columns become rows. We transpose using t() to the matrix. Let’s first look at this using the matrix we started with\n\n\nM <- matrix(c(10:21), nrow = 4, byrow = TRUE)\nM\n\n     [,1] [,2] [,3]\n[1,]   10   11   12\n[2,]   13   14   15\n[3,]   16   17   18\n[4,]   19   20   21\n\n\n\nt(M)\n\n     [,1] [,2] [,3] [,4]\n[1,]   10   13   16   19\n[2,]   11   14   17   20\n[3,]   12   15   18   21\n\nNote that the output of transposing either a matrix or a data frame will be a matrix (because the type within a column of a data frame must be the same).\n\n\ndf\n\n   A    B     C\n1 10  cat  TRUE\n2 11  dog  TRUE\n3 12 fish FALSE\n\nt(df)\n\n  [,1]   [,2]   [,3]   \nA \"10\"   \"11\"   \"12\"   \nB \"cat\"  \"dog\"  \"fish\" \nC \"TRUE\" \"TRUE\" \"FALSE\"\n\nstr(df)\n\n'data.frame':   3 obs. of  3 variables:\n $ A: int  10 11 12\n $ B: chr  \"cat\" \"dog\" \"fish\"\n $ C: logi  TRUE TRUE FALSE\n\nstr(t(df))\n\n chr [1:3, 1:3] \"10\" \"cat\" \"TRUE\" \"11\" \"dog\" \"TRUE\" \"12\" \"fish\" ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:3] \"A\" \"B\" \"C\"\n  ..$ : NULL\n\nNote how after the transposition, all items in the original df are now characters and we no longer have a dataframe.\nNow let’s try this transposition on the names matrix we’ve been working with\n\n\ntransposed_mat <- t(names_mat)\n\ntransposed_mat[1:3,1:3]\n\n        William James John\nAlabama     366   304  267\nAlaska       36    34   20\nArizona     174   215   97\n\nNote how the columns are now names and the rows are now states.\nRemember the note above where we could only divide by the rowSums? Now we can use this transposition to figure out the percent of children in each state with a given name (divide the value by the column sum * 100)\n\n\nstate_percents <- transposed_mat / rowSums(transposed_mat) * 100\n\nstate_percents <- t(state_percents)\n\nstate_percents[1:3, 1:3]\n\n         Alabama   Alaska  Arizona\nWilliam 6.435731 3.651116 2.360602\nJames   5.345525 3.448276 2.916836\nJohn    4.694918 2.028398 1.315968\n\nAbove we did this in several steps, but we can also do in in one step:\n\n\nstate_percents_2 <- t(t(names_mat) / colSums(names_mat)) * 100\n\nidentical(state_percents, state_percents_2)\n\n[1] TRUE\n\nStatistical tests\nWe can also use matrices to perform statistical tests, like t-tests. For instance, are the names Oliver and Noah, or Oliver and Thomas used different amounts?\nFirst, let’s normalize the data to account for the fact that each state reported different numbers of births. To do this normalization, let’s first divide each value by the total number of children reported for that state. Remember, we need to first transpose the matrix to be able to divide by the colSums\n\n\nnormalized_mat <- t(t(names_mat) / colSums(names_mat))\n\n\nNow that we have normalized values, we can do a t-test.\n\n\nnormalized_mat[\"Oliver\", 1:3]\n\n   Alabama     Alaska    Arizona \n0.03217865 0.04361055 0.04124271 \n\nnormalized_mat[\"Noah\", 1:3]\n\n   Alabama     Alaska    Arizona \n0.04272903 0.03549696 0.05385972 \n\nnormalized_mat[\"Thomas\", 1:3]\n\n   Alabama     Alaska    Arizona \n0.02092492 0.02028398 0.01329535 \n\n\n\nt.test(normalized_mat[\"Oliver\",], normalized_mat[\"Noah\",])\n\n\n    Welch Two Sample t-test\n\ndata:  normalized_mat[\"Oliver\", ] and normalized_mat[\"Noah\", ]\nt = -1.1861, df = 36.481, p-value = 0.2433\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.010275411  0.002689617\nsample estimates:\n mean of x  mean of y \n0.04133411 0.04512700 \n\nBetween Oliver and Noah, there does not seem to be a difference with the data we have. What about Oliver and Thomas?\n\n\nt.test(normalized_mat[\"Oliver\",], normalized_mat[\"Thomas\",])\n\n\n    Welch Two Sample t-test\n\ndata:  normalized_mat[\"Oliver\", ] and normalized_mat[\"Thomas\", ]\nt = 9.3268, df = 21.544, p-value = 5.121e-09\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01858464 0.02922945\nsample estimates:\n mean of x  mean of y \n0.04133411 0.01742706 \n\nHere we can see that there is a difference between the mean values for Oliver and Thomas using a t.test\nUsing dataframes and matricies\nFor many of the tidyverse functions you’ve learned so far, a data frame is required. Fortunately, it is very easy to change between a data frame and a matrix.\n\n\nnormalized_dat <- data.frame(normalized_mat)\n\nstr(normalized_dat)\n\n'data.frame':   40 obs. of  20 variables:\n $ Alabama    : num  0.0644 0.0535 0.0469 0.0447 0.0427 ...\n $ Alaska     : num  0.0365 0.0345 0.0203 0.0426 0.0355 ...\n $ Arizona    : num  0.0236 0.0292 0.0132 0.0385 0.0539 ...\n $ Arkansas   : num  0.0443 0.0306 0.0274 0.0417 0.0403 ...\n $ California : num  0.0247 0.0278 0.0151 0.0384 0.0636 ...\n $ Colorado   : num  0.0369 0.0364 0.0211 0.0317 0.0408 ...\n $ Connecticut: num  0.0356 0.0359 0.0302 0.024 0.0503 ...\n $ Delaware   : num  0.0269 0.0322 0.0251 0.0403 0.0466 ...\n $ Florida    : num  0.0244 0.0278 0.018 0.0436 0.0608 ...\n $ Georgia    : num  0.0469 0.0369 0.0317 0.0446 0.0494 ...\n $ Hawaii     : num  0.0261 0.0297 0.0252 0.0342 0.0513 ...\n $ Idaho      : num  0.0412 0.0399 0.0173 0.0351 0.0308 ...\n $ Illinois   : num  0.0345 0.0334 0.023 0.0309 0.053 ...\n $ Indiana    : num  0.0333 0.0321 0.0164 0.0396 0.0391 ...\n $ Iowa       : num  0.0425 0.0296 0.0154 0.0299 0.0371 ...\n $ Kansas     : num  0.0342 0.0336 0.0226 0.0358 0.0372 ...\n $ Kentucky   : num  0.0448 0.0399 0.0268 0.0419 0.0379 ...\n $ Louisiana  : num  0.0367 0.0349 0.0367 0.0478 0.044 ...\n $ Maine      : num  0.0277 0.0248 0.016 0.0255 0.035 ...\n $ Maryland   : num  0.0328 0.0378 0.0213 0.0293 0.0533 ...\n\nOnce we can move between matrices and data frames, we can start to tidy our data for plotting purposes. Let’s plot the distribution of name usage as a violin plot. Here we want the counts to be the y axis and the names to be the y axis.\nThe first thing we need to do is make our matrix into a data frame\n\n\nnames_dat <- data.frame(names_mat)\n\n\nNext, we will want the names to be a column rather than the row names. We can do this using $ or tibble::rownames_to_column\n\n\nnames_dat <- rownames_to_column(names_dat, \"name\")\n\nnames_dat[1:3,1:3]\n\n     name Alabama Alaska\n1 William     366     36\n2   James     304     34\n3    John     267     20\n\n# To set using $\n# names_dat$name <- rownames(names_dat)\n\n\nNext, we need to pivot_longer from tidyr. We want to take everything but the names column\n\n\npivot_columns <- colnames(names_dat)[colnames(names_dat) != \"name\"]\n\nnames_dat <- pivot_longer(names_dat, cols = all_of(pivot_columns),\n                          names_to = \"state\", values_to = \"count\")\n\n\nNote, we can use the pipe %>% from dplyr to put all of this into one statement.\n\n\n# Here we will specify the columns to keep first\npivot_columns <- colnames(names_mat)\n\nnames_dat <- names_mat %>% \n  data.frame %>% \n  rownames_to_column(\"name\") %>% \n  pivot_longer(cols = all_of(pivot_columns), \n               names_to = \"state\", values_to = \"count\")\n\n\nWith this new data frame, we can now plot the distribution of names\n\n\n# I first set the theme\ntheme_set(theme_classic(base_size = 10))\n\nggplot(names_dat, aes(x = name, y = count,\n                              fill = name)) + \n  geom_violin() +\n  theme(axis.text.x = element_text(angle = 90,\n                                   vjust = 0.5, hjust=1)) # rotate x axis\n\n\n\nThere are a few outliers here, almost certainly California. As we discussed above, normalizing the data helps put everything onto the same scale.\nExercise\nCan you make the same plot as above but use our normalized values?\n\n\n# TODO make plot above with normalized values\n# Hint start with normalized_dat\n\n\nI’m a biologist, why should I care?\nMany of your datasets can be analyzed using matrices\nIf you analyze RNA-seq data, all of the processing and differential testing will be done in a matrix through DESeq2\nIf you analyze protein data, that is best done in a matrix\nSingle cell RNA-seq data is analyzed using matrices, especially sparse matrices\nEven just many measurements can be analyzed using a matrix\n\nMatrices are efficient object types for most types of analysis you would want to do\nAcknowldgements and additional references\nThe content of this class borrows heavily from previous tutorials:\nMatrices\nR example\nQuick tips\n\n\n\n\n",
    "preview": "posts/2022-11-10-class-7-matricies/img/matrix_image.jpg",
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-23-class-6-intro-to-ggplot2-part3/",
    "title": "Class 6: Introduction to ggplot2 (part3)",
    "description": {},
    "author": [
      {
        "name": "Michael Kaufman",
        "url": "https://github.com/mlkaufman"
      }
    ],
    "date": "2022-12-07",
    "categories": [],
    "contents": "\nThe Rmarkdown for this document is\nhttps://github.com/rnabioco/bmsc-7810-pbda/blob/main/_posts/2022-11-23-class-6-intro-to-ggplot2-part3/class-6-intro-to-ggplot2-part3.Rmd\nGoals for today\nNew dataset diamonds\nFaceting plots\nStoring plots as variables\nColor palettes\nApplying themes\nCombining plots with patchwork\nDataset: Diamonds\n\n\n\nA dataset containing the prices and other attributes of almost 54,000\ndiamonds.\n\n\nhead(diamonds)\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\nA data frame with 53940 rows and 10 variables:\nprice = price in US dollars ($326–$18,823)\ncarat = weight of the diamond (0.2–5.01)\ncut = quality of the cut (Fair, Good, Very Good, Premium, Ideal)\ncolor = diamond color, from D (best) to J (worst)\nclarity = a measurement of how clear the diamond is (I1 (worst), SI2,\nSI1, VS2, VS1, VVS2, VVS1, IF (best))\nx = length in mm (0–10.74)\ny = width in mm (0–58.9)\nz = depth in mm (0–31.8)\ndepth = total depth percentage = z / mean(x, y) = 2 * z / (x + y)\n(43–79)\ntable = width of top of diamond relative to widest point (43–95)\n\n\nggplot(diamonds, aes(x=carat, y=price)) + \n  geom_point()\n\n\n\nExercise: Review the last class. Make a histogram showing the\ndistribution of diamond prices. Color by the cut of the diamond. What\nstatements can you make about the relationships shown.\n\n\n\nExercise: More review. Create a freqpoly plot showing the frequency\ncount of the carat and the color as the cut of diamond. Does this help\nexplain the ideal cut price?\n\n\n\nThere are so many data points in this dataset as seen by our original\nscatterplot. Before moving on we can subset this dataset by using sample\nto grab a random selection of 1000 rows for downstream analysis.\n\n\nset.seed(1337) # set the random seed so that we get the same random rows everytime\n\nsubset_diamonds <- diamonds[sample(nrow(diamonds), 1000), ]\n\nggplot(subset_diamonds, aes(x=carat, y=price)) + \n  geom_point()\n\n\n\nIntroducing the Facet\nOne way that we can take an attribute from your data and expand it to\nplot it into multiple plots, one for each level, letting you view them\nseparately. Just as a cut diamond has different flat edges called\nfacets, in ggplot this type of breaking out the levels of the data into\nmultiple plots is called “faceting”. One of the easiest ways to do this\nis by using the facet_wrap() function.\n\n\nggplot(subset_diamonds, aes(x=carat, y=price, color=cut)) +\n  geom_point() + \n  facet_wrap(~cut, nrow = 1)\n\n\n\nThe second type of facet function is the facet_grid()\n\n\nggplot(subset_diamonds, aes(x=carat, y=price, color=cut)) +\n  geom_point() + \n  facet_grid(clarity ~ cut)\n\n\n\nThis is a good time to introduce a way to modify the size of the figure\nbeing displayed in RMarkdown. We can edit the curly braces to give\nspecial instructions for the cell. Kent has previous showed this to you\nas well. Here we can add fig.width=20 to increase the width of the\nfigure. You can also try fig.height. There are numerous ways you can\ninfluence the plot using this format and most of them start with the\nfig. prefix.\n\n\nggplot(diamonds, aes(x=carat, y=price, color=cut)) +\n  geom_point() + \n  facet_grid(clarity ~ cut)\n\n\n\nExercise: Use the dataset from last class iris. Make a scatterplot of\nSepal Width and Sepal Length and color by the Species. Use a\nfacet_wrap to break out the Species.\n\n\n\nSaving Plot Objects\nOne concept that can be useful is that you can assign ggplot plots to a\nvariable just like any other object in R. This can allow you to reuse\nthe plot over and over again simply by calling the variable name you\nsaved the plot. You can also continue to add layers to these plots and\ncan we a quick way to test and compare different versions of a plot.\n\n\np1 <- ggplot(subset_diamonds, aes(x=carat, y=price, color=cut)) +\n  geom_point()\n\n\nNotice that nothing was plotting when you run this code. Instead the\nplot is saved to the p1 variable. We can visualize this plot anytime\nsimply by calling the variable.\n\n\np1\n\n\n\nWe can add any additional layers just as we would when building the\nplot. Let’s look at a facet_wrap of the clarity.\n\n\np1 + facet_wrap(~clarity)\n\n\n\nWe changed our mind and now we want to compare this to the same base\nplot but use a facet_grid breaking out the diamond color.\n\n\np1 + facet_grid(clarity~color)\n\n\n\nColor Palettes\nYou can easily change the types and ranges of colors being used in your\nplots. Here is the default color palette:\n\n\nggplot(subset_diamonds, aes(carat, price, color = clarity)) +\n  geom_point()\n\n\n\nWe can use the scale_color_brewer() to set a different type of\npalette. There are many default options to choose from and maybe more\ncustom ones you can install.\nhttps://r-graph-gallery.com/38-rcolorbrewers-palettes.html\n\n\nggplot(subset_diamonds, aes(carat, price, color = clarity)) +\n  geom_point() +\n  scale_color_brewer(palette = \"RdYlBu\")\n\n\n\n\n\nggplot(subset_diamonds, aes(carat, price, color = clarity)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Accent\")\n\n\n\n\n\nggplot(subset_diamonds, aes(carat, price, color = clarity)) +\n  geom_point() +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"white\", \"black\", \"gray\"))\n\n\n\nThemes\nOne of the most fun aspects of ggplot is the ability to quickly change\nthe entire look of your plots with themes.\n\n\nptest <- ggplot(iris, aes(x=Sepal.Width, y=Sepal.Length, color = Species)) +\n  geom_point() +\n  facet_wrap(~ Species)\n\nptest\n\n\n\n\n\nptest + theme_dark()\n\n\n\n\n\nptest + theme_minimal()\n\n\n\n\n\nptest + theme_bw()\n\n\n\n\n\nptest + theme_classic()\n\n\n\n\n\nptest + theme_void()\n\n\n\nYou can install custom themes….\nhttps://ryo-n7.github.io/2019-05-16-introducing-tvthemes-package/\nhttps://github.com/Mikata-Project/ggthemr\nhttp://xkcd.r-forge.r-project.org/\nCombining multiple plots\nOne useful technique when assembling figures is to be able to stitch\nmultiple plots together into a single image. There is a special add on\npackage that allows us to do just that with simple syntax. This package\nis called patchwork and will need to be installed as it is not\nincluded in the tidyverse. It can be installed with\ninstall.packages(\"patchwork\"). More info at\nhttps://patchwork.data-imaginist.com/\n\n\nlibrary(patchwork)\n\n\nSave the plots as object variables.\n\n\np1 <- ggplot(mtcars) + \n  geom_point(aes(mpg, disp))\n\np2 <- ggplot(mtcars) + \n  geom_boxplot(aes(gear, disp, group = gear))\n\n\nTo use patchwork simply place the plus operator to “add” two plots\ntogether:\n\n\np1 + p2\n\n\n\nWhy stop at just two plots? We can keep adding more.\n\n\np3 <- ggplot(mtcars) + \n  geom_smooth(aes(disp, qsec))\n\np4 <- ggplot(mtcars) + \n  geom_bar(aes(carb))\n\n\nAnd use more complex ways of displaying them.\n\n\n(p1 + p2 + p3) / p4\n\n\n\nTo annotate the whole group we need to use a special plot_annotation()\nfunction:\n\n\n(p1 | p2 | p3) / p4 + \n  plot_annotation(\n  title = 'The surprising truth about mtcars',\n  subtitle = 'These 3 plots will reveal yet-untold secrets about our beloved data-set',\n  caption = 'Disclaimer: None of these plots are insightful')\n\n\n\nYou can even automatically add the subplot letter annotations. Publish\ntime!\n\n\n(p1 | p2 | p3) / p4 + \n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\n(p1 | p2 | p3) / p4 + \n  plot_annotation(title = \"Figure 1: Motor Trend 1974 Car Stats\", tag_levels = 'A')\n\n\n\nExercise: Change the order of the plots combined with patchwork so that\np4 is in the middle of the top row and p2 is now on the bottom row. See\nhow the plot adapts.\n\n\n\nThanks for listening. Keep on plotting and exploring the world of\nggplot2!\n\n\n\n",
    "preview": "posts/2022-11-23-class-6-intro-to-ggplot2-part3/class-6-intro-to-ggplot2-part3_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-23-class-5-intro-to-ggplot2-part2/",
    "title": "Class 5: Introduction to ggplot2 (part2)",
    "description": {},
    "author": [
      {
        "name": "Michael Kaufman",
        "url": "https://github.com/mlkaufman"
      }
    ],
    "date": "2022-12-05",
    "categories": [],
    "contents": "\nThe Rmarkdown for this document is: https://github.com/rnabioco/bmsc-7810-pbda/blob/main/_posts/2022-11-23-class-5-intro-to-ggplot2-part2/class-5-intro-to-ggplot2-part2.Rmd\nGoals for today\nNew dataset: Iris\nPlotting the categorical data from iris measurements\nBox plots and violin plots\nFrequency and density plots\nUsing stat layers\nAdding additional annotations\nAxis, scales, and coordinate Systems\nThe Iris Dataset\nFor this class we are going to use a new built in dataset that involves\nthe measurements of Iris flowers. In particular the measurements involve\nthe width and length of two structures of the flower: the petal and the\nsepal. Here is an overview of flower structure.\n\n\n\nThe Iris dataset is classically used in machine learning and\nclassification projects. Three species of iris were included in this\nstudy: iris setosa, iris versicolor, and iris virginica. Measurements\nwere taken in 1936 by famous statistician RA Fisher known for the\nStudent’s t-test and F-distribution.\nhttp://archive.ics.uci.edu/ml/datasets/Iris\n\n\n\nLet’s look at the this new dataset with head. You can see that it is\nin tidy format with each observation being a new row.\n\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nTo get a list of the species in this study we can look at all the\nunique() entries in the Species column.\n\n\nunique(iris$Species)\n\n[1] setosa     versicolor virginica \nLevels: setosa versicolor virginica\n\nEach one of the species is represented and now we have the exact names\nas written by each measurement. To get the number of measurements for\neach species we can use the summary() function.\n\n\nsummary(iris$Species)\n\n    setosa versicolor  virginica \n        50         50         50 \n\nWe can begin by looking at the relationships between some of the\nmeasurements by looking at a scatter plot. Here we have Sepal.Length on\nthe x-axis and Sepal.Width on the y-axis.\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point()\n\n\n\nExercise: Despite this showing all the data points. How is this not very\ninformative? As a review of last class, add to this plot to make it more\ninformative?\n\n\n\nExercise: Remake this scatterplot but this time for Petal.Width and\nPetal.Length and plot ONLY the iris virginica species data points.\n\n\n\nPlotting the Categorical Data\nSpecies data points with geom_point\nTypically we can look at the distribution of a particular measurement\nvalue based on the category of the measurement, in this case the\nspecies. In this way we can make comparisons between the species. As\nbefore we can use a geom_point_() to plot the values for each species.\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width)) +\n  geom_point()\n\n\n\nWhile this does show a basic distribution of Sepal.Width for each\nSpecies, many of the points that have the same value are actually\nhidden! One way we can improve on this is by adding a bit of jitter or\nrandom horizontal position to each point.\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width)) +\n  geom_jitter()\n\n\n\nNotice that if you rerun the plot the points are in different locations.\nThe space added by the jitter is randomly generated everytime. Don’t\nexpect them to look the same everytime!\nSide note: You can also use geom_point() geometry function with the\nposition = position_jitter() setting and it will generate the same\nplot as with geom_jitter()\nYou can also tighten the range of the jitter by specifying a width.\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width)) +\n  geom_jitter(width=0.1)\n\n\n\nThe Boxplot\nA frequently used plot that is used to better descriptively show this\ntype of data is a boxplot. We can generate a box plot of this data\nsimply by adding a second geom layer called geom_boxplot(). This way\nwe keep the point layer but also have the boxplot.\n\n\n\nHere we can add a geom_boxplot layer to our existing jittered\nscatterplot.\n\n\nggplot(iris, (aes(x = Species, y = Sepal.Width))) +\n  geom_jitter() +\n  geom_boxplot()\n\n\n\nExercise: Many of the points are hidden behind the boxplot. Try changing\nthe order of the layers to see if it matters. What is another way you\ncould fix this?\n\n\n\nViolin Plot\nAnother type of frequently used plot is the violin plot. This plot shows\na continuous density distribution.\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width)) +\n  geom_violin() +\n  geom_jitter()\n\n\n\nStats Layers\nStats or statistics layers allows us to calculate certain metrics about\nour data and potentially visualize them. First we will look at some of the geom that use stats in their plots.\nFrequency and Density Plots\nFor instance here is a new type of plot that calculates frequency of counts across all measurements of\nSepal.Width. It uses a stat to count the number of measurements at specific values. We could also show the color aes to visualize all the species.\n\n\nggplot(iris) +\n  geom_freqpoly(aes(x = Sepal.Width))\n\n\n\ngeom_dotplot() is another way to visualize representative counts. Note that settings stackgroups = TRUE allows you to see all of the dots by stacking them vertically on top of one another without overlap. It uses a stat to count the number of measurements at specific values and represents them as a dot.\n\n\nggplot(iris) +\n  geom_dotplot(aes(x = Sepal.Width, fill = Species), stackgroups = TRUE)\n\n\n\nDensity plots can overlap to show a comparison between groups and visualize distribution. It uses a stat to calculate a density metric.\n\n\nggplot(iris) +\n  geom_density(aes(x = Sepal.Width, color = Species))\n\n\n\nFinally we have a traditional histogram representing the counts of specific measurement values as above but plotted as a bar plot. It also uses a stat to count the number of measurements at these specific values.\n\n\nggplot(iris) +\n  geom_histogram(aes(x = Sepal.Width))\n\n\n\nUnderneath the hood the geom_histogram function is using a stat\nfunction called bin this essentially taking each measurement and\nplacing it in a specific sized category and calculating the frequency of\nthis occurrence. We can modify either the binwidth or the number of\nbins arguments to modify this behavior. For instance if there are 50\nmeasurements from say 1 to 4.5. This range would be divided by the\nnumber of bins. Each measurement value would fall into one of these bins\nand a count would be added for that bin.\n\n\nggplot(iris) +\n  geom_histogram(aes(x = Sepal.Width), stat = \"bin\", bins = 10)\n\n\n\nStat Functions\nStats layers are additional information that we calculate and add to the\nplot. Essentially every geom_ function that we have been seen utilizes\ncalculations to produce the plots. Each of these geom_ functions has\nan equivalent stat_ function. It is beyond the scope of this class to\nget into the details of all of these stat functions. Here we will look\nat a particular function called stat_summary that we can use to plot\nsome summary statistics.\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width)) +\n  geom_jitter() +\n  stat_summary(fun = \"mean\",\n               geom = \"point\",\n               color = \"red\")\n\n\n\nSome of the other options for stat_summary:\ngeoms: point, errorbar, pointrange, linerange, crossbar\nfuns: mean, median, max, min\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width)) +\n  geom_jitter() +\n  stat_summary(fun = \"mean\",\n               geom = \"crossbar\",\n               width = 0.5,\n               color = \"red\")\n\n\n\nWe can combine multiple stat_summary layers to add additional\ninformation.\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width)) +\n  geom_jitter() +\n  stat_summary(fun = \"mean\",\n               geom = \"crossbar\",\n               width = 0.5,\n               color = \"red\") +\n  stat_summary(fun = \"median\",\n               geom = \"crossbar\",\n               width = 0.5,\n               color = \"blue\")\n\n\n\nPlotting the standard error and the confidence intervals\nPlotting the standard error.\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width)) +\n  geom_jitter() +\n  stat_summary(geom = \"errorbar\",\n               fun.data = mean_se)\n\n\n\nTo calculate the standard deviation and produce the confidence intervals\nyou can pass mean_cl_normal to the fun.data argument. Note you may\nneed to install the Hmisc package to get this working.\ninstall.packages(\"Hmisc\")\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width)) +\n  geom_jitter() +\n  stat_summary(geom = \"errorbar\",\n               fun.data = mean_cl_normal)\n\n\n\nAnnotations\nAnnotations are easy ways to add extra emphasis to your plots. It can be\nmuch more efficient to have them placed on your plots programatically\nrather than trying to add them later with Photoshop or Illustrator.\nUsing geom_text()\ngeom_text() is an easy way to play text on a plot to annotate. We can even use its aes() function to add column information to the plot like so.\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  geom_text(aes(label=Species))\n\n\n\nNot very practical. Let’s look at the documentation to get some better ideas.\n\n\n?geom_text\n\n\nThere are several options we can add to make things a little neater.\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  geom_text(aes(label=Species), nudge_y = .1, check_overlap = T, size = 3)\n\n\n\nWe can also manually place text anywhere we would like in the plot. This could be a way to annotate whole groups or parts of the visualization.\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(color= Species)) +\n  geom_text(aes(label=\"setosa\"), x=5, y=4, size = 5) +\n  geom_text(aes(label=\"versicolor\"), x=5.5, y=2.25, size = 5) + \n  geom_text(aes(label=\"virginica\"), x=7.5, y=3.5, size = 5)\n\n\n\nThe annotate function\nThe annotate function can be used to pass specific types of geometries\nthat you can manually draw on your plot.\n\n\n?annotate\n\n\nHere is an example of drawing a rectangle.\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(color= Species)) +\n  annotate(\"rect\", xmin=5.5, xmax=6.5, ymin=2.5 , ymax=3.2, alpha=0.2, color=\"blue\")\n\n\n\nUsing a segment geom to produce an arrow. Notice how we need to add the\narrow function.\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(color= Species)) +\n  annotate(\"segment\", x = 7, xend = 7, y = 4.5, yend = 3.25, color = \"pink\", size=3, alpha=0.6, arrow=arrow())\n\n\n\nDrawing intercept lines with geom_lines\nYou can add horizontal or vertical lines to show cut offs.\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(color= Species)) +\n  geom_hline(yintercept=4, color = \"orange\", size = 1)\n\n\n\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(color= Species)) +\n  geom_vline(xintercept=7, color = \"orange\", size = 1)\n\n\n\nCan add a slope line.\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(color= Species)) +\n  geom_abline(slope = .5, intercept = 1)\n\n\n\nFiltering data as annotation\nYou can also filter your data during the annotation process and use that\nas a way to clearly highlight features of interest.\nHere by limiting the color to specific measurements.\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() + \n  geom_point(data = filter(iris, Sepal.Width > 3.25), aes(color = Species))\n\n\n\nAnd here by limiting the text annotation to specific measurements.\n\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(aes(color = Species)) + \n  geom_text(data = filter(iris, Sepal.Width > 4), aes(label = Species), vjust = 1)\n\n\n\nExercise: Plot a scatter plot of the Petal.Length and Petal.Width and color by the species of iris. Place a rectangle around the group of points representing the data from the setosa species. Place text above the rectangle that displays “smallest flower”.\n\n\n\nAxis, Scales, and Coordinate Systems\nScales are ways of modifying how the data and the coordinates are shown. When you run this code below there are actually several default hidden scales functions being added.\n\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point()\n\n\n\nNotice how there are three scale function layers added. These are actually being run above but are hidden by default. If you run this version you will get the same plot as above.\n\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  scale_x_continuous() + \n  scale_y_continuous() + \n  scale_colour_discrete()\n\n\n\nBasically scale_x_ and scale_y_ functions can be used to modify the respective axis appearance and type. For instance we can change the x axis to be on a log scale by using scale_x_log10(). Great way to visualize without having to transform the actual data.\n\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  scale_x_log10()\n\n\n\nYou can also reverse an axis.\n\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  scale_x_reverse()\n\n\n\nYou can manually set the x and y axis range by using the xlim() and ylim() functions.\n\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  xlim(0,10) +\n  ylim(0,5)\n\n\n\nThe third default scale in the plot was scale_colour_discrete(). This type of scale modifies how the color can be mapped across the data.\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width, color= Sepal.Length)) + \n  geom_jitter()  + \n  scale_color_gradient(low = \"blue\", high = \"red\")\n\n\n\n\n\n#use autocomplete to all the scales options\n#scale_\n\n\nLast class I showed that you could quickly change the axis to swap the\ncoordinates. Here is another way to do that by interacting with the\ncoordinate layer using the coord_flip() function.\n\n\nggplot(iris, aes(x = Species, y = Sepal.Width)) +\n  geom_violin() +\n  geom_jitter() +\n  coord_flip()\n\n\n\nSessionInfo\n\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] forcats_0.5.2   stringr_1.4.1   dplyr_1.0.10    purrr_0.3.5    \n[5] readr_2.1.3     tidyr_1.2.1     tibble_3.1.8    ggplot2_3.4.0  \n[9] tidyverse_1.3.2\n\nloaded via a namespace (and not attached):\n [1] fs_1.5.2            lubridate_1.8.0     RColorBrewer_1.1-3 \n [4] httr_1.4.4          tools_4.2.2         backports_1.4.1    \n [7] bslib_0.4.1         utf8_1.2.2          R6_2.5.1           \n[10] rpart_4.1.19        Hmisc_4.7-2         DBI_1.1.3          \n[13] colorspace_2.0-3    nnet_7.3-18         withr_2.5.0        \n[16] tidyselect_1.2.0    gridExtra_2.3       downlit_0.4.2      \n[19] compiler_4.2.2      cli_3.4.1           rvest_1.0.3        \n[22] htmlTable_2.4.1     xml2_1.3.3          labeling_0.4.2     \n[25] sass_0.4.2          scales_1.2.1        checkmate_2.1.0    \n[28] digest_0.6.30       foreign_0.8-83      rmarkdown_2.17     \n[31] base64enc_0.1-3     jpeg_0.1-10         pkgconfig_2.0.3    \n[34] htmltools_0.5.3     dbplyr_2.2.1        fastmap_1.1.0      \n[37] highr_0.9           htmlwidgets_1.5.4   rlang_1.0.6        \n[40] readxl_1.4.1        rstudioapi_0.14     jquerylib_0.1.4    \n[43] farver_2.1.1        generics_0.1.3      jsonlite_1.8.3     \n[46] distill_1.5         googlesheets4_1.0.1 magrittr_2.0.3     \n[49] Formula_1.2-4       interp_1.1-3        Matrix_1.5-1       \n[52] Rcpp_1.0.9          munsell_0.5.0       fansi_1.0.3        \n[55] lifecycle_1.0.3     stringi_1.7.8       yaml_2.3.6         \n[58] grid_4.2.2          crayon_1.5.2        deldir_1.0-6       \n[61] lattice_0.20-45     haven_2.5.1         splines_4.2.2      \n[64] hms_1.1.2           knitr_1.40          pillar_1.8.1       \n[67] reprex_2.0.2        glue_1.6.2          evaluate_0.17      \n[70] latticeExtra_0.6-30 data.table_1.14.4   modelr_0.1.9       \n[73] png_0.1-7           vctrs_0.5.0         tzdb_0.3.0         \n[76] cellranger_1.1.0    gtable_0.3.1        assertthat_0.2.1   \n[79] cachem_1.0.6        xfun_0.34           broom_1.0.1        \n[82] survival_3.4-0      googledrive_2.0.0   gargle_1.2.1       \n[85] memoise_2.0.1       cluster_2.1.4       ellipsis_0.3.2     \n\n\n\n\n",
    "preview": "posts/2022-11-23-class-5-intro-to-ggplot2-part2/class-5-intro-to-ggplot2-part2_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-05-class-10-programming-in-r-part-1/",
    "title": "Class 10: Programming in R (part 1)",
    "description": {},
    "author": [
      {
        "name": "Ryan Sheridan and Kent Riemondy",
        "url": {}
      }
    ],
    "date": "2022-12-05",
    "categories": [],
    "contents": "\nThe Rmarkdown for this class is on github\n\n\n# conditionally download all of the files used in rmarkdown from github \nsource(\"https://github.com/rnabioco/bmsc-7810-pbda/raw/main/_posts/2022-12-05-programming-in-r-pt1/download-files.R\")\n\n\n\nWhat is a function?\nAs an analyst you will eventually find yourself in the position of wanting to reuse a block of code. There are two general ways to do this:\ncopy-and-paste\nwrite a function\nA function is essentially a block of code that you’ve given a name and saved for later. Functions have several advantages:\nThey make your code easier to read\nThey reduce the chance of mistakes from repeated copying and pasting\nThey make it easier to adapt your code for different requirements\nFurther reading\nR for Data Science by Garrett Grolemund and Hadley Wickham\nAdvanced R by Hadley Wickham\n\n\nlibrary(tidyverse)\nlibrary(cowplot)\n\n\n\n\n# An example: you want to rescale a numeric vector so all values are between 0 and 1\na <- rnorm(n = 10)\na\n\n#>  [1]  0.4042638 -0.4117894 -0.3622097  0.2160782  0.4178757  1.4108871\n#>  [7] -0.3422958  0.0395028  0.2306987  0.6757950\n\nrng <- range(a)\n(a - rng[1]) / (rng[2] - rng[1])\n\n#>  [1] 0.44772246 0.00000000 0.02720158 0.34447561 0.45519055 1.00000000\n#>  [7] 0.03812722 0.24759863 0.35249706 0.59669635\n\n# What if we want to repeat this on other vectors?\n# One way is to copy and paste\nb <- rnorm(n = 10)\nc <- rnorm(n = 10)\nrng <- range(b)\nnew_b <- (b - rng[1]) / (rng[2] - rng[1])\nrng <- range(c)\nnew_c <- (c - rng[1]) / (rng[2] - rng[1])\n# A better way is to write a function...\n\n\n\n\nHow to write a function\nThere are three general steps for writing functions:\nPick a name\nIdentify the inputs\nAdd code to the body\n\n\n# Lets write a function to rescale a numeric vector\nrescale_vec <- function(x) {\n  \n  rng <- range(x)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale_vec(b)\nrescale_vec(c)\n\n\n\nWrite functions for the following bits of code\n\n\n# function 1\nx / sum(x)\n\n# function 2\n(x + y) / z\n\n# function 3\nsqrt(sum((x - mean(x))^2) / (length(x) - 1))\n\n\n\n\n\n\nThe function execution environment\nWhen running a function an execution environment is created, which is separate from the global environment\nThe execution environment contains objects created within the function\nWhen R searches for an object referenced by a function, the execution environment takes precedence\nIf an object is not found in the function environment, R will search in the global environment\n\nCan objects present in the global environment be referenced from within a function?\n\n\n# Earlier we saved a numeric vector \"a\"\na\n\n#>  [1]  0.4042638 -0.4117894 -0.3622097  0.2160782  0.4178757  1.4108871\n#>  [7] -0.3422958  0.0395028  0.2306987  0.6757950\n\nsum_nums <- function(x) {\n  x + a\n}\n# Yes!\nsum_nums(10)\n\n#>  [1] 10.404264  9.588211  9.637790 10.216078 10.417876 11.410887\n#>  [7]  9.657704 10.039503 10.230699 10.675795\n\n\nCan code executed within a function modify an object present in the global environment?\n\n\nsum_nums <- function(x) {\n  a <- x + a\n}\n# When we run sum_nums(), will this overwrite our original vector?\nsum_nums(10)\n# No! (not when using the '<-' assignment operator)\na\n\n#>  [1]  0.4042638 -0.4117894 -0.3622097  0.2160782  0.4178757  1.4108871\n#>  [7] -0.3422958  0.0395028  0.2306987  0.6757950\n\n\n\nA more relevant example\nThe brauer_gene_exp data contains a data set from a manuscript describing how gene expression changes in yeast under several nutrient limitation conditions. We’ll use this data to illustrate the utility and the power of functions.\nUsing the Brauer data lets create a scatter plot comparing growth rate vs expression for the gene YDL104C. Use facet_wrap() to create a separate plot for each nutrient.\n\n\nbrauer_gene_exp <- read_csv(\"data/brauer_gene_exp.csv.gz\")\n\n\n\n\n\n\nWhat if you want to create this plot for other genes? Write a function the takes a data.frame and systematic_name as inputs and creates scatter plots for each nutrient\n\n# Fill in the function body\n# You can include default values for your arguments\nplot_expr <- function(input, sys_name = \"YNL049C\") {\n  \n  ????\n  \n}\n\n\n\n\n\n\np <- plot_expr(\n  input = brauer_gene_exp, \n  sys_name = \"YDL104C\"\n)\n# You can also use the %>% pipe with your custom functions\np <- brauer_gene_exp %>%\n  plot_expr(sys_name = \"YDL104C\")\np\n\n\n\n\nModify our plotting function to add the gene name as the plot title and the molecular function (MF) as a subtitle\n\n\n\n\n\nbrauer_gene_exp %>%\n  plot_expr(\"YDL104C\")\n\n\n\n\n\nCopy-on-modify semantics\nAs you’ve seen objects that are passed to a function are not modified within the function by default. Intuitively you can think of each object being copied within the function environment to avoid modification of the original. However this would be memory inefficient and slow approach, as copying multiple large objects takes time and space.\nInstead R adopts a “copy-on-modify” approach with objects. Objects are only copied when it is necessary. The same is true of objects outside of functions.\n\n\nchange_to_char <- function(large_object) {\n  # large object is not a copy, but a reference\n  large_object\n  \n  # now a new copy of large_object is made\n  large_object <- as.character(large_object)\n  large_object\n}\n\nmat <- matrix(1:100, nrow = 10)\n# not copied\na <- mat\n\n# mat not copied yet\nmat[1:5, 1:5]\n\n# now a copy is made\nmat2 <- as.character(mat)\nmat2 <- as.data.frame(mat)\n\n\nConditional statements\nif statements allow you to execute code depending on defined conditions.\n\nif (condition) {\n  code executed when condition is TRUE\n  \n} else {\n  code executed when condition is FALSE\n}\n\nR has a set of operators that can be used to write conditional statements\nOperator\nDescription\n<\nless than\n<=\nless or equal\n>\ngreater than\n>=\ngreater or equal\n==\nequal\n!=\nnot equal\n!x\nnot x\nx | y\nx or y (returns a vector of logicals)\nx || y\nx or y (returns single TRUE or FALSE)\nx & y\nx and y (returns a vector of logicals)\nx && y\nx and y (returns single TRUE or FALSE)\nx %in% y\nx is present in y\n\nAdd an if statement to our plotting function to account for a missing gene name\n\n\nplot_expr <- function(input, sys_name) {\n  gg_data <- input %>%\n    filter(systematic_name == sys_name)\n  \n  plot_title <- gg_data$name[1]\n  plot_sub <- gg_data$MF[1]\n  \n  ????\n  \n  gg_data %>%\n    ggplot(aes(rate, expression, color = nutrient)) +\n    geom_point(size = 2) +\n    labs(title = plot_title, subtitle = plot_sub) +\n    facet_wrap(~ nutrient) +\n    theme(legend.position = \"none\")\n}\n\n\n\n\n\n\n\nbrauer_gene_exp %>%\n  plot_expr(\"YNL095C\")\n\n\n\n\nConditional statements can be linked together\n\n# Using 'else if'\nif (condition_1) {\n  executed when condition_1 is TRUE\n  \n} else if (condition_2) {\n  executed when condition_1 is FALSE and condition_2 is TRUE\n  \n} else {\n  executed when condition_1 and condition_2 are FALSE\n}\n# The 'and' operator\nif (condition_1 && condition_2) {\n  executed when condition_1 and condition_2 are TRUE\n  \n} else {\n  executed when condition_1 or condition_2 are FALSE\n}\n# The 'or' operator\nif (condition_1 || condition_2) {\n  executed when condition_1 or condition_2 are TRUE\n  \n} else {\n  executed when condition_1 and condition_2 are FALSE\n}\n\n\n\nMessages, warnings, and errors\nstop() warning(), message(), and stopifnot() are commonly used functions in R for reporting information and/or stopping execution based on a condition.\n\n\nstop(\"information about error to user, stops execution\")\nwarning(\"information about warning to user, does not stop execution\")\nmessage(\"information that is not an error or warning, does not stop execution\")\nstopifnot(2 + 2 != 4) # shortcut for if(condition is FALSE) stop()\n\n\nSee also tryCatch() for “catching” errors and performing alternative actions.\nChecking inputs\nWhen writing functions it can be useful to check input values to make sure they are valid. Lets modify our plotting function to check that sys_name is a string.\nis.character()\nis.numeric()\nis.logical()\nis.factor()\n\n\nplot_expr <- function(input, sys_name) {\n  \n  if (!is.character(sys_name)) {\n    stop(\"sys_name must be a string!\")\n  }\n  \n  gg_data <- input %>%\n    filter(systematic_name == sys_name)\n  \n  plot_title <- gg_data$name[1]\n  plot_sub <- gg_data$MF[1]\n  \n  if (plot_title == \"\") {\n    plot_title <- sys_name\n  }\n  \n  gg_data %>%\n    ggplot(aes(rate, expression, color = nutrient)) +\n    geom_point(size = 2) +\n    labs(title = plot_title, subtitle = plot_sub) +\n    facet_wrap(~ nutrient) +\n    theme(legend.position = \"none\")\n}\nbrauer_gene_exp %>%\n  plot_expr(\"YDL104C\")\n\n\n\n\nModify our plotting function to check that sys_name is present in the input. Hint: try the %in% operator\n\nplot_expr <- function(input, sys_name) {\n  \n  if (!is.character(sys_name)) {\n    stop(\"sys_name must be a string!\")\n  }\n  \n  if ( ???? ) {\n    stop( ???? )\n  }\n  \n  gg_data <- input %>%\n    filter(systematic_name == sys_name)\n  \n  plot_title <- gg_data$name[1]\n  plot_sub <- gg_data$MF[1]\n  \n  if (plot_title == \"\") {\n    plot_title <- sys_name\n  }\n  \n  gg_data %>%\n    ggplot(aes(rate, expression, color = nutrient)) +\n    geom_point(size = 2) +\n    labs(title = plot_title, subtitle = plot_sub) +\n    facet_wrap(~ nutrient) +\n    theme(legend.position = \"none\")\n}\n\n\n\n\n\n\nPassing arguments with the ellipsis (…)\nThe ellipsis allows a function to take an arbitrary number of arguments, which can then be passed to an inner function. This is nice when you have an inner function that has a lot of useful arguments. Lets first try this with our simple rescale_vec() function.\n\n\nrescale_vec <- function(x, ...) {\n  rng <- range(x, ...)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale_vec(a)\n\n#>  [1] 0.44772246 0.00000000 0.02720158 0.34447561 0.45519055 1.00000000\n#>  [7] 0.03812722 0.24759863 0.35249706 0.59669635\n\na[1] <- NA\nrescale_vec(a, na.rm = T)\n\n#>  [1]         NA 0.00000000 0.02720158 0.34447561 0.45519055 1.00000000\n#>  [7] 0.03812722 0.24759863 0.35249706 0.59669635\n\n\nModify our plotting function so the user can change the point size, shape, and alpha\n\n\n# A cumbersome way\nplot_expr <- function(input, sys_name, pt_size = 2, pt_shape = 1, pt_alpha = 1) {\n  input %>%\n    filter(systematic_name == sys_name) %>%\n    ggplot(aes(rate, expression, color = nutrient)) +\n    geom_point(size = pt_size, shape = pt_shape, alpha = pt_alpha) +\n    facet_wrap(~ nutrient) +\n    theme(legend.position = \"none\")\n}\n# With the ellipsis\nplot_expr <- function(input, sys_name, ...) {\n  input %>%\n    filter(systematic_name == sys_name) %>%\n    ggplot(aes(rate, expression, color = nutrient)) +\n    geom_point(...) +\n    facet_wrap(~ nutrient) +\n    theme(legend.position = \"none\")\n}\n# Now we can easily change the point size and shape\nplot_expr(\n  input = brauer_gene_exp,\n  sys_name = \"YDL104C\",\n  size = 5,\n  shape = 2,\n  alpha = 0.75\n)\n\n\n\n\n\nSaving your functions for later\nA good way to save commonly used functions is to keep them in a separate R script. You can load your functions using the source() command.\n\n\nsource(\"path/to/my_functions.R\")\n\n\n\nShow session info\n\n\nsessionInfo()\n\n#> R version 4.2.0 (2022-04-22)\n#> Platform: x86_64-apple-darwin17.0 (64-bit)\n#> Running under: macOS Big Sur/Monterey 10.16\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n#> \n#> locale:\n#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods  \n#> [7] base     \n#> \n#> other attached packages:\n#> [1] forcats_0.5.1   stringr_1.4.1   dplyr_1.0.10    purrr_0.3.5    \n#> [5] readr_2.1.2     tidyr_1.2.0     tibble_3.1.8    ggplot2_3.3.6  \n#> [9] tidyverse_1.3.1\n#> \n#> loaded via a namespace (and not attached):\n#>  [1] lubridate_1.8.0  assertthat_0.2.1 digest_0.6.30   \n#>  [4] utf8_1.2.2       R6_2.5.1         cellranger_1.1.0\n#>  [7] backports_1.4.1  reprex_2.0.1     evaluate_0.16   \n#> [10] highr_0.9        httr_1.4.4       pillar_1.8.1    \n#> [13] rlang_1.0.6      readxl_1.4.0     rstudioapi_0.13 \n#> [16] jquerylib_0.1.4  rmarkdown_2.14   labeling_0.4.2  \n#> [19] bit_4.0.4        munsell_0.5.0    broom_0.8.0     \n#> [22] compiler_4.2.0   modelr_0.1.8     xfun_0.32       \n#> [25] pkgconfig_2.0.3  htmltools_0.5.2  downlit_0.4.2   \n#> [28] tidyselect_1.2.0 fansi_1.0.3      crayon_1.5.2    \n#> [31] tzdb_0.3.0       dbplyr_2.2.1     withr_2.5.0     \n#> [34] grid_4.2.0       jsonlite_1.8.3   gtable_0.3.0    \n#> [37] lifecycle_1.0.3  DBI_1.1.3        magrittr_2.0.3  \n#> [40] scales_1.2.0     cli_3.4.1        stringi_1.7.8   \n#> [43] vroom_1.5.7      cachem_1.0.6     farver_2.1.0    \n#> [46] fs_1.5.2         xml2_1.3.3       bslib_0.3.1     \n#> [49] ellipsis_0.3.2   generics_0.1.3   vctrs_0.4.1     \n#> [52] cowplot_1.1.1    distill_1.5      tools_4.2.0     \n#> [55] bit64_4.0.5      glue_1.6.2       hms_1.1.2       \n#> [58] parallel_4.2.0   fastmap_1.1.0    yaml_2.3.6      \n#> [61] colorspace_2.0-3 rvest_1.0.2      memoise_2.0.1   \n#> [64] knitr_1.39       haven_2.5.0      sass_0.4.1\n\n\n\n\n",
    "preview": "posts/2022-12-05-class-10-programming-in-r-part-1/programming-in-r-pt1_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-17-class-4-intro-to-ggplot2/",
    "title": "Class 4: Introduction to ggplot2 (part1)",
    "description": {},
    "author": [
      {
        "name": "Michael Kaufman",
        "url": "https://github.com/mlkaufman"
      }
    ],
    "date": "2022-12-04",
    "categories": [],
    "contents": "\nThe Rmarkdown for this document is\nhttps://github.com/rnabioco/bmsc-7810-pbda/blob/main/_posts/2022-11-17-class-4-intro-to-ggplot2/class-4-intro-to-ggplot2.Rmd\nGoals for today\nIntroduction to plotting with the ggplot2 package\nThe grammar of graphics concept\nBasic plotting\nAdding additional information\nOther geometries\nMultiple geometries\nSaving plots\nAdditional Helpful Resources\nggplot2 package homepage :: https://ggplot2.tidyverse.org/\nggplot2 reference :: https://ggplot2.tidyverse.org/reference R for\nData Science :: https://r4ds.had.co.nz/\nggplot2 Book :: https://ggplot2-book.org/\nGallery of Plots and Examples :: https://r-graph-gallery.com/\nData Visualization with ggplot2 :: Cheat sheet ::\nhttps://github.com/rstudio/cheatsheets/blob/main/data-visualization.pdf\nThe ggplot2 Package\n\n\n\nThis package allows you to declaratively create graphics by giving a set\nof variables to map to aesthetics and then layer graphical directives to\nproduce a plot. It’s part of the tidyverse of R packages for data\nscience and analysis, sharing in their design philosophy. It’s an\nalternative to the built in R graphics and plotting functions.Written by Hadley Wickham\nGrammar of Graphics\n\n\n\nGrammar gives languages rules.\nGrammar has a technical meaning.\nGrammar makes language expressive.\n-Leland Wilkinson 1945-2021\nLayers of logical command flow and readability.\nLayers of ggplot2\n\n\n\nBasic Grammar\nPlot = data + aesthetics + geometry\ndata = the dataset, typically a dataframeaesthetics = map variables x and y to axisgeometry = type of graphic or plot to be rendered\nfacets = multiple plotsstatistics = add calculationstheme = make the plot pretty or follow a particular style\n\n\n# ggplot(<DATA>, aes(<MAPPINGS>)) + <GEOM_function>()\n\n?ggplot # bring up the ggplot function help\n\n\nConsider the Type of Data you want to plot\n\n\n\nData to Plot\nTo begin plotting we need to start with some data to visualize. Here we\ncan use a built-in dataset regarding Motor Trend Car Road Tests called\nmtcars. This dataset is a dataframe which is a key format for using\nwith ggplot. We can preview the data structure using the head()\nfunction.\n\n\n#some built in data.\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nThe data was extracted from the 1974 Motor Trend US magazine, and\ncomprises fuel consumption and 10 aspects of automobile design and\nperformance for 32 automobiles (1973–74 models).\nA data frame with 32 observations on 11 (numeric) variables.\n[, 1] mpg = Miles/(US) gallon\n[, 2] cyl = Number of cylinders\n[, 3] disp = Displacement (cu.in.)\n[, 4] hp = Gross horsepower\n[, 5] dra = Rear axle ratio\n[, 6] wt = Weight (1000 lbs)\n[, 7] qsec = 1/4 mile time\n[, 8] vs = Engine (0 = V-shaped, 1 = straight)\n[, 9] am = Transmission (0 = automatic, 1 = manual)\n[,10] gear = Number of forward gears\n[,11] carb = Number of carburetors-R Documentation\nBasic Plot\nUsing the basic ggplot grammar of graphics template we can produce a\nscatterplot from the dataframe.\n\n\n# ggplot(<DATA>, aes(<MAPPINGS>)) + <GEOM_function>()\n\n\nThe first part of the expression calls the ggplot function and takes\nthe dataframe and the aes function which are the aesthetics\nmappings. In this case we are mapping the x-axis to be the wt variable\nand the y-axis to be the mpg variable . If you only evaluate the first\npart this is what you get:\n\n\nggplot(mtcars, aes(x=wt, y=mpg))\n\n\n\nNext we have to add the geometry layer to be able to actually see the\ndata. Here we are adding the geom_point geometry which allows you to\nvisualize the data as points. You use a plus sign to add these\nadditional layers.\n\n\nggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()\n\n\n\nWe can change the data being plotted by picking a different column from\nthe dataframe. For instance here we are plotting the horsepower(hp)\nversus miles per gallon(mpg). Also note that we can make the code more\nreadable by placing proceeding layers on a different line after the plus\nsign. A common error is misplacing the plus sign. It must be trailing on\nthe line before the next layer.\n\n\nggplot(mtcars, aes(x=hp, y=mpg)) + \n  geom_point()\n\n\n\nExercise: Try building a scatterplot on your own. This time plot the\nvariables corresponding to the number of cylinders and the type of\ntransmission.\n\n\n\nExercise: Modify the scatterplot to plot horsepower instead of the type\nof transmission. Can you start to see a relationship with the data?\nAdding Additional Information to the Plot\nTitle\nWe can add a title to the plot simply by adding another layer and the\nggtitle() function.\n\n\nggplot(mtcars, aes(x=hp, y=mpg)) + \n  geom_point() +\n  ggtitle(\"1974 Cars: Horsepower vs Miles Per Gallon\")\n\n\n\nX and Y axis Labels\nWe can overwrite the default labels and add our own to the x and y axis\nby using the xlab() and ylab() functions respectively.\n\n\nggplot(mtcars, aes(x=hp, y=mpg)) + \n  geom_point() +\n  ggtitle(\"1974 Cars: Horsepower vs Miles Per Gallon\") +\n  ylab(\"miles per gallon\") + \n  xlab(\"horsepower\")\n\n\n\nSet title and axis labels in one layer\n\n\nggplot(mtcars, aes(x=hp, y=mpg, alpha = 0.5)) + \n  geom_point() +\n  labs(x = \"Horepower\", \n    y = \"Miles Per Gallon\", \n    title = \"Horsepower vs Miles Per Gallon Scatterplot\",\n    subtitle = \"Motor Trend Car Road Tests - 1974\",\n    caption = \"Smith et al. 1974\")\n\n\n\nNotice that we also added an alpha aesthetic which helps us visualize\noverlapping points. We can add a show.legend = FALSE argument to the\ngeom_point function to remove the alpha legend and clean up the plot\nfigure. Let’s try it. You can also specify a vector of aesthetics to\ndisplay.\nCheck the documentation ?geom_point.\nGetting Geometry Specific Help\nWe can easily add a third bit of information to the plot by using the\ncolor aesthetic. Each geometry has its own list of aesthetics that you\ncan add and modify. Consult the help page for each one.\n\n\n?geom_point() # bring up the help page for geom_point()\n\n\nAdding the Color Aesthetic\nHere we are adding the color aesthetic.\n\n\nggplot(mtcars, aes(x=hp, y=mpg, color=cyl)) + \n  geom_point() +\n  ggtitle(\"Modern Cars: Horsepower vs Miles Per Gallon\") +\n  ylab(\"miles per gallon\") + \n  xlab(\"horsepower\")\n\n\n\nAnd we can relabel the legend title for the new color aesthetic to make\nit more readable.\n\n\nggplot(mtcars, aes(x=hp, y=mpg, color=cyl)) + \n  geom_point() +\n  ggtitle(\"Modern Cars: Horsepower vs Miles Per Gallon\") +\n  ylab(\"miles per gallon\") + \n  xlab(\"horsepower\") +\n  labs(color=\"#cylinders\")\n\n\n\nA Fourth Aesthetic\nYou can even continue to add even more information to the plot through\nadditional aesthetics. Though this might be a bit much.\n\n\nggplot(mtcars, aes(x=hp, y=mpg, color=cyl, size = wt)) + \n  geom_point() +\n  ggtitle(\"Modern Cars: Horsepower vs Miles Per Gallon\") +\n  ylab(\"miles per gallon\") + \n  xlab(\"horsepower\") +\n  labs(color=\"#cylinders\", size=\"weight (x1000lb)\")\n\n\n\nInstead we can use a specific value instead of the wt variable to\nadjust the size of the dots.\n\n\nggplot(mtcars, aes(x=hp, y=mpg, color=cyl, size = 3)) + \n  geom_point() +\n  ggtitle(\"Modern Cars: Horsepower vs Miles Per Gallon\") +\n  ylab(\"miles per gallon\") + \n  xlab(\"horsepower\") +\n  labs(color=\"#cylinders\")\n\n\n\nOther Geometries\nThere are many other geometries that you can use in your plots.\nhttps://ggplot2.tidyverse.org/reference\nHere is a short list:\ngeom_point(): scatterplot\ngeom_line(): lines connecting points by increasing value of x\ngeom_path(): lines connecting points in sequence of appearance\ngeom_boxplot(): box and whiskers plot for categorical variables\ngeom_bar(): bar charts for categorical x axis\ngeom_col(): bar chart where heights of the bars represent values in the\ndata\ngeom_histogram(): histogram for continuous x axis\ngeom_violin(): distribution kernel of data dispersion\ngeom_smooth(): function line based on data\ngeom_bin2d(): heatmap of 2d bin counts\ngeom_contour(): 2d contours of a 3d surface\ngeom_count(): count overlapping points\ngeom_density(): smoothed density estimates\ngeom_dotplot(): dot plot\ngeom_hex(): hexagonal heatmap of 2d bin counts\ngeom_freqpoly(): histogram and frequency polygons\ngeom_jitter(): jittered point plot geom_polygon(): polygons\ngeom_line()\nBut utilizing the right plot to efficiently show your data is key. Here\nwe swapped the geom_point for geom_line to see what would happen. You\ncould also try something like geom_bin2d()\n\n\nggplot(mtcars, aes(x=hp, y=mpg, color=cyl)) + \n  geom_line() +\n  ggtitle(\"Modern Cars: Horsepower vs Miles Per Gallon\") +\n  ylab(\"miles per gallon\") + \n  xlab(\"horsepower\") +\n  labs(color=\"#cylinders\")\n\n\n\nPlotting the Categories as a Bar Chart with geom_col()\nThe geom_col() geometry is a type of bar plot that uses the heights of\nthe bars to represent values in the data. Let’s look at plotting this\ntype of data for the cars in this dataset.\n\n\n?geom_col()\n\n\n\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nLooking back at the data structure of mtcars, we see that the names of\nthe cars are stored as the row names of the data frame. We can access\nthis using the rownames()function and use it in subsequent plots.\nQ: What was another way to address this issue, discussed in the first\nblock?\n\n\nrownames(mtcars)\n\n [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n[10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n[13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n[16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n[19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n[22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n[25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n[28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n[31] \"Maserati Bora\"       \"Volvo 142E\"         \n\n\n\nggplot(mtcars, aes(x=rownames(mtcars), y=mpg)) + \n  geom_col() +\n  ggtitle(\"1974 Cars: Miles Per Gallon\")\n\n\n\nYou will learn other ways to make this more legible later. For a quick\nfix we can swap the x and y mappings.\n\n\nggplot(mtcars, aes(y=rownames(mtcars), x=mpg)) + \n  geom_col() +\n  ggtitle(\"1974 Cars: Miles Per Gallon\")\n\n\n\nWe can reorder the data to make it easier to visualize important\ninformation.\n\n\nggplot(mtcars, aes(y=reorder(rownames(mtcars), mpg), x=mpg)) + \n  geom_col() +\n  ggtitle(\"1974 Cars: Ranked by Miles Per Gallon\")\n\n\n\nExercise: Plot a bar chart using geom_col() with the mtcar dataset. Plot\nthe names of the cars ranked by the weight of each car. Try adding a\nthird aesthetic color for horsepower.\n\n\n\nMultiple Geometries\nYou can also add another layer of geometry to the same ggplot. Notice\nyou can have two separate aesthetic declarations and they have moved\nfrom the ggplot function to their respective geom_ functions.\n\n\n# ggplot(data = <DATA>, mapping = aes(<MAPPINGS>)) + \n#   <GEOM_FUNCTION1>() + \n#   <GEOM_FUNCTION2>() \n\n# OR\n\n# ggplot(data = <DATA>) + \n#   <GEOM_FUNCTION1>(mapping = aes(<MAPPINGS>)) + \n#   <GEOM_FUNCTION2>(mapping = aes(<MAPPINGS>)) \n\nggplot(mtcars) +\n  geom_point(aes(x=hp, y=mpg)) +\n  geom_line(aes(x=hp, y=mpg, color=cyl)) +\n  ggtitle(\"Modern Cars: Horsepower vs Miles Per Gallon\") +\n  ylab(\"miles per gallon\") + \n  xlab(\"horsepower\") +\n  labs(color=\"#cylinders\")\n\n\n\nThis particular geometry addition isn’t very useful.\nExercise: Try adding geom_smooth() instead of geom_line().\nSaving Plots\nSaving these plots is easy! Simply call the ggsave() function to save\nthe last plot that you created. You can specify the file format by\nchanging the extension after the filename.\n\n\nggsave(\"plot.png\") # saves the last plot to a PNG file in the current working directory\n\n\nYou can also specify the dots per inch and the width of height of the\nimage to ensure publication quality figures upon saving.\n\n\nggsave(\"plot-highres.png\", dpi = 300, width = 8, height = 4) # you can specify the dots per inch (dpi) and the width and height parameters\n\n\nExercise: Try saving the last plot that we produced as a jpg. Can you\nnavigate to where it saved and open it on your computer?\nMore Examples\nLets take a look at gallery resource to preview different plot types and\nget ideas for our own plots.https://r-graph-gallery.com/\nSessionInfo\n\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] forcats_0.5.2   stringr_1.4.1   dplyr_1.0.10    purrr_0.3.5    \n[5] readr_2.1.3     tidyr_1.2.1     tibble_3.1.8    ggplot2_3.4.0  \n[9] tidyverse_1.3.2\n\nloaded via a namespace (and not attached):\n [1] lubridate_1.8.0     assertthat_0.2.1    digest_0.6.30      \n [4] utf8_1.2.2          R6_2.5.1            cellranger_1.1.0   \n [7] backports_1.4.1     reprex_2.0.2        evaluate_0.17      \n[10] httr_1.4.4          highr_0.9           pillar_1.8.1       \n[13] rlang_1.0.6         googlesheets4_1.0.1 readxl_1.4.1       \n[16] rstudioapi_0.14     jquerylib_0.1.4     rmarkdown_2.17     \n[19] labeling_0.4.2      googledrive_2.0.0   munsell_0.5.0      \n[22] broom_1.0.1         compiler_4.2.2      modelr_0.1.9       \n[25] xfun_0.34           pkgconfig_2.0.3     htmltools_0.5.3    \n[28] downlit_0.4.2       tidyselect_1.2.0    fansi_1.0.3        \n[31] crayon_1.5.2        tzdb_0.3.0          dbplyr_2.2.1       \n[34] withr_2.5.0         grid_4.2.2          jsonlite_1.8.3     \n[37] gtable_0.3.1        lifecycle_1.0.3     DBI_1.1.3          \n[40] magrittr_2.0.3      scales_1.2.1        cli_3.4.1          \n[43] stringi_1.7.8       cachem_1.0.6        farver_2.1.1       \n[46] fs_1.5.2            xml2_1.3.3          bslib_0.4.1        \n[49] ellipsis_0.3.2      generics_0.1.3      vctrs_0.5.0        \n[52] distill_1.5         tools_4.2.2         glue_1.6.2         \n[55] hms_1.1.2           fastmap_1.1.0       yaml_2.3.6         \n[58] colorspace_2.0-3    gargle_1.2.1        rvest_1.0.3        \n[61] memoise_2.0.1       knitr_1.40          haven_2.5.1        \n[64] sass_0.4.2         \n\n\n\n\n",
    "preview": "posts/2022-11-17-class-4-intro-to-ggplot2/class-4-intro-to-ggplot2_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-11-15-class-3-reshaping-data-into-a-tidy-format/",
    "title": "Class 3: Reshaping data into a tidy format",
    "description": {},
    "author": [
      {
        "name": "Kent Riemondy",
        "url": "https://github.com/kriemo"
      }
    ],
    "date": "2022-12-02",
    "categories": [],
    "contents": "\n\n\n\nThe Rmarkdown for this class is on github\nGoals for today\nDiscuss wide and long (tidy) data representations for analysis\nIntroduce tidyr for “tidying” rectangular data\nJoining related tables with dplyr\nStrategies for missing data\n\n“Data Scientists spend up to 80% of the time on data cleaning and 20 percent of their time on actual data analysis.”\n– Exploratory Data Mining and Data Cleaning. Dasu and Johnson\n\nWide versus long data formats\nData can be represented in multiple formats. Today we will discuss two common tabular formats for organizing data for analysis in R. Consider the following dataset, which contains GDP estimates per person for countries throughout history. This representation of the data is commonly referred to as ‘wide’ data format, which is a matrix-like format containing samples or features as rows or columns, with values associated with each sample and feature.\n\n# A tibble: 195 × 252\n   country `1799` `1800` `1801` `1802` `1803` `1804` `1805` `1806` `1807` `1808`\n   <chr>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1 Afghan…    683    683    683    683    683    683    683    683    683    683\n 2 Angola     700    702    705    709    711    714    718    721    725    727\n 3 Albania    755    755    755    755    755    756    756    756    756    756\n 4 Andorra   1360   1360   1360   1360   1370   1370   1370   1370   1380   1380\n 5 United…   1130   1130   1140   1140   1150   1150   1160   1160   1160   1170\n 6 Argent…   1730   1730   1740   1740   1750   1760   1760   1770   1770   1780\n 7 Armenia    582    582    582    582    582    582    582    582    582    582\n 8 Antigu…    857    857    857    857    857    857    857    858    858    858\n 9 Austra…    925    930    936    941    947    952    956    962    968    973\n10 Austria   2090   2100   2110   2120   2130   2130   2140   2150   2160   2170\n# … with 185 more rows, and 241 more variables: `1809` <dbl>, `1810` <dbl>,\n#   `1811` <dbl>, `1812` <dbl>, `1813` <dbl>, `1814` <dbl>, `1815` <dbl>,\n#   `1816` <dbl>, `1817` <dbl>, `1818` <dbl>, `1819` <dbl>, `1820` <dbl>,\n#   `1821` <dbl>, `1822` <dbl>, `1823` <dbl>, `1824` <dbl>, `1825` <dbl>,\n#   `1826` <dbl>, `1827` <dbl>, `1828` <dbl>, `1829` <dbl>, `1830` <dbl>,\n#   `1831` <dbl>, `1832` <dbl>, `1833` <dbl>, `1834` <dbl>, `1835` <dbl>,\n#   `1836` <dbl>, `1837` <dbl>, `1838` <dbl>, `1839` <dbl>, `1840` <dbl>, …\n\nThe wide matrix-like format is very useful and a common format used for statistics and machine learning. Matrices can take advantage of optimized numerical routines and are the data representation of mathematical matrices. We will work with matrices later in class, particularly with their use to generate heatmaps.\nRepresenting data in a matrix has a few practical implications:\nThere is only 1 type of data stored in matrix (e.g. each cell is the same unit of observation, the GDP per person). To store more values you need multiple matrices.\nThe format is not easily manipulated with dplyr/tidyverse.\nData in a matrix can be instead formatted in a long (aka “tidy”) format.\n\n# A tibble: 48,945 × 3\n   country     year    gdp\n   <chr>       <chr> <dbl>\n 1 Afghanistan 1799    683\n 2 Afghanistan 1800    683\n 3 Afghanistan 1801    683\n 4 Afghanistan 1802    683\n 5 Afghanistan 1803    683\n 6 Afghanistan 1804    683\n 7 Afghanistan 1805    683\n 8 Afghanistan 1806    683\n 9 Afghanistan 1807    683\n10 Afghanistan 1808    683\n# … with 48,935 more rows\n\nThe long format of this data simplifies the many columns of a matrix into a 3 column data.frame containing 3 variables (country, year, and gdp).\nTidy data format\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham\n\nTidy data is a concept introduced by Hadley Wickham, chief scientist at Rstudio, and developer of many of the tidyverse packages. A tidy dataset is structured in a manner to be most effectively processed in R using the tidyverse. It makes the data easier to plot, easier to perform computations and perform complex tasks.\nMost data tables that you’ve worked with are probably not tidy. It takes experience to understand the best way to format the data. As you work more in R and the tidyverse this will become more natural.\nTidy data has the following attributes:\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\nWhat is a variable, what is an observation, and what is a value?\nA value is a number or word.\nEvery value belongs to a variable and an observation\nA variable contains all values that measure the same attribute (e.g. height, temperature, duration, magnitude) across units.\nAn observation contains all values measured on the same unit (e.g. the same individual, day, country, gene).\n\n\n\nShown below is a simple data table in a tidy format, provided by the tidyr package.\n\n\nlibrary(tidyr)\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nThe same data, represented in wide, matrix-like format, would require 2 tables:\ne.g a matrix with the cases values per country.\n\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n* <chr>        <int>  <int>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ne.g a matrix with the population values per country\n\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n* <chr>            <int>      <int>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\nWhat advantages does the tidy format provide?\nEasy to generate summaries of the data.\ne.g. via group_by() -> summarize()\nEasy to plot the data using the ggplot2 framework (more on that in later classes)\nVery easy to join multiple data tables based on key values.\nSome disadvantages:\nNot space efficient\nNot intuitive\nDoesn’t interface well with traditional machine learning and statistical approaches.\nConverting between long and wide formats using tidyr\nThe tidyr package provides functionality to convert datasets into tidy formats.\npivot_longer(): convert wide data to long data\npivot_wider(): convert long data to wide data\nseparate(): split a single column into multiple columns\nunite(): combine multiple columns into single columns\nReshaping wide data to long\nThe pivot_longer function requires specifying the columns to pivot using the tidyselect syntax. This syntax is used elsewhere in the tidyverse and is a useful shorthand to avoid listing all columns of interest.\npivot_longer(tbl, cols = <...>)\n\n\n\nFigure 1: Tables from tidyr cheatsheet from https://posit.co/wp-content/uploads/2022/10/tidyr.pdf\n\n\n\n\n\npivot_longer(table4a, cols = `1999`:`2000`) # pivot columns from 1999 -> 2000\n\n# A tibble: 6 × 3\n  country     name   value\n  <chr>       <chr>  <int>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\npivot_longer(table4a, cols = -country) # pivot all columns not matching country\n\n# A tibble: 6 × 3\n  country     name   value\n  <chr>       <chr>  <int>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\nReshaping long data to wide\npivot_wider(tbl, names_from = <...>, values_from = <...>)\nnames_from: the column whose values will become new columns in the result.values_from: the column whose values will be in the new columns.\n\n\n\n\n\npivot_wider(table2, names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nSeparate\nseparate is useful for dealing with data in which a single column contains multiple variables.\nseperate(tbl, col = <...>, into = c(<..., ..., ...>), sep = \"...\")\ncol: column to split into multiple columnsinto: column names of new columns to be generated, supplied as a character vector (use quotes).sep: the separator used to split values in the col column. Can be a character (_) or a integer to indicate the character position to split (2).\n\n\n\n\n\nseparate(table3, col = rate, into = c(\"cases\", \"pop\"), sep = \"/\")\n\n# A tibble: 6 × 4\n  country      year cases  pop       \n  <chr>       <int> <chr>  <chr>     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nunite\nunite is the inverse operation of separate and will join multiple columns together. You can also use str_c from stringr or paste() from base R with mutate.\n\n\n\n\n\nunite(table5, \"year\", century:year, sep = \"\")\n\n# A tibble: 6 × 3\n  country     year  rate             \n  <chr>       <chr> <chr>            \n1 Afghanistan 1999  745/19987071     \n2 Afghanistan 2000  2666/20595360    \n3 Brazil      1999  37737/172006362  \n4 Brazil      2000  80488/174504898  \n5 China       1999  212258/1272915272\n6 China       2000  213766/1280428583\n\n#mutate(table5, year = str_c(century, year))\n\n\nExercises\nUse the gapminder GDP dataset to perform the following tasks and answer the following quesitons:\nConvert the GDP dataset into tidy format.\n\n\nlibrary(tidyverse)\ngdp_data <- read_csv(\"data/income_per_person.csv\")\npivot_longer(gdp_data, cols = `1799`:`2049`)\n\n# A tibble: 48,945 × 3\n   country     name  value\n   <chr>       <chr> <dbl>\n 1 Afghanistan 1799    683\n 2 Afghanistan 1800    683\n 3 Afghanistan 1801    683\n 4 Afghanistan 1802    683\n 5 Afghanistan 1803    683\n 6 Afghanistan 1804    683\n 7 Afghanistan 1805    683\n 8 Afghanistan 1806    683\n 9 Afghanistan 1807    683\n10 Afghanistan 1808    683\n# … with 48,935 more rows\n\ngdp_tidy <- pivot_longer(gdp_data, \n                         cols = -country, \n                         names_to = \"year\", \n                         values_to = \"gdp\")\ngdp_tidy\n\n# A tibble: 48,945 × 3\n   country     year    gdp\n   <chr>       <chr> <dbl>\n 1 Afghanistan 1799    683\n 2 Afghanistan 1800    683\n 3 Afghanistan 1801    683\n 4 Afghanistan 1802    683\n 5 Afghanistan 1803    683\n 6 Afghanistan 1804    683\n 7 Afghanistan 1805    683\n 8 Afghanistan 1806    683\n 9 Afghanistan 1807    683\n10 Afghanistan 1808    683\n# … with 48,935 more rows\n\nWhich country had the highest GDP per person in 1985?\n\n\n# using the wide data\nslice_max(gdp_data, `1985`)\n\n# A tibble: 1 × 252\n  country `1799` `1800` `1801` `1802` `1803` `1804` `1805` `1806` `1807` `1808`\n  <chr>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 Brunei    1710   1710   1710   1710   1710   1710   1710   1710   1710   1710\n# … with 241 more variables: `1809` <dbl>, `1810` <dbl>, `1811` <dbl>,\n#   `1812` <dbl>, `1813` <dbl>, `1814` <dbl>, `1815` <dbl>, `1816` <dbl>,\n#   `1817` <dbl>, `1818` <dbl>, `1819` <dbl>, `1820` <dbl>, `1821` <dbl>,\n#   `1822` <dbl>, `1823` <dbl>, `1824` <dbl>, `1825` <dbl>, `1826` <dbl>,\n#   `1827` <dbl>, `1828` <dbl>, `1829` <dbl>, `1830` <dbl>, `1831` <dbl>,\n#   `1832` <dbl>, `1833` <dbl>, `1834` <dbl>, `1835` <dbl>, `1836` <dbl>,\n#   `1837` <dbl>, `1838` <dbl>, `1839` <dbl>, `1840` <dbl>, `1841` <dbl>, …\n\n# using the long tidy data\ngdp_tidy %>% \n  filter(year == \"1985\") %>% \n  arrange(desc(gdp)) %>% \n  slice(1)\n\n# A tibble: 1 × 3\n  country year    gdp\n  <chr>   <chr> <dbl>\n1 Brunei  1985  77900\n\nWhat was the mean worldwide GDP in the year 1999?\n\n\n# using the tidy data\ngdp_tidy %>% \n  filter(year == \"1999\") %>% \n  summarize(mean_gdp = mean(gdp))\n\n# A tibble: 1 × 1\n  mean_gdp\n     <dbl>\n1   15202.\n\ngdp_tidy %>% \n  filter(year == \"1999\") %>% \n  pull(gdp) %>% \n  mean()\n\n[1] 15201.59\n\n# using the wide data\ngdp_data$`1999` %>% \n  mean()\n\n[1] 15201.59\n\nmean(gdp_data$`1999`)\n\n[1] 15201.59\n\nWhich country had the highest average GDP in the 19th century?\n\n\ngdp_tidy %>% \n  filter(str_detect(year, \"^18\")) %>% \n  group_by(country) %>% \n  summarize(avg_gdp = mean(gdp)) %>% \n  arrange(desc(avg_gdp))\n\n# A tibble: 195 × 2\n   country        avg_gdp\n   <chr>            <dbl>\n 1 United Kingdom   5531.\n 2 United States    4993.\n 3 Netherlands      4604.\n 4 Australia        4118.\n 5 Belgium          4079.\n 6 Switzerland      3667.\n 7 New Zealand      3501.\n 8 Monaco           3454.\n 9 Uruguay          3255.\n10 Denmark          3253 \n# … with 185 more rows\n\nBinds/Joins\ncolumn binds\n\n\n\nFigure 2: from the dplyr cheatsheet at https://posit.co/wp-content/uploads/2022/10/data-transformation-1.pdf\n\n\n\nbind_cols(tbl_1, tbl_2, ...)\nbind_cols will bind the columns from 2 or more tables into 1 table. Note that with column binds you need to ensure that each table has the same number of rows.\n\n\nlibrary(dplyr)\ntbl1 <- tibble(x = 1:3)\ntbl2 <- tibble(y = 3:5)\nbind_cols(tbl1, tbl2)\n\n# A tibble: 3 × 2\n      x     y\n  <int> <int>\n1     1     3\n2     2     4\n3     3     5\n\nrow binds\nbind_rows binds rows from multiple tables into one table. Unlike with bind_cols the number of columns doesn’t need to match when using bind_rows.\nbind_rows(tbl_1, tbl_2, ..., .id = NULL)\n\n\n\n\n\none <- starwars[1:4, 1:4]\ntwo <- starwars[9:12, 1:4]\n\nbind_rows(one, two)\n\n# A tibble: 8 × 4\n  name              height  mass hair_color   \n  <chr>              <int> <dbl> <chr>        \n1 Luke Skywalker       172    77 blond        \n2 C-3PO                167    75 <NA>         \n3 R2-D2                 96    32 <NA>         \n4 Darth Vader          202   136 none         \n5 Biggs Darklighter    183    84 black        \n6 Obi-Wan Kenobi       182    77 auburn, white\n7 Anakin Skywalker     188    84 blond        \n8 Wilhuff Tarkin       180    NA auburn, grey \n\nYou can also use a list of data.frames with bind_rows. If the list is named, you can use the .id argument to store a column specifying the name of the data.frame in the output.\n\n\nlst_of_dfs <- list(a = one,\n                   b = two)\nbind_rows(lst_of_dfs)\n\n# A tibble: 8 × 4\n  name              height  mass hair_color   \n  <chr>              <int> <dbl> <chr>        \n1 Luke Skywalker       172    77 blond        \n2 C-3PO                167    75 <NA>         \n3 R2-D2                 96    32 <NA>         \n4 Darth Vader          202   136 none         \n5 Biggs Darklighter    183    84 black        \n6 Obi-Wan Kenobi       182    77 auburn, white\n7 Anakin Skywalker     188    84 blond        \n8 Wilhuff Tarkin       180    NA auburn, grey \n\nbind_rows(lst_of_dfs, .id = \"source_table\")\n\n# A tibble: 8 × 5\n  source_table name              height  mass hair_color   \n  <chr>        <chr>              <int> <dbl> <chr>        \n1 a            Luke Skywalker       172    77 blond        \n2 a            C-3PO                167    75 <NA>         \n3 a            R2-D2                 96    32 <NA>         \n4 a            Darth Vader          202   136 none         \n5 b            Biggs Darklighter    183    84 black        \n6 b            Obi-Wan Kenobi       182    77 auburn, white\n7 b            Anakin Skywalker     188    84 blond        \n8 b            Wilhuff Tarkin       180    NA auburn, grey \n\nJoins\nJoin operations are used to join one table with another table by matching the values shared in particular columns. They are used join datasets with a shared variable.\nThere are multiple way to join two tables, depending on how you want to handle different combinations of values present in two tables.\nFrom the dplyr documentation:\nThe mutating joins add columns from y to x, matching rows based on the keys:\n\ninner_join(): includes all rows in x and y.\n\nleft_join(): includes all rows in x.\n\nright_join(): includes all rows in y.\n\nfull_join(): includes all rows in x or y.\n\nIf a row in x matches multiple rows in y, all the rows in y will \nbe returned once for each matching row in x.\nConsider the following simple tables:\n\n\nband_members\n\n# A tibble: 3 × 2\n  name  band   \n  <chr> <chr>  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\n\n\nband_instruments\n\n# A tibble: 3 × 2\n  name  plays \n  <chr> <chr> \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\nExercise:\nPerform an inner, left, and full join of the band_members and band_instruments:\n\n\ninner_join(band_members, band_instruments)\n\n# A tibble: 2 × 3\n  name  band    plays \n  <chr> <chr>   <chr> \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\nleft_join(band_members, band_instruments)\n\n# A tibble: 3 × 3\n  name  band    plays \n  <chr> <chr>   <chr> \n1 Mick  Stones  <NA>  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\nfull_join(band_members, band_instruments)\n\n# A tibble: 4 × 3\n  name  band    plays \n  <chr> <chr>   <chr> \n1 Mick  Stones  <NA>  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith <NA>    guitar\n\nThe Joining, by = \"name\" message indicates which columns were used to determine matching rows between the two tables. This is auto-detected based on shared column names. You can use the by argument to explicitly specify the columns you’d like to join, which is useful if the columns of interest have different names in the two tables.\nMissing data\nJoin operations will often generate missing data (e.g. NAs).\nZeroes, NA, NaN and NULL\nDon’t use use zeroes to represent missing data. 0 is valid observed value.\nNA (Not Available) is most often use to represent missing data.\nNaN (Not a Number) is the result of an undefined operation, e.g. 0 / 0.\nNULL means “undefined” and is only used in a programming context (i.e., a function that returns NULL). You can’t put NULL values in a data frame.\nLet’s examine a data frame with some missing data.\n\n\nstarwars\n\n# A tibble: 87 × 14\n   name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n   <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n 1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n 2 C-3PO          167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n 3 R2-D2           96    32 <NA>    white,… red        33   none  mascu… Naboo  \n 4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n 5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n 6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n 7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n 8 R5-D4           97    32 <NA>    white,… red        NA   none  mascu… Tatooi…\n 9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n# … with 77 more rows, 4 more variables: species <chr>, films <list>,\n#   vehicles <list>, starships <list>, and abbreviated variable names\n#   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\nfilter with is.na()\nYou can identify variables with NA values by combining filter() and is.na().\n\n\n# find rows where mass is NA\nfilter(starwars, is.na(mass))\n\n# find rows where mass is *not* NA\nfilter(starwars, !is.na(mass))\n\n\nna.omit()\nYou can remove all rows containing NA values with na.omit().\n\n\nna.omit(starwars)\n\n\nComputing with NA values\nExclude NA values from operations with na.rm = TRUE.\n\n\nstarwars$mass\n# if NAs are present, the result is NA\nsum(starwars$mass)\n# solution: drop NAs from the calculation\nsum(starwars$mass, na.rm = TRUE)\n\n\n\n\ngroup_by(starwars, species) %>% \n  summarize(avg_mass = mean(mass, na.rm = TRUE))\n\n\nAlso you can remove NaN values by detecting for their presence using is.nan(). These values occur because a few species don’t have any characters with values for the mass column, and computing the mean of an empty vector is NaN.\n\n\ngroup_by(starwars, species) %>% \n  summarize(avg_mass = mean(mass, na.rm = TRUE)) %>% \n  filter(!is.nan(avg_mass))\n\n# A tibble: 32 × 2\n   species   avg_mass\n   <chr>        <dbl>\n 1 Aleena        15  \n 2 Besalisk     102  \n 3 Cerean        82  \n 4 Clawdite      55  \n 5 Droid         69.8\n 6 Dug           40  \n 7 Ewok          20  \n 8 Geonosian     80  \n 9 Gungan        74  \n10 Human         82.8\n# … with 22 more rows\n\nReplacing NA values\nLet’s replace the NA vaues in hair_color with a string missing data.\n\n\nstarwars %>% \n  mutate(new_hair_color = ifelse(is.na(hair_color), \n                                 \"missing data\",\n                                 hair_color)) %>% \n  select(hair_color, new_hair_color)\n\n# A tibble: 87 × 2\n   hair_color    new_hair_color\n   <chr>         <chr>         \n 1 blond         blond         \n 2 <NA>          missing data  \n 3 <NA>          missing data  \n 4 none          none          \n 5 brown         brown         \n 6 brown, grey   brown, grey   \n 7 brown         brown         \n 8 <NA>          missing data  \n 9 black         black         \n10 auburn, white auburn, white \n# … with 77 more rows\n\nWe can also replace with values from another column, such as species.\n\n\nstarwars %>% \n  mutate(new_hair_color = ifelse(is.na(hair_color), \n                                 species,\n                                 hair_color)) %>% \n  select(hair_color, new_hair_color)\n\n# A tibble: 87 × 2\n   hair_color    new_hair_color\n   <chr>         <chr>         \n 1 blond         blond         \n 2 <NA>          Droid         \n 3 <NA>          Droid         \n 4 none          none          \n 5 brown         brown         \n 6 brown, grey   brown, grey   \n 7 brown         brown         \n 8 <NA>          Droid         \n 9 black         black         \n10 auburn, white auburn, white \n# … with 77 more rows\n\nifelse is a base R function that operates on vectors and is useful when you want to replace single values.\ncase_when()\nIf you want to perform more complex operations use case_when() from dplyr. case_when() is equivalent to performing multiple nested ifelse() operations, whereby if the first operation is not TRUE, then check for the second condition, repeating for each condition until there are no more statements.\nthe syntax for case when is :\n`case_when(conditional statement ~ \"value in result\",\n           conditional statement #2 ~ \"another value in result\",\n           TRUE ~ \"default if neither conditional statement 1 or 2 are TRUE\")`\nHere is an example from the documentation. Make a new column called type. Return “large” if the height or mass of a character is > 200. If that is not true, then return “robot” if the species is “Droid”. If that is not TRUE, then default to returning “other”.\n\n\nstarwars %>%\n  select(name:mass, species) %>%\n  mutate(type = case_when(height > 200 | mass > 200 ~ \"large\",\n                          species == \"Droid\"        ~ \"robot\",\n                          TRUE                      ~ \"other\"))\n\n# A tibble: 87 × 5\n   name               height  mass species type \n   <chr>               <int> <dbl> <chr>   <chr>\n 1 Luke Skywalker        172    77 Human   other\n 2 C-3PO                 167    75 Droid   robot\n 3 R2-D2                  96    32 Droid   robot\n 4 Darth Vader           202   136 Human   large\n 5 Leia Organa           150    49 Human   other\n 6 Owen Lars             178   120 Human   other\n 7 Beru Whitesun lars    165    75 Human   other\n 8 R5-D4                  97    32 Droid   robot\n 9 Biggs Darklighter     183    84 Human   other\n10 Obi-Wan Kenobi        182    77 Human   other\n# … with 77 more rows\n\nUsing group_by() to replace NAs with summaries from the data\nLastly, it may be beneficial to replace the NA values with a summary value representative of the data. This is an example of data “imputation”.\nFor example we might decide that we want to replace NA values in the “mass” column with the average mass of the species of the character.\nUsing group_by() + mutate() is a useful paradigm for performing this operation:\n\n\nstarwars %>% \n  select(name, mass, species) %>% \n  group_by(species) %>% \n  mutate(avg_mass = mean(mass, na.rm = TRUE)) %>% \n  mutate(imputed_mass = ifelse(is.na(mass), avg_mass, mass))\n\n# A tibble: 87 × 5\n# Groups:   species [38]\n   name                mass species avg_mass imputed_mass\n   <chr>              <dbl> <chr>      <dbl>        <dbl>\n 1 Luke Skywalker        77 Human       82.8           77\n 2 C-3PO                 75 Droid       69.8           75\n 3 R2-D2                 32 Droid       69.8           32\n 4 Darth Vader          136 Human       82.8          136\n 5 Leia Organa           49 Human       82.8           49\n 6 Owen Lars            120 Human       82.8          120\n 7 Beru Whitesun lars    75 Human       82.8           75\n 8 R5-D4                 32 Droid       69.8           32\n 9 Biggs Darklighter     84 Human       82.8           84\n10 Obi-Wan Kenobi        77 Human       82.8           77\n# … with 77 more rows\n\n\nShow session info\n\n\nsessionInfo()\n\nR version 4.2.0 (2022-04-22)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur/Monterey 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] forcats_0.5.1   stringr_1.4.1   dplyr_1.0.10    purrr_0.3.5    \n[5] tibble_3.1.8    ggplot2_3.3.6   tidyverse_1.3.1 tidyr_1.2.0    \n[9] readr_2.1.2    \n\nloaded via a namespace (and not attached):\n [1] lubridate_1.8.0  assertthat_0.2.1 digest_0.6.30    utf8_1.2.2      \n [5] R6_2.5.1         cellranger_1.1.0 backports_1.4.1  reprex_2.0.1    \n [9] evaluate_0.16    httr_1.4.4       highr_0.9        pillar_1.8.1    \n[13] rlang_1.0.6      readxl_1.4.0     rstudioapi_0.13  jquerylib_0.1.4 \n[17] rmarkdown_2.14   bit_4.0.4        munsell_0.5.0    broom_0.8.0     \n[21] compiler_4.2.0   modelr_0.1.8     xfun_0.32        pkgconfig_2.0.3 \n[25] htmltools_0.5.2  downlit_0.4.2    tidyselect_1.2.0 fansi_1.0.3     \n[29] crayon_1.5.2     tzdb_0.3.0       dbplyr_2.2.1     withr_2.5.0     \n[33] grid_4.2.0       jsonlite_1.8.3   gtable_0.3.0     lifecycle_1.0.3 \n[37] DBI_1.1.3        magrittr_2.0.3   scales_1.2.0     cli_3.4.1       \n[41] stringi_1.7.8    vroom_1.5.7      cachem_1.0.6     fs_1.5.2        \n[45] xml2_1.3.3       bslib_0.3.1      ellipsis_0.3.2   generics_0.1.3  \n[49] vctrs_0.4.1      distill_1.5      tools_4.2.0      bit64_4.0.5     \n[53] glue_1.6.2       hms_1.1.2        parallel_4.2.0   fastmap_1.1.0   \n[57] yaml_2.3.6       colorspace_2.0-3 rvest_1.0.2      memoise_2.0.1   \n[61] knitr_1.39       haven_2.5.0      sass_0.4.1      \n\nAcknowledgements and additional references\nThe content of this class borrows heavily from previous tutorials:\nTutorial organization:\nhttps://github.com/sjaganna/molb7910-2019\nR tutorials and documentation:\nhttps://github.com/tidyverse/dplyr\nhttps://r4ds.had.co.nz/index.html\n\n\n\n",
    "preview": {},
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-04-class-2/",
    "title": "Class 2: Data wrangling with the tidyverse",
    "description": {},
    "author": [
      {
        "name": "Kent Riemondy",
        "url": "https://github.com/kriemo"
      }
    ],
    "date": "2022-12-01",
    "categories": [],
    "contents": "\n\nContents\nIntroduction to the tidyverse\nloading R packages\ntibble versus data.frame\nConvertly a typical data.frame to a tibble\nExploring data\n\nData import using readr\nData import/export for excel files\nData import/export of R objects\nGrammar for data manipulation: dplyr\nChaining operations\nFilter rows\narrange rows\n\nColumn operations\nselect columns\n\nWhen to quote or not quote?\nAdding new columns with mutate\nSummarizing columns\nGrouped operations\nString manipulation\nAcknowledgements and additional references\n\nThe Rmarkdown for this class is on github\nIntroduction to the tidyverse\nThe tidyverse is a collection of packages that share similar design philosophy, syntax, and data structures. The packages are largely developed by the same team that builds Rstudio.\nSome key packages that we will touch on in this course:\nggplot2: plotting based on the “grammar of graphics”dplyr: functions to manipulate tabular datatidyr: functions to help reshape data into a tidy formatreadr: functions for data import and exportstringr: functions for working with stringstibble: a redesigned data.frame\nloading R packages\nTo use an R package in an analysis we need to load the package using the library() function. This needs to be done once in each R session and it is a good idea to do this at the beginning of your Rmarkdown. For teaching purposes I will however sometimes load a package when I introduce a function from a package.\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tibble)\n\n\ntibble versus data.frame\nA tibble is a reimagining of the base R data.frame. It has a few differences from the data.frame.The biggest differences are that it doesn’t have row.names and it has an enhanced print method. If interested in learning more, see the tibble vignette.\nCompare data to data_tbl.\nNote, by default Rstudio displays data.frames in a tibble-like format\n\n\ndata <- data.frame(a = 1:3, \n                   b = letters[1:3], \n                   c = Sys.Date() - 1:3, \n                   row.names = c(\"a\", \"b\", \"c\"))\ndata_tbl <- as_tibble(data)\ndata_tbl\n\n\nWhen you work with tidyverse functions it is a good practice to convert data.frames to tibbles.\nConvertly a typical data.frame to a tibble\nIf a data.frame has rownames, you can preserve these by moving them into a column before converting to a tibble using the rownames_to_column() from tibble.\n\n\nmtcars # built in dataset, a data.frame with information about vehicles\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\nmtcars_tbl <- rownames_to_column(mtcars, \"vehicle\")\nmtcars_tbl <- as_tibble(mtcars_tbl)\nmtcars_tbl\n\n# A tibble: 32 × 12\n   vehicle       mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Mazda RX4    21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2 Mazda RX4 …  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3 Datsun 710   22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4 Hornet 4 D…  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5 Hornet Spo…  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6 Valiant      18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7 Duster 360   14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8 Merc 240D    24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9 Merc 230     22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10 Merc 280     19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# … with 22 more rows\n\nIf you don’t need the rownames, then you can use the as_tibble() function directly.\n\n\nmtcars_tbl <- as_tibble(mtcars)\n\n\nExploring data\nView() can be used to open an excel like view of a data.frame. This is a good way to quickly look at the data. glimpse() or str() give an additional view of the data.\nView(mtcars)\nglimpse(mtcars)\nstr(mtcars)\nAdditional R functions to help with exploring data.frames (and tibbles):\n\n\ndim(mtcars) # of rows and columns\nnrow(mtcars)\nncol(mtcars)\n\nhead(mtcars) # first 6 lines\nhead(mtcars, n = 2)\ntail(mtcars) # last 6 lines\ncolnames(mtcars) # column names\nrownames(mtcars) # row names (not present in tibble)\n\n\nUseful base R functions for exploring values\n\n\nmtcars$gear # extract gear column data as a vector\nmtcars[, \"gear\"] # extract gear column data as a vector\nmtcars[[\"gear\"]] # extract gear column data as a vector\n\nsummary(mtcars$gear) # get summary stats on column\n\nunique(mtcars$cyl) # find unique values in column cyl\nlength(mtcars$cyl) # length of values in a vector\n  \ntable(mtcars$cyl) # get frequency of each value in column cyl\ntable(mtcars$gear, mtcars$cyl) # get frequency of each combination of values\n\n\nData import using readr\nThe readr package provides a series of functions for importing or writing data in common text formats.\nread_csv(): comma-separated values (CSV) filesread_tsv(): tab-separated values (TSV) filesread_delim(): delimited files (CSV and TSV are important special cases)read_fwf(): fixed-width filesread_table(): whitespace-separated files\nThese functions are faster and have better defaults than the base R equivalents (e.g. read.table). These functions also directly output tibbles compatible with the tidyverse.\nThe readr checksheet provides a concise overview of the functionality in the package.\nTo illustrate how to use readr we will load a .csv file containing information about flights from 2014.\nFirst we will download the data. You can download this data manually from github. Instead we will use R to download the dataset using the download.file() base R function.\n\n\nif(!file.exists(\"flights14.csv\")) {\n  url <- \"https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv\" \n  download.file(url, \"flights14.csv\")\n}  \n\n\nYou should now have a file called “flights14.csv” in your working directory (the same directory as the Rmarkdown). To read this data into R, we can use the read_csv() function. The defaults for this function often work for many datasets.\n\n\nflights <- read_csv(\"flights14.csv\")\nflights\n\n# A tibble: 253,316 × 11\n    year month   day dep_de…¹ arr_d…² carrier origin dest  air_t…³ dista…⁴  hour\n   <dbl> <dbl> <dbl>    <dbl>   <dbl> <chr>   <chr>  <chr>   <dbl>   <dbl> <dbl>\n 1  2014     1     1       14      13 AA      JFK    LAX       359    2475     9\n 2  2014     1     1       -3      13 AA      JFK    LAX       363    2475    11\n 3  2014     1     1        2       9 AA      JFK    LAX       351    2475    19\n 4  2014     1     1       -8     -26 AA      LGA    PBI       157    1035     7\n 5  2014     1     1        2       1 AA      JFK    LAX       350    2475    13\n 6  2014     1     1        4       0 AA      EWR    LAX       339    2454    18\n 7  2014     1     1       -2     -18 AA      JFK    LAX       338    2475    21\n 8  2014     1     1       -3     -14 AA      JFK    LAX       356    2475    15\n 9  2014     1     1       -1     -17 AA      JFK    MIA       161    1089    15\n10  2014     1     1       -2     -14 AA      JFK    SEA       349    2422    18\n# … with 253,306 more rows, and abbreviated variable names ¹​dep_delay,\n#   ²​arr_delay, ³​air_time, ⁴​distance\n\nThere are a few commonly used arguments:\ncol_names: if the data doesn’t have column names, you can provide them (or skip them).\ncol_types: set this if the data type of a column is incorrectly inferred by readr\ncomment: if there are comment lines in the file, such as a header line prefixed with #, you want to skip, set this to #.\nskip: # of lines to skip before reading in the data.\nn_max: maximum number of lines to read, useful for testing reading in large datasets.\nThe readr functions will also automatically uncompress gzipped or zipped datasets, and additionally can read data directly from a URL.\nread_csv(\"https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv\")\nThere are equivalent functions for writing data from R to files:\nwrite_csv, write_tsv, write_delim.\nData import/export for excel files\nThe readxl package can read data from excel files and is included in the tidyverse. The read_excel() function is the main function for reading data.\nThe openxlsx package, which is not part of tidyverse but is on CRAN, can write excel files. The write.xlsx() function is the main function for writing data to excel spreadsheets.\nData import/export of R objects\nOften it is useful to store R objects on disk. These could be large processed datasets, intermediate results, or complex data structures that are not easily stored in rectangular text formats.\nR provides the readRDS() and saveRDS() functions for storing data in binary formats.\n\n\nsaveRDS(flights, \"flights.rds\") # save single object into a file\ndf <- readRDS(\"flights.rds\") # read object back into R\ndf\n\n# A tibble: 253,316 × 11\n    year month   day dep_de…¹ arr_d…² carrier origin dest  air_t…³ dista…⁴  hour\n   <dbl> <dbl> <dbl>    <dbl>   <dbl> <chr>   <chr>  <chr>   <dbl>   <dbl> <dbl>\n 1  2014     1     1       14      13 AA      JFK    LAX       359    2475     9\n 2  2014     1     1       -3      13 AA      JFK    LAX       363    2475    11\n 3  2014     1     1        2       9 AA      JFK    LAX       351    2475    19\n 4  2014     1     1       -8     -26 AA      LGA    PBI       157    1035     7\n 5  2014     1     1        2       1 AA      JFK    LAX       350    2475    13\n 6  2014     1     1        4       0 AA      EWR    LAX       339    2454    18\n 7  2014     1     1       -2     -18 AA      JFK    LAX       338    2475    21\n 8  2014     1     1       -3     -14 AA      JFK    LAX       356    2475    15\n 9  2014     1     1       -1     -17 AA      JFK    MIA       161    1089    15\n10  2014     1     1       -2     -14 AA      JFK    SEA       349    2422    18\n# … with 253,306 more rows, and abbreviated variable names ¹​dep_delay,\n#   ²​arr_delay, ³​air_time, ⁴​distance\n\nIf you want to save/load multiple objects you can use save() and load().\n\n\nsave(flights, df, file = \"robjs.rda\")  # save flight_df and df\n\n\nload() will load the data into the environment with the same objects names used when saving the objects.\n\n\nrm(flights, df)\nload(\"robjs.rda\")\n\n\nGrammar for data manipulation: dplyr\ndplyr provides a suite of functions for manipulating data\nin tibbles.\n*Rows:\n- filter() chooses rows based on column values\n- slice() chooses rows based on location\n- arrange() changes the order of the rows\n- distinct() selects distinct/unique rows\n*Columns:\n- select() changes whether or not a column is included\n- rename() changes the name of columns\n- mutate() changes the values of columns and creates new columns\nGroups of rows:\n- summarise() collapses a group into a single row\nChaining operations\nThe magrittr package provides the pipe operator %>%. This operator allows you to pass data from one function to another. The pipe takes data from the left-hand operation and passes it to the first argument of the right-hand operation. x %>% f(y) is equivalent to f(x, y). There is now also a pipe operator in base R (|>) which is starting to become more widely used.\nThe pipe allows complex operations to be conducted without having many intermediate variables. Chaining multiple dplyr commands is a very power and readable\n\n\nnrow(flights)\n\n[1] 253316\n\nflights %>% nrow() # get number of rows\n\n[1] 253316\n\nflights %>% nrow(x = .) # the `.` is a placeholder for the data moving through the pipe and is implied\n\n[1] 253316\n\nflights %>% colnames() %>% sort() # sort the column names\n\n [1] \"air_time\"  \"arr_delay\" \"carrier\"   \"day\"       \"dep_delay\" \"dest\"     \n [7] \"distance\"  \"hour\"      \"month\"     \"origin\"    \"year\"     \n\n# you still need to assign the output if you want to use it later\nnumber_of_rows <- flights %>% nrow() \nnumber_of_rows \n\n[1] 253316\n\nFilter rows\nReturning to our flights data. Let’s use filter() to select certain rows.\nfilter(tibble, conditional_expression, ...)\n\n\nfilter(flights, dest == \"LAX\") #select rows where the `dest` column is equal to `LAX\n\n# A tibble: 14,434 × 11\n    year month   day dep_de…¹ arr_d…² carrier origin dest  air_t…³ dista…⁴  hour\n   <dbl> <dbl> <dbl>    <dbl>   <dbl> <chr>   <chr>  <chr>   <dbl>   <dbl> <dbl>\n 1  2014     1     1       14      13 AA      JFK    LAX       359    2475     9\n 2  2014     1     1       -3      13 AA      JFK    LAX       363    2475    11\n 3  2014     1     1        2       9 AA      JFK    LAX       351    2475    19\n 4  2014     1     1        2       1 AA      JFK    LAX       350    2475    13\n 5  2014     1     1        4       0 AA      EWR    LAX       339    2454    18\n 6  2014     1     1       -2     -18 AA      JFK    LAX       338    2475    21\n 7  2014     1     1       -3     -14 AA      JFK    LAX       356    2475    15\n 8  2014     1     1      142     133 AA      JFK    LAX       345    2475    19\n 9  2014     1     1       -4      11 B6      JFK    LAX       349    2475     9\n10  2014     1     1        3     -10 B6      JFK    LAX       349    2475    16\n# … with 14,424 more rows, and abbreviated variable names ¹​dep_delay,\n#   ²​arr_delay, ³​air_time, ⁴​distance\n\n\n\nfilter(flights, arr_delay > 200) # flights with arr_delay > 200\nfilter(flights, distance < 100) # flights less than 100 miles\nfilter(flights, year != 2014) # if no rows satisfy condition, then an empty tibble\n\n\nMultiple conditions can be used to select rows. For example we can select rows where the dest column is equal to LAX and the origin is equal to EWR. You can either use the & operator, or supply multiple arguments.\n\n\nfilter(flights, dest == \"LAX\", origin == \"EWR\")\nfilter(flights, dest == \"LAX\" & origin == \"EWR\")\n\n\nWe can select rows where the dest column is equal to LAX or the origin is equal to EWR using the | operator.\n\n\nfilter(flights, dest == \"LAX\" | origin == \"EWR\")\n\n\nThe %in% operator is useful for identifying rows with entries matching those in a vector of possibilities.\n\n\nfilter(flights, dest %in% c(\"LAX\", \"SLC\", \"SFO\"))\nfilter(flights, !dest %in% c(\"LAX\", \"SLC\", \"SFO\")) # ! will negate\n\n\nTry it out:\nUse filter to find flights to DEN with a delayed departure (dep_delay).\n\n\nfilter(flights, dest == \"DEN\", dep_delay > 0)\n\n# A tibble: 3,060 × 11\n    year month   day dep_de…¹ arr_d…² carrier origin dest  air_t…³ dista…⁴  hour\n   <dbl> <dbl> <dbl>    <dbl>   <dbl> <chr>   <chr>  <chr>   <dbl>   <dbl> <dbl>\n 1  2014     1     1       45      37 B6      JFK    DEN       237    1626    22\n 2  2014     1     1        6     -13 DL      JFK    DEN       235    1626    20\n 3  2014     1     1       13      16 DL      LGA    DEN       242    1620    18\n 4  2014     1     1       35      47 F9      LGA    DEN       246    1620    18\n 5  2014     1     1        2      19 WN      EWR    DEN       259    1605    12\n 6  2014     1     1       17      60 WN      LGA    DEN       245    1620    17\n 7  2014     1     1        3      12 WN      LGA    DEN       260    1620    11\n 8  2014     1     1       10       3 UA      EWR    DEN       224    1605    17\n 9  2014     1     1       46      43 UA      LGA    DEN       235    1620    18\n10  2014     1     1       22       8 UA      EWR    DEN       237    1605     9\n# … with 3,050 more rows, and abbreviated variable names ¹​dep_delay,\n#   ²​arr_delay, ³​air_time, ⁴​distance\n\narrange rows\narrange() can be used to sort the data based on values in a single or multiple columns\narrange(tibble, <columns_to_sort_by>)\nFor example, let’s find the flight with the shortest amount of air time by arranging the table based on the air_time (flight time in minutes).\n\n\narrange(flights, air_time) \n\n# A tibble: 253,316 × 11\n    year month   day dep_de…¹ arr_d…² carrier origin dest  air_t…³ dista…⁴  hour\n   <dbl> <dbl> <dbl>    <dbl>   <dbl> <chr>   <chr>  <chr>   <dbl>   <dbl> <dbl>\n 1  2014     2    21       46      40 EV      EWR    BDL        20     116     9\n 2  2014     6    20       -6      -2 US      LGA    BOS        20     184    14\n 3  2014     1    16       -3     -12 EV      EWR    BDL        21     116    11\n 4  2014     1    16       10      14 EV      EWR    BDL        21     116     8\n 5  2014     2    19       19       0 EV      EWR    BDL        21     116     8\n 6  2014     2    26       38      20 EV      EWR    BDL        21     116    23\n 7  2014     3     4       17      -4 EV      EWR    BDL        21     116    22\n 8  2014     6     5      105      93 EV      EWR    BDL        21     116    14\n 9  2014     6     5       16       4 EV      EWR    BDL        21     116    22\n10  2014     6    26       19      13 EV      EWR    BDL        21     116    13\n# … with 253,306 more rows, and abbreviated variable names ¹​dep_delay,\n#   ²​arr_delay, ³​air_time, ⁴​distance\n\n\n\narrange(flights, air_time, distance) # sort first on distance, then on air_time\n\n # to sort in decreasing order, wrap the column name in `desc()`.\narrange(flights, desc(air_time), distance)\n\n\nTry it out:\nUse arrange to rank the data by flight distance (distance), rank in ascending order. What flight has the shortest distance?\n\n\narrange(flights, distance) %>% slice(1) \n\n# A tibble: 1 × 11\n   year month   day dep_delay arr_d…¹ carrier origin dest  air_t…² dista…³  hour\n  <dbl> <dbl> <dbl>     <dbl>   <dbl> <chr>   <chr>  <chr>   <dbl>   <dbl> <dbl>\n1  2014     1    30         9      17 US      EWR    PHL        46      80    15\n# … with abbreviated variable names ¹​arr_delay, ²​air_time, ³​distance\n\nColumn operations\nselect columns\nselect() is a simple function that subsets the tibble to keep certain columns.\nselect(tibble, <columns_to_keep>)\n\n\nselect(flights, origin, dest)\n\n# A tibble: 253,316 × 2\n   origin dest \n   <chr>  <chr>\n 1 JFK    LAX  \n 2 JFK    LAX  \n 3 JFK    LAX  \n 4 LGA    PBI  \n 5 JFK    LAX  \n 6 EWR    LAX  \n 7 JFK    LAX  \n 8 JFK    LAX  \n 9 JFK    MIA  \n10 JFK    SEA  \n# … with 253,306 more rows\n\nthe : operator can select a range of columns, such as the columns from air_time to hour. The ! operator selects columns not listed.\n\n\nselect(flights, air_time:hour)\nselect(flights, !(air_time:hour))\n\n\nThere is a suite of utilities in the tidyverse to help with select columns based on conditions: matches(), starts_with(), ends_with(), contains(), any_of(), and all_of(). everything() is also useful as a placeholder for all columns not explicitly listed. See help ?select\n\n\n# keep columns that have \"delay\" in the name\nselect(flights, contains(\"delay\"))\n\n# select all columns except carrier\nselect(flights, -carrier)\n\n# reorder columns so that distance and hour are first columns\nselect(flights, starts_with(\"di\"), ends_with(\"ay\"))\n\n\nWhen to quote or not quote?\nIn general, when working with the tidyverse, you don’t need to quote the names of columns. In the example above, we needed quotes because “delay” is not a column name in the flights tibble.\nAdding new columns with mutate\nmutate() allows you to add new columns to the tibble.\nmutate(tibble, new_column_name = expression, ...)\n\n\nmutate(flights, total_delay = dep_delay + arr_delay)\n\n# A tibble: 253,316 × 12\n    year month   day dep_de…¹ arr_d…² carrier origin dest  air_t…³ dista…⁴  hour\n   <dbl> <dbl> <dbl>    <dbl>   <dbl> <chr>   <chr>  <chr>   <dbl>   <dbl> <dbl>\n 1  2014     1     1       14      13 AA      JFK    LAX       359    2475     9\n 2  2014     1     1       -3      13 AA      JFK    LAX       363    2475    11\n 3  2014     1     1        2       9 AA      JFK    LAX       351    2475    19\n 4  2014     1     1       -8     -26 AA      LGA    PBI       157    1035     7\n 5  2014     1     1        2       1 AA      JFK    LAX       350    2475    13\n 6  2014     1     1        4       0 AA      EWR    LAX       339    2454    18\n 7  2014     1     1       -2     -18 AA      JFK    LAX       338    2475    21\n 8  2014     1     1       -3     -14 AA      JFK    LAX       356    2475    15\n 9  2014     1     1       -1     -17 AA      JFK    MIA       161    1089    15\n10  2014     1     1       -2     -14 AA      JFK    SEA       349    2422    18\n# … with 253,306 more rows, 1 more variable: total_delay <dbl>, and abbreviated\n#   variable names ¹​dep_delay, ²​arr_delay, ³​air_time, ⁴​distance\n\nWe can’t see the new column, so we add a select command to examine the columns of interest.\n\n\nmutate(flights, total_delay = dep_delay + arr_delay) %>% \n  select(dep_delay, arr_delay, total_delay)\n\n# A tibble: 253,316 × 3\n   dep_delay arr_delay total_delay\n       <dbl>     <dbl>       <dbl>\n 1        14        13          27\n 2        -3        13          10\n 3         2         9          11\n 4        -8       -26         -34\n 5         2         1           3\n 6         4         0           4\n 7        -2       -18         -20\n 8        -3       -14         -17\n 9        -1       -17         -18\n10        -2       -14         -16\n# … with 253,306 more rows\n\nMultiple new columns can be made, and you can refer to columns made in preceding statements.\n\n\nmutate(flights, \n       total_delay = dep_delay + arr_delay,\n       rank_delay = rank(total_delay)) %>% \n  select(total_delay, rank_delay)\n\n\nTry it out:\nCalculate the flight time (air_time) in hours rather than in minutes, add as a new column.\n\n\nmutate(flights, flight_time = air_time / 60)\n\n# A tibble: 253,316 × 12\n    year month   day dep_de…¹ arr_d…² carrier origin dest  air_t…³ dista…⁴  hour\n   <dbl> <dbl> <dbl>    <dbl>   <dbl> <chr>   <chr>  <chr>   <dbl>   <dbl> <dbl>\n 1  2014     1     1       14      13 AA      JFK    LAX       359    2475     9\n 2  2014     1     1       -3      13 AA      JFK    LAX       363    2475    11\n 3  2014     1     1        2       9 AA      JFK    LAX       351    2475    19\n 4  2014     1     1       -8     -26 AA      LGA    PBI       157    1035     7\n 5  2014     1     1        2       1 AA      JFK    LAX       350    2475    13\n 6  2014     1     1        4       0 AA      EWR    LAX       339    2454    18\n 7  2014     1     1       -2     -18 AA      JFK    LAX       338    2475    21\n 8  2014     1     1       -3     -14 AA      JFK    LAX       356    2475    15\n 9  2014     1     1       -1     -17 AA      JFK    MIA       161    1089    15\n10  2014     1     1       -2     -14 AA      JFK    SEA       349    2422    18\n# … with 253,306 more rows, 1 more variable: flight_time <dbl>, and abbreviated\n#   variable names ¹​dep_delay, ²​arr_delay, ³​air_time, ⁴​distance\n\nSummarizing columns\nsummarize() is a function that will collapse the data from a column into a summary value based on a function that takes a vector and returns a single value (e.g. mean(), sum(), median()). It is not very useful yet, but will be very powerful when we discuss grouped operations.\n\n\nsummarize(flights, \n          avg_arr_delay = mean(arr_delay),\n          med_air_time = median(air_time))\n\n# A tibble: 1 × 2\n  avg_arr_delay med_air_time\n          <dbl>        <dbl>\n1          8.15          134\n\nGrouped operations\nAll of the functionality described above can be easily expressed in base R syntax (see examples here). However, where dplyr really shines is the ability to apply the functions above to groups of data within each data frame.\nWe can establish groups within the data using group_by(). The functions mutate(), summarize(), and optionally arrange() will instead operate on each group independently rather than all of the rows.\nCommon approaches:\ngroup_by -> summarize: calculate summaries per group\ngroup_by -> mutate: calculate summaries per group and add as new column to original tibble\ngroup_by(tibble, <columns_to_establish_groups>)\n\n\ngroup_by(flights, carrier) # notice the new \"Groups:\" metadata. \n\n# calculate average dep_delay per carrier\ngroup_by(flights, carrier) %>% \n  summarize(avg_dep_delay = mean(dep_delay)) \n\n# calculate average arr_delay per carrier at each airport\ngroup_by(flights, carrier, origin) %>% \n  summarize(avg_dep_delay = mean(dep_delay)) \n\n# calculate # of flights between each origin and destination city, per carrier, and average air time.\n # n() is a special function that returns the # of rows per group\ngroup_by(flights, carrier, origin, dest) %>%\n  summarize(n_flights = n(),\n            mean_air_time = mean(air_time))  \n\n\nHere are some questions that we can answer using grouped operations in a few lines of dplyr code. Use pipes.\nWhat is the average flight air_time between each origin airport and destination airport?\n\n\ngroup_by(flights, origin, dest) %>% \n  summarize(avg_air_time = mean(air_time))\n\n# A tibble: 221 × 3\n# Groups:   origin [3]\n   origin dest  avg_air_time\n   <chr>  <chr>        <dbl>\n 1 EWR    ALB           31.4\n 2 EWR    ANC          424. \n 3 EWR    ATL          111. \n 4 EWR    AUS          210. \n 5 EWR    AVL           89.7\n 6 EWR    AVP           25  \n 7 EWR    BDL           25.4\n 8 EWR    BNA          115. \n 9 EWR    BOS           40.1\n10 EWR    BQN          197. \n# … with 211 more rows\n\nWhat are the fastest and longest cities to fly between on average?\n\n\ngroup_by(flights, origin, dest) %>% \n  summarize(avg_air_time = mean(air_time)) %>% \n  arrange(avg_air_time) %>% \n  head(1)\n\n# A tibble: 1 × 3\n# Groups:   origin [1]\n  origin dest  avg_air_time\n  <chr>  <chr>        <dbl>\n1 EWR    AVP             25\n\ngroup_by(flights, origin, dest) %>% \n  summarize(avg_air_time = mean(air_time)) %>% \n  arrange(desc(avg_air_time)) %>% \n  head(1)\n\n# A tibble: 1 × 3\n# Groups:   origin [1]\n  origin dest  avg_air_time\n  <chr>  <chr>        <dbl>\n1 JFK    HNL           625.\n\nTry it out:\nWhich carrier has the fastest flight (air_time) on average from JFK to LAX?\n\n# A tibble: 5 × 2\n  carrier flight_time\n  <chr>         <dbl>\n1 DL             328.\n2 UA             328.\n3 B6             328.\n4 AA             330.\n5 VX             333.\n\nWhich month has the longest departure delays on average when flying from JFK to HNL?\n\n# A tibble: 10 × 2\n   month mean_dep_delay\n   <dbl>          <dbl>\n 1     2         52.9  \n 2     1         41.2  \n 3     7          2.48 \n 4     9          1.04 \n 5     8          1.03 \n 6     3         -0.130\n 7    10         -1.73 \n 8     6         -1.76 \n 9     5         -3.52 \n10     4         -4.5  \n\nString manipulation\nstringr is a package for working with strings (i.e. character vectors). It provides a consistent syntax for string manipulation and can perform many routine tasks:\nstr_c: concatenate strings (similar to paste() in base R)str_count: count occurrence of a substring in a stringstr_subset: keep strings with a substringstr_replace: replace a string with another stringstr_split: split a string into multiple pieces based on a string\n\n\nlibrary(stringr)\nsome_words <- c(\"a sentence\", \"with a \", \"needle in a\", \"haystack\")\nstr_detect(some_words, \"needle\") # use with dplyr::filter\nstr_subset(some_words, \"needle\")\n\nstr_replace(some_words, \"needle\", \"pumpkin\")\nstr_replace_all(some_words, \"a\", \"A\")\n\nstr_c(some_words, collapse = \" \")\n\nstr_c(some_words, \" words words words\", \" anisfhlsdihg\")\n\nstr_count(some_words, \"a\")\nstr_split(some_words, \" \")\n\n\nstringr uses regular expressions to pattern match strings. This means that you can perform complex matching to the strings of interest. Additionally this means that there are special characters with behaviors that may be surprising if you are unaware of regular expressions.\nA useful resource when using regular expressions is https://regex101.com\n\n\ncomplex_strings <- c(\"10101-howdy\", \"34-world\", \"howdy-1010\", \"world-.\")\n# keep words with a series of #s followed by a dash, + indicates one or more occurrences.\nstr_subset(complex_strings, \"[0-9]+-\") \n\n# keep words with a dash followed by a series of #s\nstr_subset(complex_strings, \"-[0-9]+\") \n\nstr_subset(complex_strings, \"^howdy\") # keep words starting with howdy\nstr_subset(complex_strings, \"howdy$\") # keep words ending with howdy\nstr_subset(complex_strings, \".\") # . signifies any character\nstr_subset(complex_strings, \"\\\\.\") # need to use backticks to match literal special character\n\n\nLet’s use dplyr and stringr together.\nWhich destinations contain an “LL” in their 3 letter code?\n\n\nlibrary(stringr)\nfilter(flights, str_detect(dest, \"LL\")) %>% \n  select(dest) %>% \n  unique()\n\n# A tibble: 1 × 1\n  dest \n  <chr>\n1 FLL  \n\nWhich 3-letter destination codes start with H?\n\n\nfilter(flights, str_detect(dest, \"^H\")) %>% \n  select(dest) %>% \n  unique()\n\n# A tibble: 4 × 1\n  dest \n  <chr>\n1 HOU  \n2 HNL  \n3 HDN  \n4 HYA  \n\nLet’s make a new column that combines the origin and dest columns.\n\n\nmutate(flights, new_col = str_c(origin, \":\", dest)) %>% \n  select(new_col, everything())\n\n# A tibble: 253,316 × 12\n   new_col  year month   day dep_delay arr_delay carrier origin dest  air_time\n   <chr>   <dbl> <dbl> <dbl>     <dbl>     <dbl> <chr>   <chr>  <chr>    <dbl>\n 1 JFK:LAX  2014     1     1        14        13 AA      JFK    LAX        359\n 2 JFK:LAX  2014     1     1        -3        13 AA      JFK    LAX        363\n 3 JFK:LAX  2014     1     1         2         9 AA      JFK    LAX        351\n 4 LGA:PBI  2014     1     1        -8       -26 AA      LGA    PBI        157\n 5 JFK:LAX  2014     1     1         2         1 AA      JFK    LAX        350\n 6 EWR:LAX  2014     1     1         4         0 AA      EWR    LAX        339\n 7 JFK:LAX  2014     1     1        -2       -18 AA      JFK    LAX        338\n 8 JFK:LAX  2014     1     1        -3       -14 AA      JFK    LAX        356\n 9 JFK:MIA  2014     1     1        -1       -17 AA      JFK    MIA        161\n10 JFK:SEA  2014     1     1        -2       -14 AA      JFK    SEA        349\n# … with 253,306 more rows, and 2 more variables: distance <dbl>, hour <dbl>\n\n\nShow session info\n\n\nsessionInfo()\n\nR version 4.2.0 (2022-04-22)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur/Monterey 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] stringr_1.4.1 tibble_3.1.8  dplyr_1.0.10  readr_2.1.2  \n\nloaded via a namespace (and not attached):\n [1] bslib_0.3.1      compiler_4.2.0   pillar_1.8.1     jquerylib_0.1.4 \n [5] tools_4.2.0      bit_4.0.4        digest_0.6.30    downlit_0.4.2   \n [9] jsonlite_1.8.3   evaluate_0.16    memoise_2.0.1    lifecycle_1.0.3 \n[13] pkgconfig_2.0.3  rlang_1.0.6      DBI_1.1.3        cli_3.4.1       \n[17] rstudioapi_0.13  parallel_4.2.0   distill_1.5      yaml_2.3.6      \n[21] xfun_0.32        fastmap_1.1.0    withr_2.5.0      knitr_1.39      \n[25] generics_0.1.3   vctrs_0.4.1      sass_0.4.1       hms_1.1.2       \n[29] bit64_4.0.5      tidyselect_1.2.0 glue_1.6.2       R6_2.5.1        \n[33] fansi_1.0.3      vroom_1.5.7      rmarkdown_2.14   tzdb_0.3.0      \n[37] magrittr_2.0.3   ellipsis_0.3.2   htmltools_0.5.2  assertthat_0.2.1\n[41] utf8_1.2.2       stringi_1.7.8    cachem_1.0.6     crayon_1.5.2    \n\nAcknowledgements and additional references\nThe content of this class borrows heavily from previous tutorials:\nR code style guide:\nhttp://adv-r.had.co.nz/Style.html\nTutorial organization:\nhttps://github.com/sjaganna/molb7910-2019\nOther R tutorials:\nhttps://github.com/matloff/fasteR\nhttps://r4ds.had.co.nz/index.html\nhttps://bookdown.org/rdpeng/rprogdatascience/\n\n\n\n",
    "preview": {},
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-03-intro-to-r-language/",
    "title": "Class 1: Introduction to the R statistical programming language",
    "description": {},
    "author": [
      {
        "name": "Kent Riemondy",
        "url": "https://github.com/kriemo"
      }
    ],
    "date": "2022-11-29",
    "categories": [],
    "contents": "\n\nContents\nGoals for this class\nWhat is R\nWhy is R a popular language?\nThe R ecosystem\nGetting help\nBuilt-in documentation\nVignettes\nRstudio Cheatsheets\n\nUsing Rmarkdown to conduct data analysis\nUsing R scripts\nOrganizing projects\nOrganizing your code\nUsing R as a calculator\nAssigning values to variables\nVectors and atomic types in R\nTypes\nNA, Inf, and NaN values\nmaking vectors from scratch\nSubsetting vectors in R\nNamed vectors\n\nR operations are vectorized\nFundamental data structures\nmatrix\ndata.frame\nlist\n\nWorkspaces\nCalling functions in R\nReview\nAcknowledgements and additional references\n\n\nThe Rmarkdown for this class is on github\nGoals for this class\nR language history and ecosystem\nReview Rstudio and Rmarkdown documents\nDiscuss analysis and coding best practices\nGetting help and reading R documentation\nReview R basics\nBasic R usage\nData types\nOperators\nVectorization\nData structures\nWorkspaces\nFunctions\n\nWhat is R\nFrom the R core developers:\n\nR is an integrated suite of software facilities for data manipulation, calculation and graphical display. It includes\nan effective data handling and storage facility,\na suite of operators for calculations on arrays, in particular matrices,\na large, coherent, integrated collection of intermediate tools for data analysis,\ngraphical facilities for data analysis and display either on-screen or on hardcopy, and\na well-developed, simple and effective programming language which includes conditionals, loops, user-defined recursive functions and input and output facilities.\n\n\nR, like S, is designed around a true computer language, and it allows users to add additional functionality by defining new functions. Much of the system is itself written in the R dialect of S, which makes it easy for users to follow the algorithmic choices made. For computationally-intensive tasks, C, C++ and Fortran code can be linked and called at run time. Advanced users can write C code to manipulate R objects directly.\n\n\nMany users think of R as a statistics system. We prefer to think of it as an environment within which statistical techniques are implemented. R can be extended (easily) via packages. There are about eight packages supplied with the R distribution and many more are available through the CRAN family of Internet sites covering a very wide range of modern statistics.\n\nWhy is R a popular language?\n\n\n\nFigure 1: R facilitates the data analysis process. From https://r4ds.had.co.nz/explore-intro.html.\n\n\n\nR is a programming language built by statisticians to facilitate interactive exporatory data analysis.\nR comes with (almost) everything you need built in to rapidly conduct data analysis and visualization.\nR has a large following, which makes it easy to find help and examples of analyses.\n- Rstudio Community\n- Bioconductor Support\n- R stackoverflow\nR works out of the box on major operating systems.\nR has a robust package system of packages from CRAN and bioinformatics focused packages from Bioconductor\nPublication quality plots can be produced with ease using functionality in the base R installation or provided by additional packages.\nR has a built in documentation system to make it easy to find help and examples of how to use R functionality.\nIt’s free, open-source, and has been around in it’s first public release since 1993.\nThe R ecosystem\nWhen you download R from CRAN, there are a number of packages included in the base installation (e.g. base, stats, and datasets). You can do effective data analysis with only the base installation (e.g. see fasteR tutorial). However a key strength of R is the 10,000+ user-developed packages which extend base R functionality.\n\n\n\nFigure 2: Major R package repositories and functions used to install packages.\n\n\n\nCRAN is the official R package repository and source for R. The tidyverse (which we will use in subsequent classes) is a set of packages with consistent design principles meant to extend functionality in base R.\nBioconductor hosts and maintains bioinformatics focused packages, built around a set of core data structures and functionality focused on genomics and bioinformatics.\nGithub hosts software for any software project. It is often used to host R packages in development stages and the actively developed source code for R packages.\nGetting help\nBuilt-in documentation\nThe ? operator can be used to pull up documentation about a function. The ?? operator uses a fuzzy search which can pull up help if you don’t remember the exact function name.\n?install.packages\n??install.package\nAlternatively you can click on the help pane and search for help in Rstudio.\nVignettes\nEvery R package includes a vignette to describe the functionality of the package which can be a great resource to learn about new packages.\nThese can be accessed via the vignette() function, or via the help menu in Rstudio.\nvignette(\"dplyr\")\nRstudio Cheatsheets\nSee Help > Cheatsheets for very helpful graphical references.The base R, dplyr, and ggplot2 cheatsheets are especially useful.\nUsing Rmarkdown to conduct data analysis\nRmarkdown is a reproducible framework to create, collaborate, and communicate your work.\nRmarkdown supports a number of output formats including pdfs, word documents, slide shows, html, etc.\nAn Rmarkdown document is a plain text file with the extension .Rmd and contains the following basic components:\nAn (optional) YAML header surrounded by —s.\nChunks of R code surrounded by ```.\nText mixed with simple text formatting like # heading and italics.\n\nRmarkdown documents are executable documents. You can execute the code and render the markdown into html using the render() function, or alternatively by clicking the knit button in Rstudio.\n\n\nlibrary(rmarkdown)\nrender(\"your-rmarkdown.Rmd\")\n\n\nUsing R scripts\nR code can also be executed using R scripts, which have the .R extension. R scripts can only contain R code, not plain text or markdown. R scripts are useful if you have code that you want to run but don’t need the functionality of an Rmarkdown. You can also put custom R functions into an .R script and then use them in another document. The source() function will execute the R code in a Rscript.\n\n\n# can be a path to a .R file or a URL\nsource(\"https://raw.githubusercontent.com/rnabioco/bmsc-7810-pbda/main/_posts/2022-10-03-intro-to-r-language/custom-functions.R\")\n\ngreeting(\"class\")\n\n\nOrganizing projects\nA little bit of time spent upfront organizing your projects will make analyses easier to manage and reproduce.\nUse Rstudio projects. For the course I recommend making a new project for each class.\nUse multiple directories to separate raw data files from the analysis of the data. Organize the analyses with directories names with chronological dates\nHere’s an example organization strategy.\n.\n├── data\n│   ├── 2022-09-flow\n│   ├── 2022-09-rnaseq-1\n│   └── 2022-09-rnaseq-2\n├── docs\n│   └── project-goals.txt\n├── results\n│   ├── 2022-09-01-rnaseq-expt1\n│   │   └── gene-expression-analysis.Rmd\n│   ├── 2022-09-28-rnaseq-expt2\n│   │   └── splicing-analysis.Rmd\n│   └── 2022-10-01-flow-expt1\n│       └── flow-plots.R\n└── src\n    └── rnaseq_pipeline.sh\nSome very good ideas and examples are discussed here:\n\nNoble WS. A quick guide to organizing computational biology projects. PLoS Comput Biol. 2009 Jul;5(7):e1000424. doi: 10.1371/journal.pcbi.1000424.\n\nProvide meaningful names for your files. Consider including ordinal values (e.g. 01, 02, 03) if analyses depend on previous results to indicate ordering of execution.\n# bad\nmodels.R\nanalysis.R\nexplore.R\nanalysis-redo-final-v2.R\n# good\nclean-data.R\nfit-model.R\nplot-data.R\n# better\n01_clean-data.R\n02_fit-model.R\n03_plot-data.R\nOrganizing your code\n\n“Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.”\n— Hadley Wickham\n\nCode is used to communicate with your computer, but it also is used to communicate with your future self and your colleagues.\nDon’t just write code for yourself right now, instead write your code with the expectation that your future self will need to reread, understand, and modify it in 6 months.\nUse comments to remind yourself what the code does. The # character tells R to ignore a line of text.\n# convert x to zscores\nzs <- (x - mean(x)) / sd(x)\nUse comments to break up long scripts into logical blocks\n# Load data ---------------------------\ndat <- read_csv(\"awesome-data.csv)\ncolnames(dat) <- c(\"sample\", \"color\", \"score\", \"prediction\")\n...\n...\n# modify data -------------------------\ndat <- mutate(dat, result = score + prediction)\n...\n...\n# Plot data ---------------------------\nggplot(dat, aes(sample, score)) + \n  geom_point()\nUse sensible names for variables. Keep them short, but meaningful. Separate words with snake_case (e.g plot_df) or camelCase (plotDf) approach.\n# good\na <- width * height\np <- 2 * width + 2 * height\nmeasurement_df <- data.frame(area = a, perimeter = p)\n# bad\ny <- x1 * x2\nyy <- 2*x1 + 2*x2\ntmp <- data.frame(a = y, b = yy)\nSpace is free in code, use it liberally. Add spaces around operators.\n# Good\naverage <- mean(feet / 12 + inches, na.rm = TRUE)\n\n# Bad\naverage<-mean(feet/12+inches,na.rm=TRUE)\nSplit up complicated operations or long function calls into multiple lines. In general you can add a newline after a comma or a pipe operation (%>%). Indenting the code can also help with readability.\n# good\ndata <- complicated_function(x,\n                             minimizer = 1.4, \n                             sigma = 100,\n                             scale_values = FALSE, \n                             verbose = TRUE, \n                             additional_args = list(x = 100,\n                                                    fun = rnorm))\n# bad\ndata <- complicated_function(x, minimizer = 1.4, sigma = 100, scale_values = FALSE, verbose = TRUE, additional_args = list(x = 100, fun = rnorm))\n#good\nplot_df <- read_csv(\"awesome_data.csv\") %>% \n  select(sample, scores, condition) %>%\n  mutate(norm_scores = scores / sum(scores))\n  \n#bad\nplot_df <- read_csv(\"awesome_data.csv\") %>% select(sample, scores, condition) %>% mutate(norm_scores = scores / sum(scores)) \nRstudio has a shortcuts to help format code\nCode -> Reformat code\nCode -> Reindent lines\nUsing R as a calculator\n\n\n2 + 3 * 5     # Note the order of operations.\n\n[1] 17\n\n3-7           # value of 3-7\n\n[1] -4\n\n3/2           # Division\n\n[1] 1.5\n\n5^2           # 5 raised to the second power\n\n[1] 25\n\nAssigning values to variables\nIn R you can use either the <- or the = operators to assign objects to variables. The <- is the preferred style. If we don’t assign an operation to a variable, then it will be printed then disappear from our environment.\n\n\nx <- 1 + 1\nx # now stores the value 2\n\n[1] 2\n\n\n\nx <- x + 10 \nx + 20\n\n[1] 32\n\nNow, if we use x, what is it’s value?\nVectors and atomic types in R\nAtomic R types are actually vectors of length 1, in contrast to other languages. For example there is no int type, only integer vectors. This is why you see the [1] next to 42 when you print it, which indicates the length of the vector.\n\n\n42\n\n[1] 42\n\nTypes\nR has character, integer, double(aka numeric) and logical vector types.\n\n\ntypeof(1.0)\n\n[1] \"double\"\n\ntypeof(\"1.0\")\n\n[1] \"character\"\n\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\ntypeof(FALSE)\n\n[1] \"logical\"\n\ntypeof(\"hello world\")\n\n[1] \"character\"\n\nAlso raw and complex types, but we wont use these in the course.\nYou can change the type of a vector, provided that there is a method to convert between types.\n\n\nas.numeric(\"1.0\")\n\n[1] 1\n\nas.numeric(\"hello world\")\n\n[1] NA\n\nas.character(1.5)\n\n[1] \"1.5\"\n\nas.integer(1.5)\n\n[1] 1\n\nas.integer(TRUE)\n\n[1] 1\n\nas.integer(FALSE)\n\n[1] 0\n\nNA, Inf, and NaN values\n\n\n1 / 0\n\n[1] Inf\n\n-( 1 / 0)\n\n[1] -Inf\n\n0 / 0\n\n[1] NaN\n\nNA\n\n[1] NA\n\n\"0\" / 1\n#' Error in \"0\"/1 : non-numeric argument to binary operator\nmaking vectors from scratch\nThe c function concatenates values into a vector.\n\n\nc(1, 2, 3)\n\n[1] 1 2 3\n\nc(TRUE, FALSE, TRUE)\n\n[1]  TRUE FALSE  TRUE\n\nc(\"dog\", \"cat\", \"bird\")\n\n[1] \"dog\"  \"cat\"  \"bird\"\n\nVectors can only have 1 type, so if you supply multiple types c will silently coerce the result to a single type.\n\n\nc(TRUE, 1.9)\n\n[1] 1.0 1.9\n\nc(FALSE, \"TRUE\")\n\n[1] \"FALSE\" \"TRUE\" \n\nc(1L, 2.0, TRUE, \"Hello\")\n\n[1] \"1\"     \"2\"     \"TRUE\"  \"Hello\"\n\nNumeric ranges can be generated using : or seq\n\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(0, 1, by = 0.1)\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\nThere are also functions for sampling from various distributions or vectors.\ne.g.\n\n\n# get 5 values from a normal distribution with mean of 0 and sd of 1\nrnorm(5)\n\n[1] -1.2633152 -0.2385732  1.3109761 -0.4006697 -0.8933422\n\n# get 5 values from uniform distribution from 0 to 1\nrunif(5)\n\n[1] 0.9435736 0.6234581 0.9316560 0.5365836 0.2182703\n\n# sample 5 values from a vector\nsample(1:100, 5)\n\n[1] 74 44 14 38 60\n\nSubsetting vectors in R\nR uses 1-based indexing to select values from a vector. The first element of a vector is at index 1. The [ operator can be used to extract (or assign) elements in a vector. Integer vectors or logical vectors can be used to extract values.\n\n\nx <- 30:40\nx[1]\n\n[1] 30\n\nx[2:5]\n\n[1] 31 32 33 34\n\nx[1000]\n\n[1] NA\n\nValues in a vector can be replaced by assignment at specific indexes.\n\n\nx[2] <- 12345\nx\n\n [1]    30 12345    32    33    34    35    36    37    38    39    40\n\nx[3:10] <- 3:10\nx\n\n [1]    30 12345     3     4     5     6     7     8     9    10    40\n\nNamed vectors\nVectors in R can also have names. A named vector allows you to lookup values by name, rather than by position.\n\n\na <- 2:4\nnames(x) <- c(\"a\", \"b\", \"c\")\n\n# alternatively\nx <- c(\"a\" = 2, \"b\" = 3, \"c\" = 4)\n\nx\n\na b c \n2 3 4 \n\nx[\"c\"]\n\nc \n4 \n\nR operations are vectorized\nMost operations in R are vectorized, meaning that the operation will occur on all element of a vector.\nFor example to take the natural log of an element we use the log function.\n\n\nx <- 1:5\nlog(x)\n\n[1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379\n\nIf you are used to programming in other languages (e.g C or python) you might have written a for loop to do the same, something like this.\nfor(i in x){ \n  log(i)\n}\nIn R this is generally not necessary. The built in vectorization saves much typing and makes for very compact and efficient code in R. You can write for loops in R (more on this later in the course) however using the built in vectorization is generally a faster and easier to read solution.\nArithmetic (e.g + -, *) and comparisons operators (>, <, ==) are vectorized.\n\n\nx <- 1:5\ny <- 2\n\nx * y\n\n[1]  2  4  6  8 10\n\nx + y\n\n[1] 3 4 5 6 7\n\nx == 3\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\nx > 5\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\nx < y\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\nLogical vectors can be used to select values from a vector. This makes it easy to use a comparison operator to filter for specific values.\ne.g:\n\n\nx <- 30:40\nx[x > 35]\n\n[1] 36 37 38 39 40\n\nFundamental data structures\n\n\n\nFigure 3: Ceballos, Maite and Nicolás Cardiel. 2013. Data structure. First Steps in R. https://web.archive.org/web/20200621022950/http://venus.ifca.unican.es/Rintro/dataStruct.html\n\n\n\nmatrix\nA matrix is a 2 dimensional rectangular data structure, where all values have the same type. A matrix is used to store a collection of vectors of the same type and same length. We can subset or assign values to specific rows or columns using the bracket notation [row_index, col_index].\n\n\nm <- matrix(1:25, nrow = 5, ncol = 5)\ntypeof(m)\n\n[1] \"integer\"\n\nm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    6   11   16   21\n[2,]    2    7   12   17   22\n[3,]    3    8   13   18   23\n[4,]    4    9   14   19   24\n[5,]    5   10   15   20   25\n\nm[1:2, 1:3] # [row_index, column_index]\n\n     [,1] [,2] [,3]\n[1,]    1    6   11\n[2,]    2    7   12\n\nMatrices can have column names and row names that identify the columns. These names can also be used for subsetting the matrix.\n\n\ncolnames(m) <- LETTERS[1:5]\nrownames(m) <- letters[1:5]\nm\n\n  A  B  C  D  E\na 1  6 11 16 21\nb 2  7 12 17 22\nc 3  8 13 18 23\nd 4  9 14 19 24\ne 5 10 15 20 25\n\n\n\nm[c(\"a\", \"b\"), c(\"C\", \"D\")]\n\n   C  D\na 11 16\nb 12 17\n\ndata.frame\nA data.frame is similar to a matrix, but each column (vector) can have a different type. The same subsetting operators for matrices work on data.frames.\n\n\ndf <- data.frame(vals = 1:4, \n                 ids = c(\"cat\", \"fish\", \"bear\", \"dog\"),\n                 is_mammal = c(TRUE, FALSE, TRUE, TRUE))\ndf\n\n  vals  ids is_mammal\n1    1  cat      TRUE\n2    2 fish     FALSE\n3    3 bear      TRUE\n4    4  dog      TRUE\n\nlist\nA list is a collection of data structures, which can have different types, and each entry can have a different length or type.\n\n\nlst <- list(vals = 1:4, \n            ids = c(\"bear\", \"dog\"),\n            is_valid = TRUE,\n            aux = m)\nlst\n\n$vals\n[1] 1 2 3 4\n\n$ids\n[1] \"bear\" \"dog\" \n\n$is_valid\n[1] TRUE\n\n$aux\n  A  B  C  D  E\na 1  6 11 16 21\nb 2  7 12 17 22\nc 3  8 13 18 23\nd 4  9 14 19 24\ne 5 10 15 20 25\n\nlst[1] # list of length 1\nlst[[1]] # first element of list\nlst[[1]][1] # first value in first element of list\nA data.frame is a specialized form of a list, whereby each list entry is a vector, and all vectors have the same length. There are functions to convert between data structures (e.g. as.list(), as.data.frame(), as.matrix(), as.vector())\n\n\ndf_lst <- as.list(df)\ndf_lst\nas.data.frame(df_lst)\nas.vector(m)\n\n\nWorkspaces\nObjects that we generate get stored in an environment known as the Global Environment. You can see the objects in the global environment using the ls() function, or by clicking on the environment tab in Rstudio.\n\n\nls()\n\n[1] \"a\"   \"df\"  \"lst\" \"m\"   \"x\"   \"y\"  \n\nObjects can be removed from the environment, which can be helpful if you have a large memory object that is no longer needed.\n\n\nbig_matrix <- matrix(1:1e6, nrow = 1e5, ncol = 100)\n# show # of rows and columns\ndim(big_matrix)\n#' [1] 100000    100\n\n# remove matrix from enviroment\nrm(big_matrix)\nbig_matrix\n# 'Error: object 'big_matrix' not found\n\n\n\nWhen you close Rstudio, by default your global R environment is saved to a hidden file called .Rdata in the project directory. When you relaunch rstudio, R objects from your previous environment will be reloaded. This behavior can lead to many problems and we recommend disabling this option \nTo disable this option, go to Rstudio preferences and uncheck the “Restore .RData into workspace at startup” option and select the “Never” option for the “Save workspace to .RData on exit”.\nWe will discuss in later classes how you can save and reload specific R objects and discuss methods to import/export specific data types.\n\nCalling functions in R\nWe have already used some functions e.g. seq, typeof, matrix, as.data.frame. Functions in R have rules for how arguments are specified.\nround(x, digits = 0)\nround: function namex: required argumentdigits: optional argument (Defaults to 0)\n\n\nnums <- c(1.5, 1.4, -1.6, 0.0099)\nround(nums)\n\n[1]  2  1 -2  0\n\nround(nums, digits = 1)\n\n[1]  1.5  1.4 -1.6  0.0\n\nThe positional order of the arguments specifies that nums will be assigned to x. Alternatively you can explicitly provide the argument x = nums.\n\n\nround(x = nums, digits = 1)\n\n[1]  1.5  1.4 -1.6  0.0\n\nround(nums, 1)\n\n[1]  1.5  1.4 -1.6  0.0\n\nround(digits = 1, x = nums)\n\n[1]  1.5  1.4 -1.6  0.0\n\nYou can write your own functions as well. Functions reduce copying and pasting code, which reduces errors and simplifies code by reducing objects in the global environment.\nWe’ll learn more about functions later in the course.\n\n\nadd <- function(x, y, z = 10){\n  x + y + z\n}\nadd(2, 2)\n\n[1] 14\n\nReview\nTo review todays material, do the following:\nFor each section with code, add you own commands. You will learn faster if you try out the code yourself. If you get errors, try to find help, or ask questions in the class slack channel.\nKnit the rmarkdown after you make changes, you may need to install the distill package: (install.packages(\"distill\"))\nAcknowledgements and additional references\nThe content of this class borrows heavily from previous tutorials:\nR code style guide:\nhttp://adv-r.had.co.nz/Style.html\nTutorial organization:\nhttps://github.com/sjaganna/molb7910-2019\nOther R tutorials:\nhttps://github.com/matloff/fasteR\nhttps://r4ds.had.co.nz/index.html\nhttps://bookdown.org/rdpeng/rprogdatascience/\n\n\n\n",
    "preview": {},
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-10-03-install-r/",
    "title": "Prerequisite: R installation and Rmarkdown/Rstudio overview",
    "description": {},
    "author": [
      {
        "name": "Kent Riemondy",
        "url": "https://github.com/kriemo"
      }
    ],
    "date": "2022-10-14",
    "categories": [],
    "contents": "\n\nContents\nDownload R\nMacOS\nWindows\nLinux\n\nDownload Rstudio\nDownload compiler tools\nMacOS\nWindows\nLinux\n\nInstalling the tidyverse and Rmarkdown packages\nIntroduction to using Rstudio and Rmarkdown\n(Appendix) Installing packages from other sources\n\nThis article will explain how to install R, Rstudio, and R packages.\nPlease watch the video at the end of the article that gives an overview of using Rstudio and the Rmarkdown format.\nDownload R\nDownload R from CRAN. Go to the CRAN homepage https://cran.r-project.org/. Select the link to download R for your operating system.\nMacOS\nInstall the newest R version (4.2.1). We recommend installing the version for Intel Macs. If you have a Mac with an M1 processor (click the Apple -> About this Mac, see information under chip), we still recommend using the Intel version of R as it is much easier to install packages built for the intel version at this time. Download the .pkg file, open and follow the prompts to install.\nWindows\nSelect the base link, then click Download R-4.2.1 for Windows to download the .exe file. Open this file to install R.\nLinux\nIf you are on linux, then follow the documentation for your linux OS.\nDownload Rstudio\nGo to the Rstudio website and download Rstudio Desktop for your OS.\nOnce downloaded and installed, open up Rstudio to complete the rest of the tutorial.\nDownload compiler tools\nMacOS\nTo install the necessary compilers, we will follow the recommend steps outlined by CRAN: https://mac.r-project.org/tools/\nYou will need to install the xcode command line tools if a package requires compilation. Open Rstudio and click on the “Terminal” pane. Alternatively you can open the Terminal app from /Applications/Utilities/ (or use the search tool to search for terminal)\nType the following into terminal:\nsudo xcode-select --install\nType in your macOS password, press enter and then click “Install”. This download will require ~9Gb. Verify the installation by typing into terminal:\ngcc --version\nWhich should print something similar to this:\n#' gcc (GCC) 4.8.5\n#' Copyright (C) 2015 Free Software Foundation, Inc.\n#' This is free software; see the source for copying conditions.  There is NO\n#' warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nHere’s a youtube video explainer\nNext you need to install gfortran, which can be obtained from the CRAN instruction site above. We recommend using the intel version of R, so please follow the instructions for Intel Macs and install gfortran using the gfortran-8.2-Mojave.dmg installer.\nOnce you’ve run the gfortran installer the last step is to make sure that this program is in your PATH. This step will make the gfortran program visible to R, and other programs.\nFirst determine which type of shell you have (typically bash or zsh). Execute the following in a terminal (click either on the terminal pane in Rstudio, or open the terminal app in macOS).\necho $SHELL\nIf you see /bin/zsh then make a plain text file called .zshrc in your home directory (e.g. /Users/Your-macOS-username/), if it doesn’t already exist. If instead you see /bin/bash then make a file called .bashrc in your home directory, if it doesn’t already exist. You can use Rstudio to make a new plain-text file (File->New file->Text) or by opening up the Textedit app, then click Format->Make Plain Text.\nAdd the following line of text to the file (and keep any other text if already present).\nexport PATH=$PATH:/usr/local/gfortran/bin\nSave the text file to your home directory. You may need to rename the file after saving to ensure that it doesn’t end with .txt. (e.g. rename .zshrc.txt -> .zshrc). This file will be a hidden file. Hidden files can be seen in the Finder app by pressing Command + Shift + . (period) to toggle on/off visulizaing hidden files.\nClose and reopen Rstudio.\nWindows\nYou need to install Rtools from CRAN. Go to this link and download the exe installer for your OS: https://cran.r-project.org/bin/windows/Rtools/\nLinux\nYou should have a compiler available already.\nInstalling the tidyverse and Rmarkdown packages\nOnce you have R and Rstudio set up, open up Rstudio, then we will install packages. Packages are extensions to the base R installation that provide additionally functionality to the language. In this course we will use packages from the tidyverse, which is a collection of packages commonly used for data science and interactive data analysis. Installing the tidyverse package will install the entire collection of tidyverse packages.\nCRAN is the official R package repository. CRAN has 18,000+ packages, including the tidyverse packages. Packages from CRAN are installed using the install.packages() function. A successful install will only need to be done once for the course.\nOpen Rstudio to launch R. Then in the console pane, execute the following command to install the tidyverse:\ninstall.packages(\"tidyverse\")\n\n\n\nThis command will take a few minutes to run while all of the packages are installed. Package installation will be completed once the > prompt reappears. Once complete, test package installation by loading the package(s)\nlibrary(tidyverse)\nIf successful you will see something like this:\n\n\n\nAn error will look like this (note misspelled package name for demonstration purposes):\n\n\n\nIf loading tidyverse completes without errors then the packages have been installed. You’ll also now see additional packages (ggplot2, dplyr, tidyr) listed under the “Packages” pane.\nIf there is an error installing tidyverse, you’ll likely see the following at the end of the command:\n#' Warning in install.packages :\n#'  installation of package ‘tidyverse’ had non-zero exit status\nIf this happens, contact the course instructors to help troubleshoot the installation issue.\nAnother package that we will use in the course is rmarkdown, to install run:\ninstall.packages(\"rmarkdown\")\nand verify installation by running library(rmarkdown)\nIntroduction to using Rstudio and Rmarkdown\nNow that you have installed R and Rstudio, please watch this video (~20 minutes) that provides an overview of how to use Rstudio IDE and an introduction to the Rmarkdown format.\nintro-to-rstudio.mp4\nintro-to-rstudio.mov\n(Appendix) Installing packages from other sources\nAt this point you will only need to install the tidyverse and rmarkdown packages \nThere are 2 additional commonly used repositories for R packages that you should know about:\nBioconductor is a repository that hosts 2,000+ bioinformatics related packages.\nTo install bioconductor packages you should use the CRAN package BiocManager. BiocManager has a function called install() to install bioconductor packages. For example to install ComplexHeatmap\ninstall.packages(\"BiocManager\")\nlibrary(BiocManager)\ninstall(\"ComplexHeatmap\")\n# or equivalently you could run BiocManager::install(\"ComplexHeatmap\")\nGithub hosts open-source code from millions of software projects. R packages hosted on github can be installed using the remotes package. Packages on github are generally the development version of a package, or a package that has not been contributed to either CRAN or Bioconductor. To install you’ll need to find the organization name and the repository name on github to install.\nFor example to install the LaCroixColorR package:\ninstall.packages(\"remotes\")\nremotes::install_github('johannesbjork/LaCroixColoR')\n\n\n\n",
    "preview": "posts/2022-10-03-install-r/img/install-packages.png",
    "last_modified": "2022-12-12T17:20:31+00:00",
    "input_file": {}
  }
]
